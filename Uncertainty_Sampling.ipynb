{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Uncertainty_Sampling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP3l9batYzWG/dXTqrLDgWg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Roho1999/Bachelor_Thesis/blob/main/Uncertainty_Sampling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning the data"
      ],
      "metadata": {
        "id": "dxC4c4qXbSo4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOzQSWL2a9Di",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cde1e034-7d0b-4b0e-ffef-66f32061da50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Created on Thu Sep 16 16:18:51 2021\n",
        "@author: Robin Feldmann\n",
        "\"\"\"\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "%load_ext tensorboard\n",
        "\n",
        "def convert_Sentiment(sentiment):\n",
        "    if sentiment == \"Extremely Positive\":\n",
        "        return 2\n",
        "    elif sentiment == \"Extremely Negative\":\n",
        "        return 0\n",
        "    elif sentiment == \"Positive\":\n",
        "        return 2\n",
        "    elif sentiment == \"Negative\":\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "def remove_URL(text):\n",
        "    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
        "    return url.sub(r\"\", text)\n",
        "\n",
        "#removes hashtags\n",
        "def remove_hashtags(text):\n",
        "   \n",
        "    text =  re.sub(r\"#\\w+\", \" \", text)\n",
        "    return text\n",
        "\n",
        "#does not increase performance so not used\n",
        "def remove_numbers(text):\n",
        "    \"\"\" This function removes numbers from a text\n",
        "        inputs:\n",
        "         - text \"\"\"\n",
        "    text = re.sub(r\"\\d+\", \" \", text)\n",
        "    return text\n",
        "\n",
        "#removes words with 2 or less letters\n",
        "def remove_short_words(text):\n",
        "    \n",
        "    text =re.sub(r'\\b\\w{1,2}\\b', \" \", text)\n",
        "    \n",
        "    return text\n",
        "\n",
        "\n",
        "#removes all mentions in form of @name\n",
        "#names are not important for the sentiment and bad for tokenization and\n",
        "#embedding\n",
        "#led to a relatively strong increase in performance\n",
        "def remove_mentions(text):\n",
        "    text = re.sub(r\"@([a-zA-Z0-9_.-]{1,100})\", \" \", text)\n",
        "    return text\n",
        "\n",
        "def decontraction(text):\n",
        "    text = re.sub(r\"won\\'t\", \" will not\", text)\n",
        "    text = re.sub(r\"won\\'t've\", \" will not have\", text)\n",
        "    text = re.sub(r\"can\\'t\", \" can not\", text)\n",
        "    text = re.sub(r\"don\\'t\", \" do not\", text)\n",
        "    \n",
        "    text = re.sub(r\"can\\'t've\", \" can not have\", text)\n",
        "    text = re.sub(r\"ma\\'am\", \" madam\", text)\n",
        "    text = re.sub(r\"let\\'s\", \" let us\", text)\n",
        "    text = re.sub(r\"ain\\'t\", \" am not\", text)\n",
        "    text = re.sub(r\"shan\\'t\", \" shall not\", text)\n",
        "    text = re.sub(r\"sha\\n't\", \" shall not\", text)\n",
        "    text = re.sub(r\"o\\'clock\", \" of the clock\", text)\n",
        "    text = re.sub(r\"y\\'all\", \" you all\", text)\n",
        "    \n",
        "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
        "    text = re.sub(r\"n\\'t've\", \" not have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'s\", \" is\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'d've\", \" would have\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ll've\", \" will have\", text)\n",
        "    text = re.sub(r\"\\'t\", \" not\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'m\", \" am\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    return text   \n",
        "\n",
        "\n",
        "\n",
        "def remove_punct(text):\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(translator)\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "stop = set(stopwords.words(\"english\"))\n",
        "#remove all stopwords from text and seperate every lowers words with space\n",
        "def remove_stopwords(text):\n",
        "    \n",
        "    filtered_words = [word.lower() for word in text.split() if word.lower() not in stop]\n",
        "    \n",
        "    return \" \".join(filtered_words)\n",
        "\n",
        "\n",
        "\n",
        "#takes chars that are more then doubled and presents them as double chars\n",
        "#Example \"Helllllo\" --> \"Hello\"\n",
        "#helps to identify identical words with misspellings\n",
        "def repeated_char(text):\n",
        "    rchar = text.group(0) \n",
        "    \n",
        "    if len(rchar) > 1:\n",
        "        return rchar[0:2] \n",
        "\n",
        "def unique_char(rep, text):\n",
        "    substitute = re.sub(r'(\\w)\\1+', rep, text)\n",
        "    return substitute\n",
        "\n",
        "\n",
        "\n",
        "#returns text as lower case\n",
        "def to_lower(text):\n",
        "    \n",
        "    return text.lower()\n",
        "\n",
        "def remove_articles(text):\n",
        "\n",
        "\n",
        "  articles = {'a': '', 'an':'', 'and':'', 'the':''}\n",
        "  rest = []\n",
        "  for word in text.split():\n",
        "    if word not in articles:\n",
        "      rest.append(word)\n",
        "  return ' '.join(rest)\n",
        "\n",
        "#counts how often a word is used and determines the number of unique words\n",
        "def count_words(text_col):\n",
        "   \n",
        "    \n",
        "    count = Counter()\n",
        "    for text in text_col.values:\n",
        "        for word in text.split():\n",
        "            count[word] += 1\n",
        "    number_unique_words = len(count)\n",
        "    return count, number_unique_words\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#tokenize the words \n",
        "def tokenize(train_sentences, val_sentences, num_unique_words):\n",
        "   \n",
        "\n",
        "    #vectorize a text corpus by turning each text into a sequence of integers\n",
        "    tokenizer = Tokenizer(num_words=num_unique_words)\n",
        "    \n",
        "    tokenizer.fit_on_texts(np.append(train_sentences, val_sentences))\n",
        "    \n",
        "    word_index = tokenizer.word_index\n",
        "    return tokenizer, word_index\n",
        "\n",
        "def simple_barplot(data,names=\"\",color=['green','red', 'black', 'blue', 'violet'],\n",
        "                   title=\"\", labelx=\"\", labely=\"\"):\n",
        "    \n",
        "    f = plt.figure()\n",
        "    plt.bar(x = data, height = names, color=color)\n",
        "    f.set_figwidth(10)\n",
        "    plt.title(title, fontweight=\"heavy\")\n",
        "    plt.ylabel(labely, fontweight=\"heavy\")\n",
        "    plt.xlabel(labelx, fontweight=\"heavy\")\n",
        "    plt.show()\n",
        "    #f.savefig(f\"barplot_{color[-1]}.png\")\n",
        "    #files.download(f\"barplot_{color[-1]}.png\")\n",
        "\n",
        "#no hugh difference for the length to cut outliers: so not used\n",
        "#maybe because tweets are inherently limited to a certain amount of letters\n",
        "#sequences are just padded to the longest sequence\n",
        "def show_length_distribution(train_sequences):\n",
        "    \n",
        "    #statistic of tweet length\n",
        "    tweet_len=[len(item) for item in train_sequences]\n",
        "    \n",
        "    #important stats\n",
        "    max_len = len(max(train_sequences, key=len))\n",
        "    print(f\"Length longest squence: {max_len}\")\n",
        "    quantile = np.quantile(tweet_len, 0.99)\n",
        "    print(f\"99% quantile: {quantile}\")\n",
        "    '''\n",
        "    #plot\n",
        "    dist = plt.figure()\n",
        "    plt.scatter(np.arange(len(tweet_len))[::100], tweet_len[::100], color = \"b\")\n",
        "    plt.axhline(quantile, color = \"r\")\n",
        "    plt.title('Tweet-length')\n",
        "    plt.ylabel('Length')\n",
        "    plt.xlabel('Tweet-index')\n",
        "    plt.legend(['99% Quantile', 'Tweets'], loc='upper right')\n",
        "    plt.show()\n",
        "    '''\n",
        "    #dist.savefig(\"Length_distribution.png\")\n",
        "    #files.download(\"Length_distribution.png\")\n",
        "    #return int(quantile)\n",
        "    return int(max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To use the code, download the train and test data from https://www.kaggle.com/datatattle/covid-19-nlp-text-classification and upload it to colab named corona_tweets_train.csv and corona_tweets_test.csv respectively"
      ],
      "metadata": {
        "id": "pzr_eRqTQuP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\"\"\"\n",
        "loading the corona tweets training and test data\n",
        "from https://www.kaggle.com/datatattle/covid-19-nlp-text-classification\n",
        "\n",
        "\"\"\"\n",
        "df = pd.read_csv(\"corona_tweets_train.csv\", encoding='latin1')\n",
        "df_test = pd.read_csv(\"corona_tweets_test.csv\", encoding='latin1')\n",
        "\n",
        "\n",
        "#append the test data to the df to do the cleaning on both, after that split \n",
        "#split again\n",
        "df = df.append(df_test)\n",
        "#print(df.head())\n",
        "\n",
        "#remove unnessessary colums \n",
        "df = df.drop(['Location','TweetAt','ScreenName'], axis=1)\n",
        "\n",
        "#print(df.head())\n",
        "\n",
        "\"\"\"\n",
        "Process the target sentiments help:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#show the distribution of sentiments\n",
        "#simple_barplot(df.Sentiment.value_counts().index, df.Sentiment.value_counts().values,\n",
        "#                   title=\"Original Data Distribution\", labelx=\"Labels\", labely=\"Samples\") \n",
        "\n",
        "#simplify to positive,neutral and negative\n",
        "\n",
        "df.Sentiment = df.Sentiment.apply(lambda x : convert_Sentiment(x))\n",
        "\n",
        "\n",
        "\n",
        "#show new labels df.Sentiment.value_counts().index\n",
        "#simple_barplot([\"2 (Positive)\",\"0 (Negative)\",\"1 (Neutral)\"], df.Sentiment.value_counts().values\n",
        "#               ,title=\"Simplified Data Distribution\", labelx=\"Labels\", labely=\"Samples\",\n",
        "#               color = [\"green\", \"red\", \"black\"]) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Clean and preprocess the data, removing duplicates, punctation, URLs\n",
        "and stopwords. Functions from utils.\n",
        "\"\"\"\n",
        "df = df.drop_duplicates()\n",
        "print(\"\\nOriginal Shape: \", df.shape, \"\\n\")\n",
        "print(\"No preprocessing:\\n\",df[\"OriginalTweet\"].head(5))\n",
        "\n",
        "#removes URLs, mentions, hashtags and short words in dataset   \n",
        "df[\"OriginalTweet\"] = df.OriginalTweet.map(remove_URL)\n",
        "\n",
        "df[\"OriginalTweet\"] = df.OriginalTweet.map(remove_mentions)\n",
        "\n",
        "#does not increase accuracy or performance\n",
        "#df[\"OriginalTweet\"] = df.OriginalTweet.map(decontraction)\n",
        "\n",
        "#increases accuracy and performance\n",
        "df[\"OriginalTweet\"] = df.OriginalTweet.map(remove_hashtags)\n",
        "\n",
        "\n",
        "#removing short words does not lead to better accuracy\n",
        "#df[\"OriginalTweet\"] = df.OriginalTweet.map(remove_short_words)\n",
        "\n",
        "\n",
        "#removing numbers does not changes accuracy by a lot but decreases runtime\n",
        "df[\"OriginalTweet\"] = df.OriginalTweet.map(remove_numbers)\n",
        "\n",
        "\n",
        "#removes punctation dataset\n",
        "df[\"OriginalTweet\"] = df.OriginalTweet.map(remove_punct)\n",
        "\n",
        "\n",
        "#removes stopwords dataset, and also removes multible consecutive spaces\n",
        "#decreases complexity of the samples but also decreases accuracy significantly \n",
        "df[\"OriginalTweet\"] = df.OriginalTweet.map(remove_stopwords)\n",
        "\n",
        "#remove double words\n",
        "df['OriginalTweet'] = df['OriginalTweet'].apply(lambda x : unique_char(repeated_char,x))\n",
        "\n",
        "#all to lower case\n",
        "df[\"OriginalTweet\"] = df.OriginalTweet.map(to_lower)\n",
        "\n",
        "df[\"OriginalTweet\"] = df.OriginalTweet.map(remove_articles)\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nAfter prep.:\\n \", df[\"OriginalTweet\"].head(5))\n",
        "\n",
        "#gets a list of word counts \"counter\" and the number of unique words\n",
        "counter, num_unique_words = count_words(df.OriginalTweet)\n",
        "\n",
        "print(\"\\n\", \"The number of unique words in the OriginalTweet column: \", \n",
        "      num_unique_words, \"\\n\")\n",
        "\n",
        "'''\n",
        "\n",
        "split dataset into original training and validation set \n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "train_df = df[:41157]\n",
        "\n",
        "val_df = df[41157:]\n",
        "\n",
        "#split text and labels into numpy arrays\n",
        "train_sentences = train_df.OriginalTweet.to_numpy()\n",
        "train_labels = train_df.Sentiment.to_numpy()\n",
        "val_sentences = val_df.OriginalTweet.to_numpy()\n",
        "val_labels = val_df.Sentiment.to_numpy()\n",
        "\n",
        "\n",
        "# Convert labels to categorical \n",
        "train_labels = to_categorical(train_labels, 3)\n",
        "val_labels  = to_categorical(val_labels, 3)\n",
        "print(f\"The shape of the training and validation data: \\n\",\n",
        "f\"{train_sentences.shape} \\n {val_sentences.shape} \\n\")\n",
        "#create a tokenizer on the whole dataset and the respective word_index\n",
        "tokenizer, word_index = tokenize(train_sentences, val_sentences, num_unique_words)\n",
        "\n",
        "\"\"\"\n",
        "tokenize the validation and trainings data \n",
        "\"\"\"\n",
        "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "val_sequences = tokenizer.texts_to_sequences(val_sentences)\n",
        "                                           \n",
        "print(f\"Sequence example: {train_sequences[0]}\")\n",
        "\n",
        "'''\n",
        "show the distribution of the tweet-length and the maximal length\n",
        "to find a good pad-size\n",
        "'''\n",
        "\n",
        "pad_size = show_length_distribution(train_sequences)\n",
        "\n",
        "\n",
        "'''\n",
        "pads the squences length of the longest tweet \n",
        "to simplify training\n",
        "'''\n",
        "\n",
        "train_padded = pad_sequences(train_sequences, maxlen = pad_size, \n",
        "                             padding=\"post\", truncating=\"post\")\n",
        "\n",
        "val_padded = pad_sequences(val_sequences, maxlen = pad_size,\n",
        "                             padding=\"post\", truncating=\"post\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "O4mgGzSGbc3p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fda5164-3412-4c91-9ebf-207d0ff6e185"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original Shape:  (44955, 3) \n",
            "\n",
            "No preprocessing:\n",
            " 0    @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...\n",
            "1    advice Talk to your neighbours family to excha...\n",
            "2    Coronavirus Australia: Woolworths to give elde...\n",
            "3    My food stock is not the only one which is emp...\n",
            "4    Me, ready to go at supermarket during the #COV...\n",
            "Name: OriginalTweet, dtype: object\n",
            "\n",
            "After prep.:\n",
            "  0                                                     \n",
            "1    advice talk neighbours family exchange phone n...\n",
            "2    coronavirus australia woolworths give elderly ...\n",
            "3    food stock one empty please dont panic enough ...\n",
            "4    ready go supermarket outbreak im paranoid food...\n",
            "Name: OriginalTweet, dtype: object\n",
            "\n",
            " The number of unique words in the OriginalTweet column:  40974 \n",
            "\n",
            "The shape of the training and validation data: \n",
            " (41157,) \n",
            " (3798,) \n",
            "\n",
            "Sequence example: []\n",
            "Length longest squence: 41\n",
            "99% quantile: 30.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Model"
      ],
      "metadata": {
        "id": "Ify5rH1AbjQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "create LSTM model \n",
        "'''\n",
        "name = \"Bothreguse4_12_16\"\n",
        "from tensorflow.keras import layers, regularizers\n",
        "\n",
        "#regularizers and constraints\n",
        "l1 = regularizers.l1(1e-4)\n",
        "constraint = tf.keras.constraints.max_norm(3.)\n",
        "\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "\n",
        "model.add(layers.Embedding(num_unique_words, 512 , input_length = pad_size))\n",
        "\n",
        "#adding a convolutional layer didn't help the learning prozess, contrary to the paper \n",
        "#\"Evaluation of Deep Learning Techniques in\n",
        "#Sentiment Analysis from Twitter Data\"\n",
        "\n",
        "#breaks down the dimesionality of data and speeds up the learning significantly\n",
        "\n",
        "#bidirectional because it is works well for language processing, since language is recursive by nature\n",
        "#dropout to reduce overfitting on small details \n",
        "#regularizer to reduce exploding gradients\n",
        "#\n",
        "# with 512 dimensions best result but slower\n",
        "model.add(layers.Bidirectional(\n",
        "    layers.LSTM(512, return_sequences=True,#unroll = True,\n",
        "                recurrent_regularizer =l1,\n",
        "                dropout =0.2,#recurrent_dropout=0.35,\n",
        "                kernel_constraint= constraint,\n",
        "                bias_constraint = constraint\n",
        "                )))\n",
        "\n",
        "#MaxPool worked better than global average pooling\n",
        "model.add(layers.GlobalMaxPool1D())\n",
        "\n",
        "\n",
        "\n",
        "model.add(layers.Dense(32, activation= \"relu\", \n",
        "                       bias_regularizer = l1))\n",
        "                      # bias_constraint = constraint ))\n",
        "\n",
        "model.add(layers.Dropout(0.2))\n",
        "\n",
        "\n",
        "\n",
        "model.add(layers.Dense(3, activation= \"softmax\"))\n",
        "\n",
        "print(model.summary())\n"
      ],
      "metadata": {
        "id": "f99-xGZZbcyr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "915e7d97-c39b-413b-bed4-efab46b965fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 41, 512)           20978688  \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 41, 1024)         4198400   \n",
            " l)                                                              \n",
            "                                                                 \n",
            " global_max_pooling1d (Globa  (None, 1024)             0         \n",
            " lMaxPooling1D)                                                  \n",
            "                                                                 \n",
            " dense (Dense)               (None, 32)                32800     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 32)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3)                 99        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,209,987\n",
            "Trainable params: 25,209,987\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creates a new model with the same architecture but new weights for experiment \n",
        "#with Uncertainty Sampling\n",
        "model_entro = tf.keras.models.clone_model(model)\n",
        "\n",
        "\n",
        "#compile the model\n",
        "loss= keras.losses.CategoricalCrossentropy(from_logits = False)\n",
        "#small learning rate to get a clear understanding of the learning curve and\n",
        "#better comparability\n",
        "optim = keras.optimizers.Adam(learning_rate=0.00006)\n",
        "metrics = [\"accuracy\",tf.keras.metrics.Precision(),tf.keras.metrics.Recall()]\n",
        "model_entro.compile(loss=loss, optimizer=optim, metrics=metrics)\n"
      ],
      "metadata": {
        "id": "bX-3xN-_Bd9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Different Implementations for calculating the entropy"
      ],
      "metadata": {
        "id": "H3tysFGQb58g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The fastest implementation with simple numpy functions.\n",
        "this implementation (list_entropys3) is faster than the list_entropys1 function (with scpy.stats.entropy) by around 88.9006%:\n",
        "\n",
        "(100/0.0946) * 0.0105 - 100 = -88.9006 %"
      ],
      "metadata": {
        "id": "Asfxx3-6cUEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "fastest implementation with numpy log\n",
        "gets the prediction of the model over the whole set of samples then computes\n",
        "entropy\n",
        "\n",
        "in a trial with 640 samples it took 6.734 seconds, 0.0105s per sample\n",
        "'''\n",
        "def list_entropys3(model, training_pool):\n",
        "\n",
        "  \"\"\" Computes entropy of label distribution. \"\"\"\n",
        "  entropys = []\n",
        "  probs = model.predict(training_pool)\n",
        "  for i in range(0,len(probs)):\n",
        "    entro = 0.\n",
        "   \n",
        "    prob = probs[i]\n",
        "    # Compute entropy\n",
        "   \n",
        "    for j in prob:\n",
        "      entro -= j * np.log(j)\n",
        "\n",
        "    entropys.append(entro)\n",
        "  \n",
        "  return entropys"
      ],
      "metadata": {
        "id": "pLYMRpGnb9um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two slower functions to compute the entropy for the whole dataset."
      ],
      "metadata": {
        "id": "oH0nOyaRcR8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from scipy.stats import entropy\n",
        "from numpy import e,log\n",
        "\n",
        "\n",
        "#the two functions give back the list of the list of entropies of the training_pool in respect to\n",
        "#the model\n",
        "\n",
        "\n",
        "#slower implementation with scipy.stats.entropy \n",
        "#in a trial with 640 samples it took 60.56 seconds, 0.0946s per sample\n",
        "def list_entropys(model, training_pool):\n",
        "  entropys = []\n",
        " \n",
        "  for i in range(0,len(training_pool)):\n",
        "  \n",
        "    entro = entropy(model.predict(training_pool[[i][:]]).reshape(3,))\n",
        "    entropys.append(entro)\n",
        "  return entropys\n",
        "\n",
        "#faster implementation with numpy log\n",
        "#in a trial with 640 samples it took 50.07 seconds, 0.0782s per sample\n",
        "def list_entropys2(model, training_pool):\n",
        "\n",
        "  \"\"\" Computes entropy of label distribution. \"\"\"\n",
        "  entropys = []\n",
        "  for i in range(0,len(training_pool)):\n",
        "    entro = 0.\n",
        "   \n",
        "    probs = model.predict(training_pool[[i][:]]).reshape(3,)\n",
        "    # Compute entropy\n",
        "   \n",
        "    for i in probs:\n",
        "      entro -= i * np.log(i)\n",
        "\n",
        "    entropys.append(entro)\n",
        "  \n",
        "  return entropys\n",
        "\n"
      ],
      "metadata": {
        "id": "IlGVymEMbvYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing execution speed"
      ],
      "metadata": {
        "id": "tXS4EXBPiHcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# '''\n",
        "# Testing 1. implementation of entropy\n",
        "# '''\n",
        "# import copy\n",
        "# training_pool = copy.deepcopy(train_padded)\n",
        "# training_label = copy.deepcopy(train_labels)\n",
        "# #select\n",
        "# training_pool = training_pool[:,:] \n",
        "# training_label = training_label[:,:]\n",
        "\n",
        "# import time\n",
        "# start_time = time.time()\n",
        "# entros = np.array(list_entropys(model_entro, training_pool))\n",
        "# print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "# #entros = np.array(list_entropys(model_entro, training_pool))\n",
        "# print(entros)\n",
        "# #print(training_pool)\n",
        "\n",
        "# #sortes the training pool samples by their entropy value in descending order\n",
        "# arr1inds = entros.argsort()\n",
        "# sorted_arr1 = entros[arr1inds[::-1]]\n",
        "# sorted_arr2 = training_pool[arr1inds[::-1]]\n",
        "\n",
        "# print(sorted_arr1)\n",
        "# print(sorted_arr2)"
      ],
      "metadata": {
        "id": "bURxNJDRh9-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# '''\n",
        "# Testing 2. implementation of entropy\n",
        "# '''\n",
        "\n",
        "# start_time = time.time()\n",
        "# entros2 = np.array(list_entropys2(model_entro, training_pool))\n",
        "# print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "\n",
        "# #print(entros2)\n",
        "\n",
        "# arr1inds = entros2.argsort()\n",
        "# sorted_entropys = entros2[arr1inds[::-1]]\n",
        "# sorted_samples = training_pool[arr1inds[::-1]]\n",
        "\n",
        "# print(sorted_entropys)\n",
        "# print(sorted_samples)\n"
      ],
      "metadata": {
        "id": "Xxj4_SBLiBEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# '''\n",
        "# Testing 3. implementation of entropy test: \n",
        "# '''\n",
        "\n",
        "# start_time = time.time()\n",
        "# entros3 = np.array(list_entropys3(model_entro, training_pool))\n",
        "# print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "\n",
        "# #print(entros2)\n",
        "\n",
        "# arr1inds = entros3.argsort()\n",
        "# sorted_entropys = entros3[arr1inds[::-1]]\n",
        "# sorted_samples = training_pool[arr1inds[::-1]]\n",
        "\n",
        "# print(sorted_entropys)\n",
        "# print(sorted_samples)"
      ],
      "metadata": {
        "id": "5sOQcYvziEz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model "
      ],
      "metadata": {
        "id": "wiQqX6_oimpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "train the model and return the accuracy and val_accuracy\n",
        "\"\"\"\n",
        "\n",
        "def train_model(model, train_padded, train_labels, val_padded, val_labels):\n",
        "\n",
        "  import datetime\n",
        " \n",
        "  # Clear any logs from previous runs\n",
        "  !rm -rf /logs/\n",
        "\n",
        "  #clearing memory to avoid clutter from old runs\n",
        "  tf.keras.backend.clear_session()\n",
        " \n",
        "  ## Hyperparameters\n",
        "  epochs = 30\n",
        "  batchsize = 64\n",
        "  early_stopping = keras.callbacks.EarlyStopping(monitor =\"val_loss\", \n",
        "                                          mode =\"min\", patience=2)\n",
        "\n",
        "  #create the log file for tensorboard\n",
        "  log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "  \n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        " \n",
        "\n",
        "\n",
        "  checkpoint_filepath = '/tmp/checkpoint'\n",
        "  model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "  filepath=checkpoint_filepath,\n",
        "  save_weights_only=True,\n",
        "  monitor='val_accuracy',\n",
        "  mode='max',\n",
        "  save_best_only=True)\n",
        "   \n",
        "  #shuffels dataset before training so that the order isn't always the same and\n",
        "  #model isn't bias towards the first samples more\n",
        "  #then trains with given parameters\n",
        "  history = model.fit(train_padded,\n",
        "                      train_labels,\n",
        "                      epochs=epochs,\n",
        "                      batch_size = batchsize,\n",
        "                      validation_data=(val_padded, val_labels),\n",
        "                      callbacks=[early_stopping,tensorboard_callback,\n",
        "                        model_checkpoint_callback],\n",
        "                      verbose=1, shuffle=True)\n",
        "\n",
        "  model.load_weights(checkpoint_filepath)\n",
        "\n",
        "  return model, history.history['accuracy'][:], history.history['val_accuracy'][:]"
      ],
      "metadata": {
        "id": "0QiMr271i0fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Training-Loop"
      ],
      "metadata": {
        "id": "A_GEPPKJjJ8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#helper function to reset weights after each loop\n",
        "weights = model_entro.get_weights()\n",
        "reset_model = lambda model: model.set_weights(weights)"
      ],
      "metadata": {
        "id": "ObsuPXLNjUmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "###HYPERPARAMETERS\n",
        "x= 2048\n",
        "loops = 10\n",
        "patience = 10\n",
        "\n",
        "\n",
        "# copy the padded training data and training labels for future use\n",
        "training_pool = copy.deepcopy(train_padded)\n",
        "training_label = copy.deepcopy(train_labels)\n",
        "#select\n",
        "training_pool = training_pool[:,:] \n",
        "training_label = training_label[:,:]\n",
        "\n",
        "\n",
        "\n",
        "selected_samples = np.zeros( shape =(0,pad_size), dtype=np.int32)\n",
        "selected_labels  = np.zeros( shape =(0,3), dtype=np.int32)\n",
        "\n",
        "#take the first x random samples from the training pool and the labels to train\n",
        "#the model \n",
        "#also delete the used samples from original training_pool and training_label\n",
        "\n",
        "#512*32= 16384 around half of the samples used for the orginial model\n",
        "#x are the amount of samples that are added in each iteration\n",
        "\n",
        "\n",
        "selected_samples = np.append(selected_samples, training_pool[:x][:], axis=0)\n",
        "training_pool = np.delete(training_pool,slice(0,x) , axis=0)\n",
        "\n",
        "selected_labels = np.append(selected_labels, training_label[:x][:], axis=0)\n",
        "training_label = np.delete(training_label,slice(0,x) , axis=0)\n",
        "\n",
        "\n",
        "'''\n",
        "repeat loops times:\n",
        "  train the model for one loop and calculate the entropy in respect to the trained\n",
        "  model, select x new samples with highest entropy, reset model and relearn on the \n",
        "  new + old samples\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "val_accuracies = []\n",
        "train_accuracies = []\n",
        "wait = 0\n",
        "best = 1\n",
        "\n",
        "for i in range(loops):\n",
        "\n",
        "  print(f\"\\nIteration :{i+1} - Samples: {(i+1)*x} \\n \")\n",
        "  \n",
        "  #clearing the old model graph and reset all the weights\n",
        "  keras.backend.clear_session()\n",
        "  reset_model(model_entro)\n",
        "  \n",
        "  \n",
        "  model_entro, train_accuracy, val_accuracy = train_model(model_entro,\n",
        "                                                          selected_samples,\n",
        "                                                          selected_labels,\n",
        "                                                          val_padded, val_labels)\n",
        "  \n",
        "  #adding max accuracy from each iteration to the accuracy lists\n",
        "  max_index = np.argmax(val_accuracy)\n",
        "  val_accuracies.append(val_accuracy[max_index])\n",
        "  train_accuracies.append(train_accuracy[max_index])\n",
        "  \n",
        "\n",
        "  \n",
        "  '''\n",
        "  Select and add the samples with highest entropy to the selected samples set\n",
        "  and labels. Then delete the samples from training pool.\n",
        "  '''\n",
        "  #get a list of entropys of the samples in the training pool\n",
        "  entros = np.array(list_entropys3(model_entro, training_pool))\n",
        "  \n",
        "\n",
        "  #sort the set into partitions with highest x entropies in the first x indices\n",
        "  sorted_indices = np.argpartition(entros, -x)[::-1]\n",
        "\n",
        "  sorted_entropys = entros[sorted_indices]\n",
        "  training_pool = training_pool[sorted_indices]\n",
        "  training_label = training_label[sorted_indices]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  #add the x samples with the highest entropy values to the selected samples and\n",
        "  #labels\n",
        "  selected_samples = np.append(selected_samples, training_pool[:x][:], axis=0)\n",
        "  selected_labels = np.append(selected_labels, training_label[:x][:], axis=0)\n",
        "\n",
        "\n",
        "  #delete the selected samples and labels from orginal pool \n",
        "  training_pool = training_pool[x:][:]\n",
        "  training_label = training_label[x:][:]\n",
        "\n",
        "  '''\n",
        "  early stopping method that doesn't need labeled test data:\n",
        "  if the uncertainty for the test data does not \n",
        "  decrease for a certain amount of iterations the training stops\n",
        "  '''\n",
        "  #val = val_accuracy[max_index]\n",
        "\n",
        " \n",
        "  mean_entropy = np.mean((list_entropys3(model_entro, val_padded)))\n",
        "\n",
        "  \n",
        "  wait += 1\n",
        "  if mean_entropy < best:\n",
        "    best = mean_entropy\n",
        "    wait = 0\n",
        "  if wait >= patience:\n",
        "    break\n",
        "\n",
        "  \n",
        "  print(f\"Mean entropy for test data: {mean_entropy} - Lowest entropy for test data: {best}\")\n",
        "\n",
        "\n",
        "'''\n",
        "visualize training\n",
        "'''\n",
        "test = plt.figure()\n",
        "plt.plot(train_accuracies)\n",
        "plt.plot(val_accuracies)\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('Iteration')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "\n",
        "result = np.round(np.amax(val_accuracies), decimals = 4)\n",
        "plt.title(f'model accuracy:{result}' )\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VsnSpK-CjVYy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "af3b6a31-6e5a-4cab-ffb3-0cdff346ee41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration :1 - Samples: 2048 \n",
            " \n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 9s 170ms/step - loss: 4.5931 - accuracy: 0.3970 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 4.3810 - val_accuracy: 0.4687 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 4.1950 - accuracy: 0.4175 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 3.9981 - val_accuracy: 0.4213 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 4s 136ms/step - loss: 3.8364 - accuracy: 0.4185 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 3.6562 - val_accuracy: 0.5179 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 4s 133ms/step - loss: 3.5131 - accuracy: 0.4585 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 3.3471 - val_accuracy: 0.5253 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 4s 137ms/step - loss: 3.2174 - accuracy: 0.4536 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 3.0664 - val_accuracy: 0.5348 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 2.9441 - accuracy: 0.5034 - precision: 1.0000 - recall: 0.0015 - val_loss: 2.8088 - val_accuracy: 0.5121 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 2.6935 - accuracy: 0.5293 - precision: 0.8235 - recall: 0.0068 - val_loss: 2.5730 - val_accuracy: 0.5032 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 2.4645 - accuracy: 0.5371 - precision: 0.8857 - recall: 0.0303 - val_loss: 2.3584 - val_accuracy: 0.5300 - val_precision: 0.8294 - val_recall: 0.0371\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 4s 134ms/step - loss: 2.2468 - accuracy: 0.5859 - precision: 0.8349 - recall: 0.0889 - val_loss: 2.1604 - val_accuracy: 0.5374 - val_precision: 0.7466 - val_recall: 0.0861\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 4s 137ms/step - loss: 2.0501 - accuracy: 0.6079 - precision: 0.8215 - recall: 0.1304 - val_loss: 1.9816 - val_accuracy: 0.5434 - val_precision: 0.7456 - val_recall: 0.0995\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 4s 133ms/step - loss: 1.8568 - accuracy: 0.6338 - precision: 0.8499 - recall: 0.2295 - val_loss: 1.8170 - val_accuracy: 0.5495 - val_precision: 0.7801 - val_recall: 0.1606\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 4s 133ms/step - loss: 1.6785 - accuracy: 0.6865 - precision: 0.8777 - recall: 0.3120 - val_loss: 1.6666 - val_accuracy: 0.5648 - val_precision: 0.7762 - val_recall: 0.2457\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 4s 137ms/step - loss: 1.5002 - accuracy: 0.7114 - precision: 0.8773 - recall: 0.4468 - val_loss: 1.5279 - val_accuracy: 0.5935 - val_precision: 0.7556 - val_recall: 0.3312\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 4s 130ms/step - loss: 1.3220 - accuracy: 0.7617 - precision: 0.8730 - recall: 0.5708 - val_loss: 1.3999 - val_accuracy: 0.6293 - val_precision: 0.7330 - val_recall: 0.4121\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 4s 132ms/step - loss: 1.1264 - accuracy: 0.8037 - precision: 0.8997 - recall: 0.6875 - val_loss: 1.3019 - val_accuracy: 0.6330 - val_precision: 0.7087 - val_recall: 0.5047\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 4s 136ms/step - loss: 0.9245 - accuracy: 0.8506 - precision: 0.9017 - recall: 0.7837 - val_loss: 1.2217 - val_accuracy: 0.6632 - val_precision: 0.7198 - val_recall: 0.5661\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 4s 135ms/step - loss: 0.7648 - accuracy: 0.8784 - precision: 0.9172 - recall: 0.8271 - val_loss: 1.1845 - val_accuracy: 0.6654 - val_precision: 0.7095 - val_recall: 0.5840\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 4s 137ms/step - loss: 0.6559 - accuracy: 0.9038 - precision: 0.9308 - recall: 0.8667 - val_loss: 1.1548 - val_accuracy: 0.6685 - val_precision: 0.7046 - val_recall: 0.5961\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 4s 112ms/step - loss: 0.5696 - accuracy: 0.9263 - precision: 0.9512 - recall: 0.8940 - val_loss: 1.1497 - val_accuracy: 0.6609 - val_precision: 0.6984 - val_recall: 0.6085\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.4955 - accuracy: 0.9409 - precision: 0.9593 - recall: 0.9102 - val_loss: 1.1604 - val_accuracy: 0.6632 - val_precision: 0.6919 - val_recall: 0.6095\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.4409 - accuracy: 0.9492 - precision: 0.9653 - recall: 0.9243 - val_loss: 1.1397 - val_accuracy: 0.6669 - val_precision: 0.6942 - val_recall: 0.6174\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.3999 - accuracy: 0.9526 - precision: 0.9629 - recall: 0.9370 - val_loss: 1.1448 - val_accuracy: 0.6669 - val_precision: 0.6958 - val_recall: 0.6222\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.3480 - accuracy: 0.9634 - precision: 0.9721 - recall: 0.9531 - val_loss: 1.1727 - val_accuracy: 0.6659 - val_precision: 0.6920 - val_recall: 0.6272\n",
            "Mean entropy for test data: 0.6602139402158657 - Lowest entropy for test data: 0.6602139402158657\n",
            "\n",
            "Iteration :2 - Samples: 4096 \n",
            " \n",
            "Epoch 1/30\n",
            "64/64 [==============================] - 6s 86ms/step - loss: 4.5246 - accuracy: 0.3901 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 4.1845 - val_accuracy: 0.4315 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/30\n",
            "64/64 [==============================] - 5s 80ms/step - loss: 3.9053 - accuracy: 0.4045 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 3.5917 - val_accuracy: 0.4695 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/30\n",
            "64/64 [==============================] - 4s 71ms/step - loss: 3.3685 - accuracy: 0.4128 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 3.0961 - val_accuracy: 0.4300 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/30\n",
            "64/64 [==============================] - 6s 89ms/step - loss: 2.9163 - accuracy: 0.4033 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.6816 - val_accuracy: 0.4950 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/30\n",
            "64/64 [==============================] - 4s 70ms/step - loss: 2.5384 - accuracy: 0.4253 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.3389 - val_accuracy: 0.4587 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 6/30\n",
            "64/64 [==============================] - 4s 71ms/step - loss: 2.2275 - accuracy: 0.4409 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.0616 - val_accuracy: 0.4508 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 7/30\n",
            "64/64 [==============================] - 4s 69ms/step - loss: 1.9742 - accuracy: 0.4500 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.8374 - val_accuracy: 0.4934 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 8/30\n",
            "64/64 [==============================] - 4s 69ms/step - loss: 1.7766 - accuracy: 0.4651 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.6623 - val_accuracy: 0.4847 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 9/30\n",
            "64/64 [==============================] - 4s 70ms/step - loss: 1.6173 - accuracy: 0.5037 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.5248 - val_accuracy: 0.4839 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 10/30\n",
            "64/64 [==============================] - 5s 82ms/step - loss: 1.4931 - accuracy: 0.5007 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.4191 - val_accuracy: 0.5345 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 11/30\n",
            "64/64 [==============================] - 5s 81ms/step - loss: 1.3943 - accuracy: 0.5442 - precision: 0.9412 - recall: 0.0039 - val_loss: 1.3349 - val_accuracy: 0.5458 - val_precision: 0.7500 - val_recall: 7.8989e-04\n",
            "Epoch 12/30\n",
            "64/64 [==============================] - 5s 71ms/step - loss: 1.3087 - accuracy: 0.5879 - precision: 0.8323 - recall: 0.0339 - val_loss: 1.2629 - val_accuracy: 0.5295 - val_precision: 0.7993 - val_recall: 0.0587\n",
            "Epoch 13/30\n",
            "64/64 [==============================] - 5s 81ms/step - loss: 1.2268 - accuracy: 0.6179 - precision: 0.8167 - recall: 0.0740 - val_loss: 1.1971 - val_accuracy: 0.5461 - val_precision: 0.7267 - val_recall: 0.1190\n",
            "Epoch 14/30\n",
            "64/64 [==============================] - 5s 82ms/step - loss: 1.1411 - accuracy: 0.6416 - precision: 0.8361 - recall: 0.1494 - val_loss: 1.1317 - val_accuracy: 0.5727 - val_precision: 0.7145 - val_recall: 0.1627\n",
            "Epoch 15/30\n",
            "64/64 [==============================] - 5s 81ms/step - loss: 1.0276 - accuracy: 0.6934 - precision: 0.9073 - recall: 0.3130 - val_loss: 1.0580 - val_accuracy: 0.6190 - val_precision: 0.7781 - val_recall: 0.2880\n",
            "Epoch 16/30\n",
            "64/64 [==============================] - 6s 95ms/step - loss: 0.8791 - accuracy: 0.7639 - precision: 0.9217 - recall: 0.5256 - val_loss: 0.9750 - val_accuracy: 0.6659 - val_precision: 0.7629 - val_recall: 0.4542\n",
            "Epoch 17/30\n",
            "64/64 [==============================] - 5s 83ms/step - loss: 0.6843 - accuracy: 0.8594 - precision: 0.9248 - recall: 0.7175 - val_loss: 0.9677 - val_accuracy: 0.6856 - val_precision: 0.7383 - val_recall: 0.5972\n",
            "Epoch 18/30\n",
            "64/64 [==============================] - 4s 71ms/step - loss: 0.5009 - accuracy: 0.9185 - precision: 0.9439 - recall: 0.8667 - val_loss: 1.0004 - val_accuracy: 0.6788 - val_precision: 0.7164 - val_recall: 0.6251\n",
            "Epoch 19/30\n",
            "64/64 [==============================] - 4s 71ms/step - loss: 0.3891 - accuracy: 0.9495 - precision: 0.9629 - recall: 0.9314 - val_loss: 1.0715 - val_accuracy: 0.6827 - val_precision: 0.7058 - val_recall: 0.6493\n",
            "Mean entropy for test data: 0.7139031976215461 - Lowest entropy for test data: 0.6602139402158657\n",
            "\n",
            "Iteration :3 - Samples: 6144 \n",
            " \n",
            "Epoch 1/30\n",
            "96/96 [==============================] - 7s 68ms/step - loss: 4.3643 - accuracy: 0.3898 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 3.8583 - val_accuracy: 0.4300 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/30\n",
            "96/96 [==============================] - 5s 56ms/step - loss: 3.4642 - accuracy: 0.3896 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 3.0481 - val_accuracy: 0.4300 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/30\n",
            "96/96 [==============================] - 5s 56ms/step - loss: 2.7557 - accuracy: 0.3900 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.4340 - val_accuracy: 0.4300 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/30\n",
            "96/96 [==============================] - 5s 57ms/step - loss: 2.2256 - accuracy: 0.3944 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.9843 - val_accuracy: 0.4300 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/30\n",
            "96/96 [==============================] - 5s 56ms/step - loss: 1.8465 - accuracy: 0.3918 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.6686 - val_accuracy: 0.4300 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 6/30\n",
            "96/96 [==============================] - 5s 56ms/step - loss: 1.5835 - accuracy: 0.3939 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.4549 - val_accuracy: 0.4300 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 7/30\n",
            "96/96 [==============================] - 6s 64ms/step - loss: 1.4094 - accuracy: 0.3963 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3154 - val_accuracy: 0.4308 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 8/30\n",
            "96/96 [==============================] - 6s 63ms/step - loss: 1.2942 - accuracy: 0.4077 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.2199 - val_accuracy: 0.4502 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 9/30\n",
            "96/96 [==============================] - 6s 65ms/step - loss: 1.2163 - accuracy: 0.4398 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.1580 - val_accuracy: 0.4555 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 10/30\n",
            "96/96 [==============================] - 6s 63ms/step - loss: 1.1559 - accuracy: 0.4548 - precision: 1.0000 - recall: 3.2552e-04 - val_loss: 1.1016 - val_accuracy: 0.5864 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 11/30\n",
            "96/96 [==============================] - 6s 64ms/step - loss: 1.0960 - accuracy: 0.5599 - precision: 0.9444 - recall: 0.0055 - val_loss: 1.0442 - val_accuracy: 0.6272 - val_precision: 0.9355 - val_recall: 0.0076\n",
            "Epoch 12/30\n",
            "96/96 [==============================] - 6s 64ms/step - loss: 1.0084 - accuracy: 0.6553 - precision: 0.8901 - recall: 0.0778 - val_loss: 0.9502 - val_accuracy: 0.6527 - val_precision: 0.8031 - val_recall: 0.2341\n",
            "Epoch 13/30\n",
            "96/96 [==============================] - 6s 64ms/step - loss: 0.8681 - accuracy: 0.7456 - precision: 0.8966 - recall: 0.3473 - val_loss: 0.8425 - val_accuracy: 0.6846 - val_precision: 0.7727 - val_recall: 0.4879\n",
            "Epoch 14/30\n",
            "96/96 [==============================] - 6s 63ms/step - loss: 0.6905 - accuracy: 0.8224 - precision: 0.8975 - recall: 0.6585 - val_loss: 0.7719 - val_accuracy: 0.7064 - val_precision: 0.7549 - val_recall: 0.6164\n",
            "Epoch 15/30\n",
            "96/96 [==============================] - 6s 63ms/step - loss: 0.5246 - accuracy: 0.8802 - precision: 0.9105 - recall: 0.8200 - val_loss: 0.7410 - val_accuracy: 0.7278 - val_precision: 0.7557 - val_recall: 0.6793\n",
            "Epoch 16/30\n",
            "96/96 [==============================] - 6s 64ms/step - loss: 0.3863 - accuracy: 0.9191 - precision: 0.9361 - recall: 0.8944 - val_loss: 0.7776 - val_accuracy: 0.7346 - val_precision: 0.7526 - val_recall: 0.7104\n",
            "Epoch 17/30\n",
            "96/96 [==============================] - 6s 64ms/step - loss: 0.2878 - accuracy: 0.9450 - precision: 0.9555 - recall: 0.9359 - val_loss: 0.7953 - val_accuracy: 0.7422 - val_precision: 0.7556 - val_recall: 0.7235\n",
            "Mean entropy for test data: 0.41233476350919074 - Lowest entropy for test data: 0.41233476350919074\n",
            "\n",
            "Iteration :4 - Samples: 8192 \n",
            " \n",
            "Epoch 1/30\n",
            "128/128 [==============================] - 8s 58ms/step - loss: 4.2033 - accuracy: 0.3577 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 3.5585 - val_accuracy: 0.4071 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/30\n",
            "128/128 [==============================] - 6s 49ms/step - loss: 3.0702 - accuracy: 0.3623 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.5954 - val_accuracy: 0.4071 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/30\n",
            "128/128 [==============================] - 7s 55ms/step - loss: 2.2773 - accuracy: 0.3628 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.9549 - val_accuracy: 0.4078 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/30\n",
            "128/128 [==============================] - 7s 55ms/step - loss: 1.7657 - accuracy: 0.3691 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.5599 - val_accuracy: 0.4092 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/30\n",
            "128/128 [==============================] - 7s 56ms/step - loss: 1.4605 - accuracy: 0.3746 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3324 - val_accuracy: 0.4363 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 6/30\n",
            "128/128 [==============================] - 7s 56ms/step - loss: 1.2888 - accuracy: 0.3816 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.2031 - val_accuracy: 0.4702 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 7/30\n",
            "128/128 [==============================] - 7s 55ms/step - loss: 1.1964 - accuracy: 0.4056 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.1377 - val_accuracy: 0.5100 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 8/30\n",
            "128/128 [==============================] - 7s 55ms/step - loss: 1.1435 - accuracy: 0.4304 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.0937 - val_accuracy: 0.5116 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 9/30\n",
            "128/128 [==============================] - 7s 56ms/step - loss: 1.1075 - accuracy: 0.4563 - precision: 1.0000 - recall: 2.4414e-04 - val_loss: 1.0591 - val_accuracy: 0.5558 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 10/30\n",
            "128/128 [==============================] - 6s 51ms/step - loss: 1.0741 - accuracy: 0.5024 - precision: 0.8000 - recall: 0.0088 - val_loss: 1.0199 - val_accuracy: 0.5421 - val_precision: 0.7656 - val_recall: 0.0679\n",
            "Epoch 11/30\n",
            "128/128 [==============================] - 7s 55ms/step - loss: 1.0262 - accuracy: 0.5615 - precision: 0.8039 - recall: 0.0305 - val_loss: 0.9588 - val_accuracy: 0.5869 - val_precision: 0.7206 - val_recall: 0.1161\n",
            "Epoch 12/30\n",
            "128/128 [==============================] - 7s 55ms/step - loss: 0.9339 - accuracy: 0.6627 - precision: 0.8538 - recall: 0.1333 - val_loss: 0.8814 - val_accuracy: 0.6395 - val_precision: 0.7698 - val_recall: 0.2941\n",
            "Epoch 13/30\n",
            "128/128 [==============================] - 7s 55ms/step - loss: 0.7792 - accuracy: 0.7561 - precision: 0.8831 - recall: 0.4419 - val_loss: 0.7937 - val_accuracy: 0.6814 - val_precision: 0.7559 - val_recall: 0.5211\n",
            "Epoch 14/30\n",
            "128/128 [==============================] - 7s 56ms/step - loss: 0.5992 - accuracy: 0.8253 - precision: 0.8910 - recall: 0.7065 - val_loss: 0.7240 - val_accuracy: 0.7135 - val_precision: 0.7538 - val_recall: 0.6651\n",
            "Epoch 15/30\n",
            "128/128 [==============================] - 7s 55ms/step - loss: 0.4394 - accuracy: 0.8851 - precision: 0.9121 - recall: 0.8441 - val_loss: 0.6941 - val_accuracy: 0.7457 - val_precision: 0.7650 - val_recall: 0.7167\n",
            "Epoch 16/30\n",
            "128/128 [==============================] - 7s 55ms/step - loss: 0.3243 - accuracy: 0.9208 - precision: 0.9346 - recall: 0.9004 - val_loss: 0.6969 - val_accuracy: 0.7630 - val_precision: 0.7756 - val_recall: 0.7425\n",
            "Epoch 17/30\n",
            "128/128 [==============================] - 6s 50ms/step - loss: 0.2476 - accuracy: 0.9464 - precision: 0.9548 - recall: 0.9358 - val_loss: 0.7370 - val_accuracy: 0.7625 - val_precision: 0.7729 - val_recall: 0.7472\n",
            "Mean entropy for test data: 0.4687295996913436 - Lowest entropy for test data: 0.41233476350919074\n",
            "\n",
            "Iteration :5 - Samples: 10240 \n",
            " \n",
            "Epoch 1/30\n",
            "160/160 [==============================] - 9s 52ms/step - loss: 4.0385 - accuracy: 0.3529 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 3.2673 - val_accuracy: 0.4350 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/30\n",
            "160/160 [==============================] - 8s 50ms/step - loss: 2.7115 - accuracy: 0.3573 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.2127 - val_accuracy: 0.4455 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/30\n",
            "160/160 [==============================] - 7s 45ms/step - loss: 1.8978 - accuracy: 0.3691 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.6071 - val_accuracy: 0.4394 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/30\n",
            "160/160 [==============================] - 8s 50ms/step - loss: 1.4588 - accuracy: 0.3711 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3042 - val_accuracy: 0.4942 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/30\n",
            "160/160 [==============================] - 7s 45ms/step - loss: 1.2480 - accuracy: 0.3946 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.1616 - val_accuracy: 0.4497 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 6/30\n",
            "160/160 [==============================] - 7s 45ms/step - loss: 1.1545 - accuracy: 0.3915 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.1027 - val_accuracy: 0.4768 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 7/30\n",
            "160/160 [==============================] - 8s 50ms/step - loss: 1.1127 - accuracy: 0.4357 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.0679 - val_accuracy: 0.5477 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 8/30\n",
            "160/160 [==============================] - 8s 50ms/step - loss: 1.0877 - accuracy: 0.4695 - precision: 0.8333 - recall: 4.8828e-04 - val_loss: 1.0446 - val_accuracy: 0.5524 - val_precision: 0.6667 - val_recall: 5.2659e-04\n",
            "Epoch 9/30\n",
            "160/160 [==============================] - 8s 50ms/step - loss: 1.0645 - accuracy: 0.5117 - precision: 0.7857 - recall: 0.0118 - val_loss: 1.0074 - val_accuracy: 0.5732 - val_precision: 0.7933 - val_recall: 0.0627\n",
            "Epoch 10/30\n",
            "160/160 [==============================] - 8s 49ms/step - loss: 1.0274 - accuracy: 0.5633 - precision: 0.7738 - recall: 0.0341 - val_loss: 0.9595 - val_accuracy: 0.6114 - val_precision: 0.7234 - val_recall: 0.1219\n",
            "Epoch 11/30\n",
            "160/160 [==============================] - 8s 50ms/step - loss: 0.9483 - accuracy: 0.6517 - precision: 0.8608 - recall: 0.1178 - val_loss: 0.8642 - val_accuracy: 0.6685 - val_precision: 0.7672 - val_recall: 0.3202\n",
            "Epoch 12/30\n",
            "160/160 [==============================] - 8s 50ms/step - loss: 0.7880 - accuracy: 0.7519 - precision: 0.8832 - recall: 0.4076 - val_loss: 0.7172 - val_accuracy: 0.7204 - val_precision: 0.7795 - val_recall: 0.5864\n",
            "Epoch 13/30\n",
            "160/160 [==============================] - 8s 50ms/step - loss: 0.5577 - accuracy: 0.8469 - precision: 0.9016 - recall: 0.7291 - val_loss: 0.6214 - val_accuracy: 0.7662 - val_precision: 0.7866 - val_recall: 0.7249\n",
            "Epoch 14/30\n",
            "160/160 [==============================] - 8s 49ms/step - loss: 0.3713 - accuracy: 0.9063 - precision: 0.9278 - recall: 0.8800 - val_loss: 0.6159 - val_accuracy: 0.7841 - val_precision: 0.7973 - val_recall: 0.7704\n",
            "Epoch 15/30\n",
            "160/160 [==============================] - 8s 50ms/step - loss: 0.2622 - accuracy: 0.9439 - precision: 0.9523 - recall: 0.9329 - val_loss: 0.6540 - val_accuracy: 0.7891 - val_precision: 0.7977 - val_recall: 0.7799\n",
            "Epoch 16/30\n",
            "160/160 [==============================] - 7s 46ms/step - loss: 0.2008 - accuracy: 0.9607 - precision: 0.9672 - recall: 0.9557 - val_loss: 0.7224 - val_accuracy: 0.7849 - val_precision: 0.7910 - val_recall: 0.7780\n",
            "Mean entropy for test data: 0.3445098322194187 - Lowest entropy for test data: 0.3445098322194187\n",
            "\n",
            "Iteration :6 - Samples: 12288 \n",
            " \n",
            "Epoch 1/30\n",
            "192/192 [==============================] - 10s 49ms/step - loss: 3.8957 - accuracy: 0.3556 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 3.0177 - val_accuracy: 0.4350 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/30\n",
            "192/192 [==============================] - 9s 47ms/step - loss: 2.4401 - accuracy: 0.3652 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.9360 - val_accuracy: 0.4626 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/30\n",
            "192/192 [==============================] - 9s 47ms/step - loss: 1.6620 - accuracy: 0.3835 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.4214 - val_accuracy: 0.5208 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/30\n",
            "192/192 [==============================] - 8s 43ms/step - loss: 1.3129 - accuracy: 0.4008 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.1991 - val_accuracy: 0.4692 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/30\n",
            "192/192 [==============================] - 9s 46ms/step - loss: 1.1763 - accuracy: 0.4062 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.1222 - val_accuracy: 0.5363 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 6/30\n",
            "192/192 [==============================] - 8s 43ms/step - loss: 1.1239 - accuracy: 0.4331 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.0778 - val_accuracy: 0.4989 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 7/30\n",
            "192/192 [==============================] - 9s 46ms/step - loss: 1.0962 - accuracy: 0.4496 - precision: 0.8000 - recall: 3.2552e-04 - val_loss: 1.0446 - val_accuracy: 0.5558 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 8/30\n",
            "192/192 [==============================] - 8s 43ms/step - loss: 1.0687 - accuracy: 0.4826 - precision: 0.6893 - recall: 0.0116 - val_loss: 1.0126 - val_accuracy: 0.5208 - val_precision: 0.7630 - val_recall: 0.0771\n",
            "Epoch 9/30\n",
            "192/192 [==============================] - 9s 46ms/step - loss: 1.0224 - accuracy: 0.5297 - precision: 0.7423 - recall: 0.0628 - val_loss: 0.9528 - val_accuracy: 0.5661 - val_precision: 0.7246 - val_recall: 0.1614\n",
            "Epoch 10/30\n",
            "192/192 [==============================] - 9s 47ms/step - loss: 0.9318 - accuracy: 0.5989 - precision: 0.7858 - recall: 0.1995 - val_loss: 0.8620 - val_accuracy: 0.6135 - val_precision: 0.6893 - val_recall: 0.2991\n",
            "Epoch 11/30\n",
            "192/192 [==============================] - 9s 47ms/step - loss: 0.7796 - accuracy: 0.6965 - precision: 0.8147 - recall: 0.4372 - val_loss: 0.7716 - val_accuracy: 0.6854 - val_precision: 0.7402 - val_recall: 0.5521\n",
            "Epoch 12/30\n",
            "192/192 [==============================] - 9s 47ms/step - loss: 0.6196 - accuracy: 0.7856 - precision: 0.8430 - recall: 0.6742 - val_loss: 0.6883 - val_accuracy: 0.7391 - val_precision: 0.7645 - val_recall: 0.6898\n",
            "Epoch 13/30\n",
            "192/192 [==============================] - 9s 47ms/step - loss: 0.4734 - accuracy: 0.8549 - precision: 0.8829 - recall: 0.8090 - val_loss: 0.6257 - val_accuracy: 0.7775 - val_precision: 0.7925 - val_recall: 0.7520\n",
            "Epoch 14/30\n",
            "192/192 [==============================] - 9s 47ms/step - loss: 0.3600 - accuracy: 0.8980 - precision: 0.9127 - recall: 0.8761 - val_loss: 0.5995 - val_accuracy: 0.7909 - val_precision: 0.8014 - val_recall: 0.7788\n",
            "Epoch 15/30\n",
            "192/192 [==============================] - 9s 46ms/step - loss: 0.2782 - accuracy: 0.9241 - precision: 0.9335 - recall: 0.9128 - val_loss: 0.6087 - val_accuracy: 0.7949 - val_precision: 0.7995 - val_recall: 0.7852\n",
            "Epoch 16/30\n",
            "192/192 [==============================] - 8s 43ms/step - loss: 0.2254 - accuracy: 0.9417 - precision: 0.9469 - recall: 0.9343 - val_loss: 0.6249 - val_accuracy: 0.7936 - val_precision: 0.7991 - val_recall: 0.7865\n",
            "Mean entropy for test data: 0.39015033837444635 - Lowest entropy for test data: 0.3445098322194187\n",
            "\n",
            "Iteration :7 - Samples: 14336 \n",
            " \n",
            "Epoch 1/30\n",
            "224/224 [==============================] - 11s 46ms/step - loss: 3.7523 - accuracy: 0.3544 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.7720 - val_accuracy: 0.4086 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/30\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 2.1906 - accuracy: 0.3638 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.6940 - val_accuracy: 0.4308 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/30\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 1.4694 - accuracy: 0.3781 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.2531 - val_accuracy: 0.5068 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/30\n",
            "224/224 [==============================] - 9s 41ms/step - loss: 1.2025 - accuracy: 0.3926 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.1046 - val_accuracy: 0.4897 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/30\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 1.1159 - accuracy: 0.4116 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.0438 - val_accuracy: 0.5511 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 6/30\n",
            "224/224 [==============================] - 9s 41ms/step - loss: 1.0796 - accuracy: 0.4369 - precision: 1.0000 - recall: 6.9754e-05 - val_loss: 1.0182 - val_accuracy: 0.5432 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 7/30\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 1.0477 - accuracy: 0.4817 - precision: 0.6855 - recall: 0.0059 - val_loss: 0.9708 - val_accuracy: 0.5666 - val_precision: 0.9545 - val_recall: 0.0055\n",
            "Epoch 8/30\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 0.9926 - accuracy: 0.5366 - precision: 0.7336 - recall: 0.0711 - val_loss: 0.8958 - val_accuracy: 0.6285 - val_precision: 0.7743 - val_recall: 0.2330\n",
            "Epoch 9/30\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 0.8916 - accuracy: 0.6134 - precision: 0.7656 - recall: 0.2627 - val_loss: 0.7999 - val_accuracy: 0.6890 - val_precision: 0.7662 - val_recall: 0.4513\n",
            "Epoch 10/30\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 0.7518 - accuracy: 0.7091 - precision: 0.7920 - recall: 0.5080 - val_loss: 0.7041 - val_accuracy: 0.7328 - val_precision: 0.7691 - val_recall: 0.6377\n",
            "Epoch 11/30\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 0.5980 - accuracy: 0.7962 - precision: 0.8384 - recall: 0.7134 - val_loss: 0.6158 - val_accuracy: 0.7773 - val_precision: 0.7962 - val_recall: 0.7425\n",
            "Epoch 12/30\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 0.4529 - accuracy: 0.8620 - precision: 0.8815 - recall: 0.8285 - val_loss: 0.5701 - val_accuracy: 0.7983 - val_precision: 0.8093 - val_recall: 0.7823\n",
            "Epoch 13/30\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 0.3472 - accuracy: 0.9033 - precision: 0.9143 - recall: 0.8866 - val_loss: 0.5664 - val_accuracy: 0.8023 - val_precision: 0.8115 - val_recall: 0.7923\n",
            "Epoch 14/30\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 0.2739 - accuracy: 0.9296 - precision: 0.9382 - recall: 0.9185 - val_loss: 0.5939 - val_accuracy: 0.8057 - val_precision: 0.8134 - val_recall: 0.7988\n",
            "Epoch 15/30\n",
            "224/224 [==============================] - 9s 41ms/step - loss: 0.2134 - accuracy: 0.9481 - precision: 0.9544 - recall: 0.9415 - val_loss: 0.6343 - val_accuracy: 0.8002 - val_precision: 0.8063 - val_recall: 0.7936\n",
            "Mean entropy for test data: 0.3583310166214639 - Lowest entropy for test data: 0.3445098322194187\n",
            "\n",
            "Iteration :8 - Samples: 16384 \n",
            " \n",
            "Epoch 1/30\n",
            "256/256 [==============================] - 12s 44ms/step - loss: 3.6255 - accuracy: 0.3580 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.5677 - val_accuracy: 0.4084 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/30\n",
            "256/256 [==============================] - 11s 42ms/step - loss: 2.0009 - accuracy: 0.3672 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.5268 - val_accuracy: 0.4866 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/30\n",
            "256/256 [==============================] - 10s 40ms/step - loss: 1.3531 - accuracy: 0.3815 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.1695 - val_accuracy: 0.4205 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/30\n",
            "256/256 [==============================] - 10s 39ms/step - loss: 1.1532 - accuracy: 0.3998 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.0712 - val_accuracy: 0.4466 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/30\n",
            "256/256 [==============================] - 11s 42ms/step - loss: 1.0941 - accuracy: 0.4120 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.0264 - val_accuracy: 0.5392 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 6/30\n",
            "256/256 [==============================] - 10s 39ms/step - loss: 1.0613 - accuracy: 0.4435 - precision: 0.6667 - recall: 0.0021 - val_loss: 0.9929 - val_accuracy: 0.5245 - val_precision: 0.9167 - val_recall: 0.0029\n",
            "Epoch 7/30\n",
            "256/256 [==============================] - 11s 42ms/step - loss: 1.0212 - accuracy: 0.4839 - precision: 0.6503 - recall: 0.0341 - val_loss: 0.9428 - val_accuracy: 0.6006 - val_precision: 0.8004 - val_recall: 0.1014\n",
            "Epoch 8/30\n",
            "256/256 [==============================] - 11s 42ms/step - loss: 0.9519 - accuracy: 0.5396 - precision: 0.7049 - recall: 0.1566 - val_loss: 0.8709 - val_accuracy: 0.6237 - val_precision: 0.7637 - val_recall: 0.2825\n",
            "Epoch 9/30\n",
            "256/256 [==============================] - 11s 42ms/step - loss: 0.8496 - accuracy: 0.6118 - precision: 0.7235 - recall: 0.3417 - val_loss: 0.8008 - val_accuracy: 0.6946 - val_precision: 0.7701 - val_recall: 0.4753\n",
            "Epoch 10/30\n",
            "256/256 [==============================] - 11s 42ms/step - loss: 0.7375 - accuracy: 0.6807 - precision: 0.7506 - recall: 0.5299 - val_loss: 0.7389 - val_accuracy: 0.7141 - val_precision: 0.7571 - val_recall: 0.6203\n",
            "Epoch 11/30\n",
            "256/256 [==============================] - 11s 42ms/step - loss: 0.6239 - accuracy: 0.7657 - precision: 0.8068 - recall: 0.6829 - val_loss: 0.6589 - val_accuracy: 0.7680 - val_precision: 0.7938 - val_recall: 0.7259\n",
            "Epoch 12/30\n",
            "256/256 [==============================] - 11s 42ms/step - loss: 0.5068 - accuracy: 0.8322 - precision: 0.8562 - recall: 0.7923 - val_loss: 0.5964 - val_accuracy: 0.7878 - val_precision: 0.8052 - val_recall: 0.7672\n",
            "Epoch 13/30\n",
            "256/256 [==============================] - 11s 42ms/step - loss: 0.3968 - accuracy: 0.8796 - precision: 0.8939 - recall: 0.8585 - val_loss: 0.5840 - val_accuracy: 0.7965 - val_precision: 0.8072 - val_recall: 0.7828\n",
            "Epoch 14/30\n",
            "256/256 [==============================] - 11s 42ms/step - loss: 0.3137 - accuracy: 0.9146 - precision: 0.9259 - recall: 0.9011 - val_loss: 0.6102 - val_accuracy: 0.8004 - val_precision: 0.8096 - val_recall: 0.7894\n",
            "Epoch 15/30\n",
            "256/256 [==============================] - 11s 42ms/step - loss: 0.2422 - accuracy: 0.9391 - precision: 0.9472 - recall: 0.9294 - val_loss: 0.6386 - val_accuracy: 0.8023 - val_precision: 0.8097 - val_recall: 0.7952\n",
            "Mean entropy for test data: 0.32075447021665415 - Lowest entropy for test data: 0.32075447021665415\n",
            "\n",
            "Iteration :9 - Samples: 18432 \n",
            " \n",
            "Epoch 1/30\n",
            "288/288 [==============================] - 13s 43ms/step - loss: 3.5094 - accuracy: 0.3736 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.4005 - val_accuracy: 0.4071 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/30\n",
            "288/288 [==============================] - 12s 41ms/step - loss: 1.8535 - accuracy: 0.3799 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.4144 - val_accuracy: 0.4089 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/30\n",
            "288/288 [==============================] - 12s 41ms/step - loss: 1.2853 - accuracy: 0.3902 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.1348 - val_accuracy: 0.4113 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/30\n",
            "288/288 [==============================] - 12s 41ms/step - loss: 1.1343 - accuracy: 0.3921 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.0567 - val_accuracy: 0.4289 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/30\n",
            "288/288 [==============================] - 12s 40ms/step - loss: 1.0863 - accuracy: 0.4138 - precision: 1.0000 - recall: 1.0851e-04 - val_loss: 1.0229 - val_accuracy: 0.5082 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 6/30\n",
            "288/288 [==============================] - 12s 41ms/step - loss: 1.0518 - accuracy: 0.4477 - precision: 0.6331 - recall: 0.0048 - val_loss: 0.9818 - val_accuracy: 0.5458 - val_precision: 0.7965 - val_recall: 0.0361\n",
            "Epoch 7/30\n",
            "288/288 [==============================] - 12s 41ms/step - loss: 1.0044 - accuracy: 0.4953 - precision: 0.6650 - recall: 0.0561 - val_loss: 0.9147 - val_accuracy: 0.5637 - val_precision: 0.7655 - val_recall: 0.1890\n",
            "Epoch 8/30\n",
            "288/288 [==============================] - 12s 41ms/step - loss: 0.9279 - accuracy: 0.5615 - precision: 0.6923 - recall: 0.2070 - val_loss: 0.8387 - val_accuracy: 0.6430 - val_precision: 0.7623 - val_recall: 0.3841\n",
            "Epoch 9/30\n",
            "288/288 [==============================] - 12s 41ms/step - loss: 0.8270 - accuracy: 0.6462 - precision: 0.7298 - recall: 0.4198 - val_loss: 0.7554 - val_accuracy: 0.7109 - val_precision: 0.7659 - val_recall: 0.5487\n",
            "Epoch 10/30\n",
            "288/288 [==============================] - 12s 41ms/step - loss: 0.7073 - accuracy: 0.7278 - precision: 0.7793 - recall: 0.6069 - val_loss: 0.6694 - val_accuracy: 0.7501 - val_precision: 0.7796 - val_recall: 0.6901\n",
            "Epoch 11/30\n",
            "288/288 [==============================] - 12s 41ms/step - loss: 0.5801 - accuracy: 0.8026 - precision: 0.8292 - recall: 0.7460 - val_loss: 0.5904 - val_accuracy: 0.7836 - val_precision: 0.8044 - val_recall: 0.7588\n",
            "Epoch 12/30\n",
            "288/288 [==============================] - 12s 41ms/step - loss: 0.4560 - accuracy: 0.8573 - precision: 0.8717 - recall: 0.8305 - val_loss: 0.5612 - val_accuracy: 0.8007 - val_precision: 0.8153 - val_recall: 0.7809\n",
            "Epoch 13/30\n",
            "288/288 [==============================] - 12s 41ms/step - loss: 0.3562 - accuracy: 0.9000 - precision: 0.9114 - recall: 0.8842 - val_loss: 0.5714 - val_accuracy: 0.8049 - val_precision: 0.8146 - val_recall: 0.7936\n",
            "Epoch 14/30\n",
            "288/288 [==============================] - 12s 41ms/step - loss: 0.2889 - accuracy: 0.9242 - precision: 0.9340 - recall: 0.9127 - val_loss: 0.5998 - val_accuracy: 0.8065 - val_precision: 0.8150 - val_recall: 0.7959\n",
            "Mean entropy for test data: 0.34652030941421724 - Lowest entropy for test data: 0.32075447021665415\n",
            "\n",
            "Iteration :10 - Samples: 20480 \n",
            " \n",
            "Epoch 1/30\n",
            "320/320 [==============================] - 14s 41ms/step - loss: 3.3876 - accuracy: 0.3862 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.2230 - val_accuracy: 0.4071 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/30\n",
            "320/320 [==============================] - 13s 40ms/step - loss: 1.7070 - accuracy: 0.3913 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3088 - val_accuracy: 0.4086 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/30\n",
            "320/320 [==============================] - 13s 40ms/step - loss: 1.2143 - accuracy: 0.3973 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.0921 - val_accuracy: 0.4229 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/30\n",
            "320/320 [==============================] - 13s 40ms/step - loss: 1.0976 - accuracy: 0.4183 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.0244 - val_accuracy: 0.4760 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/30\n",
            "320/320 [==============================] - 13s 40ms/step - loss: 1.0537 - accuracy: 0.4397 - precision: 0.6226 - recall: 0.0048 - val_loss: 0.9771 - val_accuracy: 0.5395 - val_precision: 0.8667 - val_recall: 0.0068\n",
            "Epoch 6/30\n",
            "320/320 [==============================] - 12s 38ms/step - loss: 1.0055 - accuracy: 0.4827 - precision: 0.6537 - recall: 0.0642 - val_loss: 0.9200 - val_accuracy: 0.5311 - val_precision: 0.7628 - val_recall: 0.1956\n",
            "Epoch 7/30\n",
            "320/320 [==============================] - 13s 40ms/step - loss: 0.9347 - accuracy: 0.5443 - precision: 0.6614 - recall: 0.1995 - val_loss: 0.8514 - val_accuracy: 0.6177 - val_precision: 0.7431 - val_recall: 0.3381\n",
            "Epoch 8/30\n",
            "320/320 [==============================] - 13s 40ms/step - loss: 0.8370 - accuracy: 0.6167 - precision: 0.7036 - recall: 0.3810 - val_loss: 0.7794 - val_accuracy: 0.6785 - val_precision: 0.7446 - val_recall: 0.5050\n",
            "Epoch 9/30\n",
            "320/320 [==============================] - 13s 40ms/step - loss: 0.7331 - accuracy: 0.6967 - precision: 0.7508 - recall: 0.5648 - val_loss: 0.6989 - val_accuracy: 0.7422 - val_precision: 0.7829 - val_recall: 0.6627\n",
            "Epoch 10/30\n",
            "320/320 [==============================] - 13s 40ms/step - loss: 0.6199 - accuracy: 0.7756 - precision: 0.8074 - recall: 0.7102 - val_loss: 0.6097 - val_accuracy: 0.7799 - val_precision: 0.8014 - val_recall: 0.7438\n",
            "Epoch 11/30\n",
            "320/320 [==============================] - 13s 40ms/step - loss: 0.5060 - accuracy: 0.8336 - precision: 0.8509 - recall: 0.8021 - val_loss: 0.5503 - val_accuracy: 0.7994 - val_precision: 0.8179 - val_recall: 0.7830\n",
            "Epoch 12/30\n",
            "320/320 [==============================] - 13s 40ms/step - loss: 0.4007 - accuracy: 0.8807 - precision: 0.8931 - recall: 0.8636 - val_loss: 0.5389 - val_accuracy: 0.8086 - val_precision: 0.8185 - val_recall: 0.7967\n",
            "Epoch 13/30\n",
            "320/320 [==============================] - 13s 40ms/step - loss: 0.3126 - accuracy: 0.9154 - precision: 0.9240 - recall: 0.9038 - val_loss: 0.5571 - val_accuracy: 0.8094 - val_precision: 0.8179 - val_recall: 0.8020\n",
            "Epoch 14/30\n",
            "320/320 [==============================] - 13s 40ms/step - loss: 0.2482 - accuracy: 0.9373 - precision: 0.9466 - recall: 0.9286 - val_loss: 0.5959 - val_accuracy: 0.8102 - val_precision: 0.8178 - val_recall: 0.7999\n",
            "Mean entropy for test data: 0.3036852089457196 - Lowest entropy for test data: 0.3036852089457196\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import asarray\n",
        "from numpy import savetxt\n",
        "from google.colab import files\n",
        "'''\n",
        "Saving the learning curve in csv file for later inspection.\n",
        "'''\n",
        "\n",
        "savetxt('val_accuracies.csv', np.round(val_accuracies, decimals= 4) , delimiter=',')\n",
        "\n",
        "savetxt('train_accuracies.csv', np.round(train_accuracies, decimals= 4) , delimiter=',')\n",
        "\n",
        "files.download('train_accuracies.csv')\n",
        "files.download('val_accuracies.csv')\n",
        "\n",
        "print(val_accuracies)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Ns-U2F9aSoCY",
        "outputId": "50d6eb3d-177e-41a3-9320-74db9748009f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_eeec5f61-f49b-4ffa-8bcd-fe64be37223d\", \"train_accuracies.csv\", 250)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_8348e0e7-d3c9-488f-9cd9-786de9ebdde0\", \"val_accuracies.csv\", 250)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.6685097217559814, 0.6856240034103394, 0.7422327399253845, 0.7630331516265869, 0.7890995144844055, 0.7948920726776123, 0.8056871891021729, 0.8022643327713013, 0.8064770698547363, 0.8101632595062256]\n"
          ]
        }
      ]
    }
  ]
}