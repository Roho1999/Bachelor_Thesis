{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Semi_Random_Uncertainty_Sampling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP6dC1x0xmOxBxaD1Baxo4n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Roho1999/Bachelor_Thesis/blob/main/Semi_Random_Uncertainty_Sampling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning the data"
      ],
      "metadata": {
        "id": "dxC4c4qXbSo4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOzQSWL2a9Di",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c164988-feb7-4f85-db3f-d7dcd83e5bda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Created on Thu Sep 16 16:18:51 2021\n",
        "@author: Robin Feldmann\n",
        "\"\"\"\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "%load_ext tensorboard\n",
        "\n",
        "#Simpliefies the Data to 3 labels Positive, Negative and Neutral\n",
        "def convert_Sentiment(sentiment):\n",
        "    if sentiment == \"Extremely Positive\":\n",
        "        return 2\n",
        "    elif sentiment == \"Extremely Negative\":\n",
        "        return 0\n",
        "    elif sentiment == \"Positive\":\n",
        "        return 2\n",
        "    elif sentiment == \"Negative\":\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "#removes all URLs \n",
        "def remove_URL(text):\n",
        "    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
        "    return url.sub(r\"\", text)\n",
        "\n",
        "#removes hashtags\n",
        "def remove_hashtags(text):\n",
        "   \n",
        "    text =  re.sub(r\"#\\w+\", \" \", text)\n",
        "    return text\n",
        "\n",
        "#deletes all numbers\n",
        "#this doesn't increases performance and results, so it isn't used\n",
        "def remove_numbers(text):\n",
        "    text = re.sub(r\"\\d+\", \" \", text)\n",
        "    return text\n",
        "\n",
        "#removes words with 2 or less letters\n",
        "#this doesn't increases performance and results, so it isn't used\n",
        "def remove_short_words(text):\n",
        "    \n",
        "    text = re.sub(r'\\b\\w{1,2}\\b', \" \", text)\n",
        "    \n",
        "    return text\n",
        "\n",
        "\n",
        "#removes all mentions in form of @name\n",
        "#names are not important for the sentiment and bad for tokenization and\n",
        "#embedding\n",
        "#led to a relatively strong increase in performance\n",
        "def remove_mentions(text):\n",
        "    text = re.sub(r\"@([a-zA-Z0-9_.-]{1,100})\", \" \", text)\n",
        "    return text\n",
        "\n",
        "#removes contractions\n",
        "#this doesn't increases performance and results, so it isn't used\n",
        "def decontraction(text):\n",
        "    text = re.sub(r\"won\\'t\", \" will not\", text)\n",
        "    text = re.sub(r\"won\\'t've\", \" will not have\", text)\n",
        "    text = re.sub(r\"can\\'t\", \" can not\", text)\n",
        "    text = re.sub(r\"don\\'t\", \" do not\", text)\n",
        "    \n",
        "    text = re.sub(r\"can\\'t've\", \" can not have\", text)\n",
        "    text = re.sub(r\"ma\\'am\", \" madam\", text)\n",
        "    text = re.sub(r\"let\\'s\", \" let us\", text)\n",
        "    text = re.sub(r\"ain\\'t\", \" am not\", text)\n",
        "    text = re.sub(r\"shan\\'t\", \" shall not\", text)\n",
        "    text = re.sub(r\"sha\\n't\", \" shall not\", text)\n",
        "    text = re.sub(r\"o\\'clock\", \" of the clock\", text)\n",
        "    text = re.sub(r\"y\\'all\", \" you all\", text)\n",
        "    \n",
        "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
        "    text = re.sub(r\"n\\'t've\", \" not have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'s\", \" is\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'d've\", \" would have\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ll've\", \" will have\", text)\n",
        "    text = re.sub(r\"\\'t\", \" not\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'m\", \" am\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    return text   \n",
        "\n",
        "\n",
        "\n",
        "def remove_punct(text):\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(translator)\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "stop = set(stopwords.words(\"english\"))\n",
        "#remove all stopwords from text and seperate every lowers words with space\n",
        "def remove_stopwords(text):\n",
        "    \n",
        "    filtered_words = [word.lower() for word in text.split() if word.lower() not in stop]\n",
        "    \n",
        "    return \" \".join(filtered_words)\n",
        "\n",
        "\n",
        "\n",
        "#takes chars that are more then doubled and presents them as double chars\n",
        "#Example \"Helllllo\" --> \"Hello\"\n",
        "#helps to identify identical words with misspellings\n",
        "def repeated_char(text):\n",
        "    rchar = text.group(0) \n",
        "    \n",
        "    if len(rchar) > 1:\n",
        "        return rchar[0:2] \n",
        "\n",
        "#helper function for repeated_char\n",
        "def unique_char(rep, text):\n",
        "    substitute = re.sub(r'(\\w)\\1+', rep, text)\n",
        "    return substitute\n",
        "\n",
        "\n",
        "\n",
        "#returns text as lower case\n",
        "def to_lower(text):\n",
        "    \n",
        "    return text.lower()\n",
        "\n",
        "\n",
        "#removes articles from data, leads to less complex data and no hit in accuracy\n",
        "def remove_articles(text):\n",
        "\n",
        "\n",
        "  articles = {'a': '', 'an':'', 'and':'', 'the':''}\n",
        "  rest = []\n",
        "  for word in text.split():\n",
        "    if word not in articles:\n",
        "      rest.append(word)\n",
        "  return ' '.join(rest)\n",
        "\n",
        "#counts how often a word is used and determines the number of unique words\n",
        "def count_words(text_col):\n",
        "   \n",
        "    \n",
        "    count = Counter()\n",
        "    for text in text_col.values:\n",
        "        for word in text.split():\n",
        "            count[word] += 1\n",
        "    number_unique_words = len(count)\n",
        "    return count, number_unique_words\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#tokenize the words \n",
        "def tokenize(train_sentences, val_sentences, num_unique_words):\n",
        "   \n",
        "\n",
        "    #vectorize a text corpus by turning each text into a sequence of integers\n",
        "    tokenizer = Tokenizer(num_words=num_unique_words)\n",
        "    \n",
        "    tokenizer.fit_on_texts(np.append(train_sentences, val_sentences))\n",
        "    \n",
        "    word_index = tokenizer.word_index\n",
        "    return tokenizer, word_index\n",
        "\n",
        "#shows a simple barplot of given data \n",
        "def simple_barplot(data,names=\"\",color=['green','red', 'black', 'blue', 'violet'],\n",
        "                   title=\"\", labelx=\"\", labely=\"\"):\n",
        "    \n",
        "    f = plt.figure()\n",
        "    plt.bar(x = data, height = names, color=color)\n",
        "    f.set_figwidth(10)\n",
        "    plt.title(title, fontweight=\"heavy\")\n",
        "    plt.ylabel(labely, fontweight=\"heavy\")\n",
        "    plt.xlabel(labelx, fontweight=\"heavy\")\n",
        "    plt.show()\n",
        "    #f.savefig(f\"barplot_{color[-1]}.png\")\n",
        "    #files.download(f\"barplot_{color[-1]}.png\")\n",
        "\n",
        "#no hugh difference for the length to cut outliers: so not used\n",
        "#maybe because tweets are inherently limited to a certain amount of letters\n",
        "#sequences are just padded to the longest sequence\n",
        "#twitter only allows for 280 characters\n",
        "def show_length_distribution(train_sequences):\n",
        "    \n",
        "    #statistic of tweet length\n",
        "    tweet_len=[len(item) for item in train_sequences]\n",
        "    \n",
        "    #important stats\n",
        "    max_len = len(max(train_sequences, key=len))\n",
        "    print(f\"Length longest squence: {max_len}\")\n",
        "    quantile = np.quantile(tweet_len, 0.99)\n",
        "    print(f\"99% quantile: {quantile}\")\n",
        "    '''\n",
        "    #plot\n",
        "    dist = plt.figure()\n",
        "    plt.scatter(np.arange(len(tweet_len))[::100], tweet_len[::100], color = \"b\")\n",
        "    plt.axhline(quantile, color = \"r\")\n",
        "    plt.title('Tweet-length')\n",
        "    plt.ylabel('Length')\n",
        "    plt.xlabel('Tweet-index')\n",
        "    plt.legend(['99% Quantile', 'Tweets'], loc='upper right')\n",
        "    plt.show()\n",
        "    '''\n",
        "    #dist.savefig(\"Length_distribution.png\")\n",
        "    #files.download(\"Length_distribution.png\")\n",
        "    #return int(quantile)\n",
        "    return int(max_len)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To use the code, download the train and test data from https://www.kaggle.com/datatattle/covid-19-nlp-text-classification and upload it to colab named corona_tweets_train.csv and corona_tweets_test.csv respectively"
      ],
      "metadata": {
        "id": "1j-HsqBnQq60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\"\"\"\n",
        "loading the corona tweets training and test data\n",
        "from https://www.kaggle.com/datatattle/covid-19-nlp-text-classification\n",
        "\n",
        "\"\"\"\n",
        "df = pd.read_csv(\"corona_tweets_train.csv\", encoding='latin1')\n",
        "df_test = pd.read_csv(\"corona_tweets_test.csv\", encoding='latin1')\n",
        "\n",
        "\n",
        "#append the test data to the df to do the cleaning on both, after that split \n",
        "#split again\n",
        "df = df.append(df_test)\n",
        "#print(df.head())\n",
        "\n",
        "#remove unnessessary colums \n",
        "df = df.drop(['Location','TweetAt','ScreenName'], axis=1)\n",
        "\n",
        "#print(df.head())\n",
        "\n",
        "\"\"\"\n",
        "Process the target sentiments help:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#show the distribution of sentiments\n",
        "#simple_barplot(df.Sentiment.value_counts().index, df.Sentiment.value_counts().values,\n",
        "#                   title=\"Original Data Distribution\", labelx=\"Labels\", labely=\"Samples\") \n",
        "\n",
        "#simplify to positive,neutral and negative\n",
        "\n",
        "df.Sentiment = df.Sentiment.apply(lambda x : convert_Sentiment(x))\n",
        "\n",
        "\n",
        "\n",
        "#show new labels df.Sentiment.value_counts().index\n",
        "#simple_barplot([\"2 (Positive)\",\"0 (Negative)\",\"1 (Neutral)\"], df.Sentiment.value_counts().values\n",
        "#               ,title=\"Simplified Data Distribution\", labelx=\"Labels\", labely=\"Samples\",\n",
        "#               color = [\"green\", \"red\", \"black\"]) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Clean and preprocess the data, removing duplicates, punctation, URLs\n",
        "and stopwords. Functions from utils.\n",
        "\"\"\"\n",
        "df = df.drop_duplicates()\n",
        "print(\"\\nOriginal Shape: \", df.shape, \"\\n\")\n",
        "print(\"No preprocessing:\\n\",df[\"OriginalTweet\"].head(5))\n",
        "\n",
        "#removes URLs, mentions, hashtags and short words in dataset   \n",
        "df[\"OriginalTweet\"] = df.OriginalTweet.map(remove_URL)\n",
        "\n",
        "df[\"OriginalTweet\"] = df.OriginalTweet.map(remove_mentions)\n",
        "\n",
        "#does not increase accuracy or performance\n",
        "#df[\"OriginalTweet\"] = df.OriginalTweet.map(decontraction)\n",
        "\n",
        "#increases accuracy and performance\n",
        "df[\"OriginalTweet\"] = df.OriginalTweet.map(remove_hashtags)\n",
        "\n",
        "\n",
        "#removing short words does not lead to better accuracy\n",
        "#df[\"OriginalTweet\"] = df.OriginalTweet.map(remove_short_words)\n",
        "\n",
        "\n",
        "#removing numbers does not changes accuracy by a lot but decreases runtime\n",
        "df[\"OriginalTweet\"] = df.OriginalTweet.map(remove_numbers)\n",
        "\n",
        "\n",
        "#removes punctation dataset\n",
        "df[\"OriginalTweet\"] = df.OriginalTweet.map(remove_punct)\n",
        "\n",
        "\n",
        "#removes stopwords dataset, and also removes multible consecutive spaces\n",
        "#decreases complexity of the samples but also decreases accuracy significantly \n",
        "df[\"OriginalTweet\"] = df.OriginalTweet.map(remove_stopwords)\n",
        "\n",
        "#remove double words\n",
        "df['OriginalTweet'] = df['OriginalTweet'].apply(lambda x : unique_char(repeated_char,x))\n",
        "\n",
        "#all to lower case\n",
        "df[\"OriginalTweet\"] = df.OriginalTweet.map(to_lower)\n",
        "\n",
        "df[\"OriginalTweet\"] = df.OriginalTweet.map(remove_articles)\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nAfter prep.:\\n \", df[\"OriginalTweet\"].head(5))\n",
        "\n",
        "#gets a list of word counts \"counter\" and the number of unique words\n",
        "counter, num_unique_words = count_words(df.OriginalTweet)\n",
        "\n",
        "print(\"\\n\", \"The number of unique words in the OriginalTweet column: \", \n",
        "      num_unique_words, \"\\n\")\n",
        "\n",
        "'''\n",
        "\n",
        "split dataset into original training and validation set \n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "train_df = df[:41157]\n",
        "\n",
        "val_df = df[41157:]\n",
        "\n",
        "#split text and labels into numpy arrays\n",
        "train_sentences = train_df.OriginalTweet.to_numpy()\n",
        "train_labels = train_df.Sentiment.to_numpy()\n",
        "val_sentences = val_df.OriginalTweet.to_numpy()\n",
        "val_labels = val_df.Sentiment.to_numpy()\n",
        "\n",
        "\n",
        "# Convert labels to categorical \n",
        "train_labels = to_categorical(train_labels, 3)\n",
        "val_labels  = to_categorical(val_labels, 3)\n",
        "print(f\"The shape of the training and validation data: \\n\",\n",
        "f\"{train_sentences.shape} \\n {val_sentences.shape} \\n\")\n",
        "#create a tokenizer on the whole dataset and the respective word_index\n",
        "tokenizer, word_index = tokenize(train_sentences, val_sentences, num_unique_words)\n",
        "\n",
        "\"\"\"\n",
        "tokenize the validation and trainings data \n",
        "\"\"\"\n",
        "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "val_sequences = tokenizer.texts_to_sequences(val_sentences)\n",
        "                                           \n",
        "print(f\"Sequence example: {train_sequences[0]}\")\n",
        "\n",
        "'''\n",
        "show the distribution of the tweet-length and the maximal length\n",
        "to find a good pad-size\n",
        "'''\n",
        "\n",
        "pad_size = show_length_distribution(train_sequences)\n",
        "\n",
        "\n",
        "'''\n",
        "pads the squences length of the longest tweet \n",
        "to simplify training\n",
        "'''\n",
        "\n",
        "train_padded = pad_sequences(train_sequences, maxlen = pad_size, \n",
        "                             padding=\"post\", truncating=\"post\")\n",
        "\n",
        "val_padded = pad_sequences(val_sequences, maxlen = pad_size,\n",
        "                             padding=\"post\", truncating=\"post\")\n",
        "\n"
      ],
      "metadata": {
        "id": "O4mgGzSGbc3p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b05e294-39f1-4656-f6ac-98e12283987d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original Shape:  (44955, 3) \n",
            "\n",
            "No preprocessing:\n",
            " 0    @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...\n",
            "1    advice Talk to your neighbours family to excha...\n",
            "2    Coronavirus Australia: Woolworths to give elde...\n",
            "3    My food stock is not the only one which is emp...\n",
            "4    Me, ready to go at supermarket during the #COV...\n",
            "Name: OriginalTweet, dtype: object\n",
            "\n",
            "After prep.:\n",
            "  0                                                     \n",
            "1    advice talk neighbours family exchange phone n...\n",
            "2    coronavirus australia woolworths give elderly ...\n",
            "3    food stock one empty please dont panic enough ...\n",
            "4    ready go supermarket outbreak im paranoid food...\n",
            "Name: OriginalTweet, dtype: object\n",
            "\n",
            " The number of unique words in the OriginalTweet column:  40974 \n",
            "\n",
            "The shape of the training and validation data: \n",
            " (41157,) \n",
            " (3798,) \n",
            "\n",
            "Sequence example: []\n",
            "Length longest squence: 41\n",
            "99% quantile: 30.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Model"
      ],
      "metadata": {
        "id": "Ify5rH1AbjQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "create LSTM model \n",
        "'''\n",
        "name = \"Bothreguse4_12_16\"\n",
        "from tensorflow.keras import layers, regularizers\n",
        "\n",
        "#regularizers and constraints\n",
        "l1 = regularizers.l1(1e-4)\n",
        "constraint = tf.keras.constraints.max_norm(3.)\n",
        "\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "\n",
        "model.add(layers.Embedding(num_unique_words, 512 , input_length = pad_size))\n",
        "\n",
        "#adding a convolutional layer didn't help the learning prozess, contrary to the paper \n",
        "#\"Evaluation of Deep Learning Techniques in\n",
        "#Sentiment Analysis from Twitter Data\"\n",
        "\n",
        "#breaks down the dimesionality of data and speeds up the learning significantly\n",
        "\n",
        "#bidirectional because it is works well for language processing, since language is recursive by nature\n",
        "#dropout to reduce overfitting on small details \n",
        "#regularizer to reduce exploding gradients\n",
        "#\n",
        "# with 512 dimensions best result but slower\n",
        "model.add(layers.Bidirectional(\n",
        "    layers.LSTM(512, return_sequences=True,\n",
        "                recurrent_regularizer =l1,\n",
        "                dropout =0.2,\n",
        "                kernel_constraint= constraint,\n",
        "                bias_constraint = constraint\n",
        "                )))\n",
        "\n",
        "#MaxPool worked better than global average pooling\n",
        "model.add(layers.GlobalMaxPool1D())\n",
        "\n",
        "\n",
        "\n",
        "model.add(layers.Dense(32, activation= \"relu\", \n",
        "                       bias_regularizer = l1))\n",
        "                     \n",
        "\n",
        "model.add(layers.Dropout(0.2))\n",
        "\n",
        "\n",
        "\n",
        "model.add(layers.Dense(3, activation= \"softmax\"))\n",
        "\n",
        "print(model.summary())\n"
      ],
      "metadata": {
        "id": "f99-xGZZbcyr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "830d71ea-7d73-4334-a34d-2d1c487794ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 41, 512)           20978688  \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 41, 1024)         4198400   \n",
            " l)                                                              \n",
            "                                                                 \n",
            " global_max_pooling1d (Globa  (None, 1024)             0         \n",
            " lMaxPooling1D)                                                  \n",
            "                                                                 \n",
            " dense (Dense)               (None, 32)                32800     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 32)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3)                 99        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,209,987\n",
            "Trainable params: 25,209,987\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creates a new model with the same architecture but new weights for experiment \n",
        "#with Uncertainty Sampling\n",
        "model_entro = tf.keras.models.clone_model(model)\n",
        "\n",
        "\n",
        "#compile the model\n",
        "loss= keras.losses.CategoricalCrossentropy(from_logits = False)\n",
        "#small learning rate to get a clear understanding of the learning curve and\n",
        "#better comparability\n",
        "optim = keras.optimizers.Adam(learning_rate=0.00006)\n",
        "metrics = [\"accuracy\",tf.keras.metrics.Precision(),tf.keras.metrics.Recall()]\n",
        "model_entro.compile(loss=loss, optimizer=optim, metrics=metrics)\n"
      ],
      "metadata": {
        "id": "bX-3xN-_Bd9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Different Implementations for calculating the entropy"
      ],
      "metadata": {
        "id": "H3tysFGQb58g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The fastest implementation with simple numpy functions.\n",
        "this implementation (list_entropys3) is faster than the list_entropys1 function (with scpy.stats.entropy) by around 88.9006%:\n",
        "\n",
        "(100/0.0946) * 0.0105 - 100 = -88.9006 %"
      ],
      "metadata": {
        "id": "Asfxx3-6cUEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "fastest implementation with numpy log\n",
        "gets the prediction of the model over the whole set of samples then computes\n",
        "entropy\n",
        "\n",
        "in a trial with 640 samples it took 6.734 seconds, 0.0105s per sample\n",
        "'''\n",
        "def list_entropys3(model, training_pool):\n",
        "\n",
        "  \"\"\" Computes entropy of label distribution. \"\"\"\n",
        "  entropys = []\n",
        "  probs = model.predict(training_pool)\n",
        "  for i in range(0,len(probs)):\n",
        "    entro = 0.\n",
        "   \n",
        "    prob = probs[i]\n",
        "    # Compute entropy\n",
        "   \n",
        "    for j in prob:\n",
        "      entro -= j * np.log(j)\n",
        "\n",
        "    entropys.append(entro)\n",
        "  \n",
        "  return entropys"
      ],
      "metadata": {
        "id": "pLYMRpGnb9um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two slower functions to compute the entropy for the whole dataset."
      ],
      "metadata": {
        "id": "oH0nOyaRcR8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from scipy.stats import entropy\n",
        "from numpy import e,log\n",
        "\n",
        "\n",
        "#the two functions give back the list of the list of entropies of the training_pool in respect to\n",
        "#the model\n",
        "\n",
        "\n",
        "#slower implementation with scipy.stats.entropy \n",
        "#in a trial with 640 samples it took 60.56 seconds, 0.0946s per sample\n",
        "def list_entropys(model, training_pool):\n",
        "  entropys = []\n",
        " \n",
        "  for i in range(0,len(training_pool)):\n",
        "  \n",
        "    entro = entropy(model.predict(training_pool[[i][:]]).reshape(3,))\n",
        "    entropys.append(entro)\n",
        "  return entropys\n",
        "\n",
        "#faster implementation with numpy log\n",
        "#in a trial with 640 samples it took 50.07 seconds, 0.0782s per sample\n",
        "def list_entropys2(model, training_pool):\n",
        "\n",
        "  \"\"\" Computes entropy of label distribution. \"\"\"\n",
        "  entropys = []\n",
        "  for i in range(0,len(training_pool)):\n",
        "    entro = 0.\n",
        "   \n",
        "    probs = model.predict(training_pool[[i][:]]).reshape(3,)\n",
        "    # Compute entropy\n",
        "   \n",
        "    for i in probs:\n",
        "      entro -= i * np.log(i)\n",
        "\n",
        "    entropys.append(entro)\n",
        "  \n",
        "  return entropys\n",
        "\n"
      ],
      "metadata": {
        "id": "IlGVymEMbvYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing execution speed"
      ],
      "metadata": {
        "id": "tXS4EXBPiHcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# '''\n",
        "# Testing 1. implementation of entropy\n",
        "# '''\n",
        "# import copy\n",
        "# training_pool = copy.deepcopy(train_padded)\n",
        "# training_label = copy.deepcopy(train_labels)\n",
        "# #select\n",
        "# training_pool = training_pool[:,:] \n",
        "# training_label = training_label[:,:]\n",
        "\n",
        "# import time\n",
        "# start_time = time.time()\n",
        "# entros = np.array(list_entropys(model_entro, training_pool))\n",
        "# print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "# #entros = np.array(list_entropys(model_entro, training_pool))\n",
        "# print(entros)\n",
        "# #print(training_pool)\n",
        "\n",
        "# #sortes the training pool samples by their entropy value in descending order\n",
        "# arr1inds = entros.argsort()\n",
        "# sorted_arr1 = entros[arr1inds[::-1]]\n",
        "# sorted_arr2 = training_pool[arr1inds[::-1]]\n",
        "\n",
        "# print(sorted_arr1)\n",
        "# print(sorted_arr2)"
      ],
      "metadata": {
        "id": "bURxNJDRh9-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# '''\n",
        "# Testing 2. implementation of entropy\n",
        "# '''\n",
        "\n",
        "# start_time = time.time()\n",
        "# entros2 = np.array(list_entropys2(model_entro, training_pool))\n",
        "# print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "\n",
        "# #print(entros2)\n",
        "\n",
        "# arr1inds = entros2.argsort()\n",
        "# sorted_entropys = entros2[arr1inds[::-1]]\n",
        "# sorted_samples = training_pool[arr1inds[::-1]]\n",
        "\n",
        "# print(sorted_entropys)\n",
        "# print(sorted_samples)\n"
      ],
      "metadata": {
        "id": "Xxj4_SBLiBEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# '''\n",
        "# Testing 3. implementation of entropy test: \n",
        "# '''\n",
        "\n",
        "# start_time = time.time()\n",
        "# entros3 = np.array(list_entropys3(model_entro, training_pool))\n",
        "# print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "\n",
        "# #print(entros2)\n",
        "\n",
        "# arr1inds = entros3.argsort()\n",
        "# sorted_entropys = entros3[arr1inds[::-1]]\n",
        "# sorted_samples = training_pool[arr1inds[::-1]]\n",
        "\n",
        "# print(sorted_entropys)\n",
        "# print(sorted_samples)"
      ],
      "metadata": {
        "id": "5sOQcYvziEz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model "
      ],
      "metadata": {
        "id": "wiQqX6_oimpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "train the model and return the accuracy and val_accuracy\n",
        "\"\"\"\n",
        "\n",
        "def train_model(model, train_padded, train_labels, val_padded, val_labels):\n",
        "\n",
        "  import datetime\n",
        " \n",
        "  # Clear any logs from previous runs\n",
        "  !rm -rf /logs/\n",
        "\n",
        "  #clearing memory to avoid clutter from old runs\n",
        "  tf.keras.backend.clear_session()\n",
        " \n",
        "  ## Hyperparameters\n",
        "  epochs = 30\n",
        "  batchsize = 64\n",
        "  early_stopping = keras.callbacks.EarlyStopping(monitor =\"val_loss\", \n",
        "                                          mode =\"min\", patience=2)\n",
        "\n",
        "  #create the log file for tensorboard\n",
        "  log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "  \n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        " \n",
        "\n",
        "\n",
        "  checkpoint_filepath = '/tmp/checkpoint'\n",
        "  model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "  filepath=checkpoint_filepath,\n",
        "  save_weights_only=True,\n",
        "  monitor='val_accuracy',\n",
        "  mode='max',\n",
        "  save_best_only=True)\n",
        "   \n",
        "  #shuffels dataset before training so that the order isn't always the same and\n",
        "  #model isn't bias towards the first samples more\n",
        "  #then trains with given parameters\n",
        "  history = model.fit(train_padded,\n",
        "                      train_labels,\n",
        "                      epochs=epochs,\n",
        "                      batch_size = batchsize,\n",
        "                      validation_data=(val_padded, val_labels),\n",
        "                      callbacks=[early_stopping,tensorboard_callback,\n",
        "                        model_checkpoint_callback],\n",
        "                      verbose=1, shuffle=True)\n",
        "\n",
        "  model.load_weights(checkpoint_filepath)\n",
        "\n",
        "  return model, history.history['accuracy'][:], history.history['val_accuracy'][:]"
      ],
      "metadata": {
        "id": "0QiMr271i0fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Training-Loop"
      ],
      "metadata": {
        "id": "A_GEPPKJjJ8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#helper function to reset weights after each loop\n",
        "weights = model_entro.get_weights()\n",
        "reset_model = lambda model: model.set_weights(weights)"
      ],
      "metadata": {
        "id": "ObsuPXLNjUmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "###HYPERPARAMETERS\n",
        "x= 2048\n",
        "lam = 0.5\n",
        "x_half = int(x*lam)\n",
        "loops = 10\n",
        "patience = 10\n",
        "\n",
        "\n",
        "# copy the padded training data and training labels for future use\n",
        "training_pool = copy.deepcopy(train_padded)\n",
        "training_label = copy.deepcopy(train_labels)\n",
        "#select\n",
        "training_pool = training_pool[:,:] \n",
        "training_label = training_label[:,:]\n",
        "\n",
        "\n",
        "\n",
        "selected_samples = np.zeros( shape =(0,pad_size), dtype=np.int32)\n",
        "selected_labels  = np.zeros( shape =(0,3), dtype=np.int32)\n",
        "\n",
        "#take the first x random samples from the training pool and the labels to train\n",
        "#the model \n",
        "#also delete the used samples from original training_pool and training_label\n",
        "\n",
        "#512*32= 16384 around half of the samples used for the orginial model\n",
        "#x are the amount of samples that are added in each iteration\n",
        "\n",
        "\n",
        "selected_samples = np.append(selected_samples, training_pool[:x][:], axis=0)\n",
        "training_pool = np.delete(training_pool,slice(0,x) , axis=0)\n",
        "\n",
        "selected_labels = np.append(selected_labels, training_label[:x][:], axis=0)\n",
        "training_label = np.delete(training_label,slice(0,x) , axis=0)\n",
        "\n",
        "\n",
        "'''\n",
        "repeat loops times:\n",
        "  train the model for one loop and calculate the entropy in respect to the trained\n",
        "  model, select 1/2x new samples with highest entropy and 1/2x random samples, reset model and relearn on the \n",
        "  new + old samples\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "val_accuracies = []\n",
        "train_accuracies = []\n",
        "wait = 0\n",
        "best = 1\n",
        "\n",
        "for i in range(loops):\n",
        "\n",
        "  print(f\"\\nIteration :{i+1} - Samples: {(i+1)*x} \\n \")\n",
        "  \n",
        "  #clearing the old model graph and reset all the weights\n",
        "  keras.backend.clear_session()\n",
        "  reset_model(model_entro)\n",
        "  \n",
        "  \n",
        "  model_entro, train_accuracy, val_accuracy = train_model(model_entro,\n",
        "                                                          selected_samples,\n",
        "                                                          selected_labels,\n",
        "                                                          val_padded, val_labels)\n",
        "  \n",
        "  #adding max accuracy from each iteration to the accuracy lists\n",
        "  max_index = np.argmax(val_accuracy)\n",
        "  val_accuracies.append(val_accuracy[max_index])\n",
        "  train_accuracies.append(train_accuracy[max_index])\n",
        "  \n",
        "  '''\n",
        "  Select and add the random samples to the selected samples set\n",
        "  and labels. Then delete the samples from training pool.\n",
        "  '''\n",
        "\n",
        "  #add the 1/2x random samples\n",
        "  random_idx = np.random.randint(np.size(training_pool, axis = 0), size=x_half)\n",
        "  selected_samples = np.append(selected_samples, training_pool[random_idx][:], axis=0)\n",
        "  selected_labels = np.append(selected_labels, training_label[random_idx][:], axis=0)\n",
        "\n",
        "\n",
        "  #delete the selected samples and labels from original pool\n",
        "  training_pool = np.delete(training_pool,random_idx , axis=0)\n",
        "  training_label = np.delete(training_label,random_idx , axis=0)\n",
        "  \n",
        "\n",
        "  \n",
        "  '''\n",
        "  Select and add the samples with highest entropy to the selected samples set\n",
        "  and labels. Then delete the samples from training pool.\n",
        "  '''\n",
        "  #get the entropys and sort samples and labels in order of decending entropy\n",
        "  entros = np.array(list_entropys3(model_entro, training_pool))\n",
        "\n",
        "  #sort the set into partitions with highest x_half entropies in the first x indices\n",
        "  sorted_indices = np.argpartition(entros, -x_half)[::-1]\n",
        "\n",
        "  sorted_entropys = entros[sorted_indices]\n",
        "  training_pool = training_pool[sorted_indices]\n",
        "  training_label = training_label[sorted_indices]\n",
        "\n",
        "\n",
        "  #add the 1/2x samples with the highest entropy values to the selected samples and\n",
        "  #labels\n",
        "\n",
        "  \n",
        "  selected_samples = np.append(selected_samples, training_pool[:x_half][:], axis=0)\n",
        "  selected_labels = np.append(selected_labels, training_label[:x_half][:], axis=0)\n",
        "\n",
        " \n",
        "\n",
        "  #delete the selected samples and labels from orginal pool \n",
        "  training_pool = training_pool[x_half:][:]\n",
        "  training_label = training_label[x_half:][:]\n",
        "\n",
        "  '''\n",
        "  early stopping method that doesn't need labeled test data:\n",
        "  if the uncertainty for the test data does not \n",
        "  decrease for a certain amount of iterations the training stops\n",
        "  '''\n",
        "  #val = val_accuracy[max_index]\n",
        "\n",
        " \n",
        "  mean_entropy = np.mean((list_entropys3(model_entro, val_padded)))\n",
        "\n",
        "  \n",
        "  wait += 1\n",
        "  if mean_entropy < best:\n",
        "    best = mean_entropy\n",
        "    wait = 0\n",
        "  if wait >= patience:\n",
        "    break\n",
        "\n",
        "  \n",
        "  print(f\"Mean entropy for test data: {mean_entropy} - Lowest entropy for test data: {best}\")\n",
        "\n",
        "\n",
        "'''\n",
        "visualize training\n",
        "'''\n",
        "test = plt.figure()\n",
        "plt.plot(train_accuracies)\n",
        "plt.plot(val_accuracies)\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('Iteration')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "\n",
        "result = np.round(np.amax(val_accuracies), decimals = 4)\n",
        "plt.title(f'model accuracy:{result}' )\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VsnSpK-CjVYy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e01d96d8-2416-4d8c-bc66-6b92804e480e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration :1 - Samples: 2048 \n",
            " \n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 10s 206ms/step - loss: 4.6015 - accuracy: 0.4102 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 4.3924 - val_accuracy: 0.4300 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 4s 129ms/step - loss: 4.2052 - accuracy: 0.4126 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 4.0092 - val_accuracy: 0.4300 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 4s 126ms/step - loss: 3.8445 - accuracy: 0.4551 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 3.6619 - val_accuracy: 0.4300 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 5s 156ms/step - loss: 3.5160 - accuracy: 0.4443 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 3.3488 - val_accuracy: 0.4326 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 5s 149ms/step - loss: 3.2161 - accuracy: 0.4536 - precision: 0.5000 - recall: 4.8828e-04 - val_loss: 3.0652 - val_accuracy: 0.4663 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 5s 158ms/step - loss: 2.9396 - accuracy: 0.4819 - precision: 0.5714 - recall: 0.0020 - val_loss: 2.8054 - val_accuracy: 0.4818 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 5s 167ms/step - loss: 2.6949 - accuracy: 0.4883 - precision: 0.8205 - recall: 0.0156 - val_loss: 2.5700 - val_accuracy: 0.5134 - val_precision: 1.0000 - val_recall: 7.8989e-04\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 4s 125ms/step - loss: 2.4635 - accuracy: 0.5225 - precision: 0.8095 - recall: 0.0498 - val_loss: 2.3561 - val_accuracy: 0.5008 - val_precision: 0.8447 - val_recall: 0.0358\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 4s 130ms/step - loss: 2.2467 - accuracy: 0.5571 - precision: 0.7767 - recall: 0.0781 - val_loss: 2.1593 - val_accuracy: 0.5034 - val_precision: 0.7524 - val_recall: 0.0816\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 4s 131ms/step - loss: 2.0482 - accuracy: 0.5854 - precision: 0.7825 - recall: 0.1089 - val_loss: 1.9806 - val_accuracy: 0.5118 - val_precision: 0.7437 - val_recall: 0.0932\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 5s 146ms/step - loss: 1.8646 - accuracy: 0.5952 - precision: 0.8066 - recall: 0.1792 - val_loss: 1.8195 - val_accuracy: 0.5221 - val_precision: 0.7372 - val_recall: 0.1174\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 4s 144ms/step - loss: 1.6895 - accuracy: 0.6333 - precision: 0.8100 - recall: 0.2622 - val_loss: 1.6777 - val_accuracy: 0.5545 - val_precision: 0.7419 - val_recall: 0.1567\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 5s 147ms/step - loss: 1.5203 - accuracy: 0.6836 - precision: 0.8538 - recall: 0.3594 - val_loss: 1.5476 - val_accuracy: 0.5790 - val_precision: 0.7202 - val_recall: 0.2493\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 5s 154ms/step - loss: 1.3543 - accuracy: 0.7227 - precision: 0.8472 - recall: 0.5171 - val_loss: 1.4362 - val_accuracy: 0.5914 - val_precision: 0.7489 - val_recall: 0.3454\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 5s 151ms/step - loss: 1.1891 - accuracy: 0.7661 - precision: 0.8518 - recall: 0.6284 - val_loss: 1.3367 - val_accuracy: 0.6098 - val_precision: 0.7334 - val_recall: 0.4418\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 5s 145ms/step - loss: 1.0337 - accuracy: 0.8037 - precision: 0.8669 - recall: 0.7158 - val_loss: 1.2511 - val_accuracy: 0.6219 - val_precision: 0.7028 - val_recall: 0.5142\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 5s 150ms/step - loss: 0.8766 - accuracy: 0.8354 - precision: 0.8723 - recall: 0.7803 - val_loss: 1.1729 - val_accuracy: 0.6453 - val_precision: 0.7020 - val_recall: 0.5595\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 5s 145ms/step - loss: 0.7431 - accuracy: 0.8667 - precision: 0.8887 - recall: 0.8345 - val_loss: 1.1319 - val_accuracy: 0.6538 - val_precision: 0.7007 - val_recall: 0.5924\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 4s 128ms/step - loss: 0.6158 - accuracy: 0.8960 - precision: 0.9125 - recall: 0.8706 - val_loss: 1.1419 - val_accuracy: 0.6438 - val_precision: 0.6798 - val_recall: 0.6037\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 4s 131ms/step - loss: 0.5325 - accuracy: 0.9141 - precision: 0.9216 - recall: 0.8950 - val_loss: 1.1390 - val_accuracy: 0.6382 - val_precision: 0.6721 - val_recall: 0.6093\n",
            "Mean entropy for test data: 0.7138258032264726 - Lowest entropy for test data: 0.7138258032264726\n",
            "\n",
            "Iteration :2 - Samples: 4096 \n",
            " \n",
            "Epoch 1/30\n",
            "64/64 [==============================] - 7s 95ms/step - loss: 4.5117 - accuracy: 0.3899 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 4.1561 - val_accuracy: 0.4300 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/30\n",
            "64/64 [==============================] - 5s 79ms/step - loss: 3.8496 - accuracy: 0.4065 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 3.5232 - val_accuracy: 0.4300 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/30\n",
            "64/64 [==============================] - 5s 79ms/step - loss: 3.2747 - accuracy: 0.4075 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.9901 - val_accuracy: 0.4300 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/30\n",
            "64/64 [==============================] - 5s 80ms/step - loss: 2.7946 - accuracy: 0.4075 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.5519 - val_accuracy: 0.4300 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/30\n",
            "64/64 [==============================] - 5s 79ms/step - loss: 2.3958 - accuracy: 0.4082 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.1940 - val_accuracy: 0.4300 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 6/30\n",
            "64/64 [==============================] - 5s 80ms/step - loss: 2.0733 - accuracy: 0.4111 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.9079 - val_accuracy: 0.4300 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 7/30\n",
            "64/64 [==============================] - 6s 90ms/step - loss: 1.8160 - accuracy: 0.4131 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.6811 - val_accuracy: 0.4308 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 8/30\n",
            "64/64 [==============================] - 6s 90ms/step - loss: 1.6120 - accuracy: 0.4272 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.5009 - val_accuracy: 0.4310 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 9/30\n",
            "64/64 [==============================] - 6s 89ms/step - loss: 1.4517 - accuracy: 0.4370 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3627 - val_accuracy: 0.4484 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 10/30\n",
            "64/64 [==============================] - 6s 89ms/step - loss: 1.3212 - accuracy: 0.4700 - precision: 0.7143 - recall: 0.0012 - val_loss: 1.2560 - val_accuracy: 0.5016 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 11/30\n",
            "64/64 [==============================] - 6s 91ms/step - loss: 1.2074 - accuracy: 0.5720 - precision: 0.8667 - recall: 0.0190 - val_loss: 1.1614 - val_accuracy: 0.5437 - val_precision: 0.9130 - val_recall: 0.0111\n",
            "Epoch 12/30\n",
            "64/64 [==============================] - 6s 93ms/step - loss: 1.0899 - accuracy: 0.6091 - precision: 0.8274 - recall: 0.1252 - val_loss: 1.0670 - val_accuracy: 0.6201 - val_precision: 0.8446 - val_recall: 0.1316\n",
            "Epoch 13/30\n",
            "64/64 [==============================] - 6s 89ms/step - loss: 0.9740 - accuracy: 0.6667 - precision: 0.8163 - recall: 0.3157 - val_loss: 0.9856 - val_accuracy: 0.6248 - val_precision: 0.7552 - val_recall: 0.3265\n",
            "Epoch 14/30\n",
            "64/64 [==============================] - 6s 92ms/step - loss: 0.8504 - accuracy: 0.7197 - precision: 0.8208 - recall: 0.4919 - val_loss: 0.9128 - val_accuracy: 0.6417 - val_precision: 0.7364 - val_recall: 0.4531\n",
            "Epoch 15/30\n",
            "64/64 [==============================] - 6s 88ms/step - loss: 0.7279 - accuracy: 0.7793 - precision: 0.8445 - recall: 0.6472 - val_loss: 0.8514 - val_accuracy: 0.6611 - val_precision: 0.7258 - val_recall: 0.5324\n",
            "Epoch 16/30\n",
            "64/64 [==============================] - 6s 89ms/step - loss: 0.6014 - accuracy: 0.8333 - precision: 0.8816 - recall: 0.7561 - val_loss: 0.8038 - val_accuracy: 0.6790 - val_precision: 0.7273 - val_recall: 0.6024\n",
            "Epoch 17/30\n",
            "64/64 [==============================] - 6s 88ms/step - loss: 0.4877 - accuracy: 0.8757 - precision: 0.9021 - recall: 0.8301 - val_loss: 0.7840 - val_accuracy: 0.6954 - val_precision: 0.7329 - val_recall: 0.6422\n",
            "Epoch 18/30\n",
            "64/64 [==============================] - 6s 90ms/step - loss: 0.3792 - accuracy: 0.9150 - precision: 0.9306 - recall: 0.8936 - val_loss: 0.8158 - val_accuracy: 0.6977 - val_precision: 0.7198 - val_recall: 0.6582\n",
            "Epoch 19/30\n",
            "64/64 [==============================] - 6s 89ms/step - loss: 0.2896 - accuracy: 0.9463 - precision: 0.9567 - recall: 0.9326 - val_loss: 0.8327 - val_accuracy: 0.7027 - val_precision: 0.7263 - val_recall: 0.6751\n",
            "Mean entropy for test data: 0.48765941472952007 - Lowest entropy for test data: 0.48765941472952007\n",
            "\n",
            "Iteration :3 - Samples: 6144 \n",
            " \n",
            "Epoch 1/30\n",
            "96/96 [==============================] - 8s 75ms/step - loss: 4.3547 - accuracy: 0.3813 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 3.8316 - val_accuracy: 0.4300 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/30\n",
            "96/96 [==============================] - 7s 69ms/step - loss: 3.4298 - accuracy: 0.3887 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 3.0016 - val_accuracy: 0.4318 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/30\n",
            "96/96 [==============================] - 6s 64ms/step - loss: 2.7067 - accuracy: 0.4067 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.3721 - val_accuracy: 0.4300 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/30\n",
            "96/96 [==============================] - 7s 69ms/step - loss: 2.1682 - accuracy: 0.4080 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.9161 - val_accuracy: 0.5255 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/30\n",
            "96/96 [==============================] - 6s 63ms/step - loss: 1.7821 - accuracy: 0.4261 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.5982 - val_accuracy: 0.4394 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 6/30\n",
            "96/96 [==============================] - 7s 70ms/step - loss: 1.5147 - accuracy: 0.4461 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3802 - val_accuracy: 0.5319 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 7/30\n",
            "96/96 [==============================] - 6s 64ms/step - loss: 1.3336 - accuracy: 0.4897 - precision: 1.0000 - recall: 1.6276e-04 - val_loss: 1.2297 - val_accuracy: 0.5295 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 8/30\n",
            "96/96 [==============================] - 6s 68ms/step - loss: 1.2074 - accuracy: 0.5076 - precision: 0.8966 - recall: 0.0127 - val_loss: 1.1272 - val_accuracy: 0.5508 - val_precision: 0.8167 - val_recall: 0.0516\n",
            "Epoch 9/30\n",
            "96/96 [==============================] - 7s 71ms/step - loss: 1.1105 - accuracy: 0.5516 - precision: 0.8182 - recall: 0.0483 - val_loss: 1.0494 - val_accuracy: 0.5816 - val_precision: 0.7454 - val_recall: 0.0856\n",
            "Epoch 10/30\n",
            "96/96 [==============================] - 7s 70ms/step - loss: 1.0154 - accuracy: 0.6081 - precision: 0.8337 - recall: 0.1094 - val_loss: 0.9692 - val_accuracy: 0.6156 - val_precision: 0.7834 - val_recall: 0.1885\n",
            "Epoch 11/30\n",
            "96/96 [==============================] - 7s 71ms/step - loss: 0.8942 - accuracy: 0.7041 - precision: 0.8592 - recall: 0.3058 - val_loss: 0.8769 - val_accuracy: 0.6532 - val_precision: 0.7765 - val_recall: 0.4144\n",
            "Epoch 12/30\n",
            "96/96 [==============================] - 7s 71ms/step - loss: 0.7257 - accuracy: 0.7842 - precision: 0.8753 - recall: 0.5715 - val_loss: 0.7867 - val_accuracy: 0.6898 - val_precision: 0.7551 - val_recall: 0.5611\n",
            "Epoch 13/30\n",
            "96/96 [==============================] - 7s 69ms/step - loss: 0.5510 - accuracy: 0.8485 - precision: 0.8916 - recall: 0.7632 - val_loss: 0.7333 - val_accuracy: 0.7170 - val_precision: 0.7588 - val_recall: 0.6535\n",
            "Epoch 14/30\n",
            "96/96 [==============================] - 7s 69ms/step - loss: 0.3999 - accuracy: 0.8988 - precision: 0.9223 - recall: 0.8631 - val_loss: 0.7423 - val_accuracy: 0.7346 - val_precision: 0.7580 - val_recall: 0.7017\n",
            "Epoch 15/30\n",
            "96/96 [==============================] - 7s 70ms/step - loss: 0.2938 - accuracy: 0.9422 - precision: 0.9531 - recall: 0.9232 - val_loss: 0.7962 - val_accuracy: 0.7375 - val_precision: 0.7506 - val_recall: 0.7148\n",
            "Mean entropy for test data: 0.41868778712973953 - Lowest entropy for test data: 0.41868778712973953\n",
            "\n",
            "Iteration :4 - Samples: 8192 \n",
            " \n",
            "Epoch 1/30\n",
            "128/128 [==============================] - 9s 62ms/step - loss: 4.2051 - accuracy: 0.3593 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 3.5551 - val_accuracy: 0.4300 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/30\n",
            "128/128 [==============================] - 7s 58ms/step - loss: 3.0792 - accuracy: 0.3770 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.5963 - val_accuracy: 0.4776 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/30\n",
            "128/128 [==============================] - 8s 59ms/step - loss: 2.2903 - accuracy: 0.3885 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.9572 - val_accuracy: 0.4900 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/30\n",
            "128/128 [==============================] - 7s 55ms/step - loss: 1.7789 - accuracy: 0.3936 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.5660 - val_accuracy: 0.4863 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/30\n",
            "128/128 [==============================] - 7s 55ms/step - loss: 1.4700 - accuracy: 0.4082 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3308 - val_accuracy: 0.4824 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 6/30\n",
            "128/128 [==============================] - 7s 58ms/step - loss: 1.2928 - accuracy: 0.4214 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.1966 - val_accuracy: 0.5250 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 7/30\n",
            "128/128 [==============================] - 7s 53ms/step - loss: 1.1938 - accuracy: 0.4390 - precision: 1.0000 - recall: 1.2207e-04 - val_loss: 1.1176 - val_accuracy: 0.5211 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 8/30\n",
            "128/128 [==============================] - 8s 59ms/step - loss: 1.1306 - accuracy: 0.4659 - precision: 0.7931 - recall: 0.0028 - val_loss: 1.0680 - val_accuracy: 0.5427 - val_precision: 1.0000 - val_recall: 0.0024\n",
            "Epoch 9/30\n",
            "128/128 [==============================] - 8s 59ms/step - loss: 1.0795 - accuracy: 0.5045 - precision: 0.8266 - recall: 0.0250 - val_loss: 1.0192 - val_accuracy: 0.5598 - val_precision: 0.7488 - val_recall: 0.0800\n",
            "Epoch 10/30\n",
            "128/128 [==============================] - 8s 59ms/step - loss: 1.0211 - accuracy: 0.5560 - precision: 0.7885 - recall: 0.0701 - val_loss: 0.9609 - val_accuracy: 0.5940 - val_precision: 0.7557 - val_recall: 0.1401\n",
            "Epoch 11/30\n",
            "128/128 [==============================] - 8s 60ms/step - loss: 0.9311 - accuracy: 0.6296 - precision: 0.8217 - recall: 0.2053 - val_loss: 0.8946 - val_accuracy: 0.6393 - val_precision: 0.7554 - val_recall: 0.3294\n",
            "Epoch 12/30\n",
            "128/128 [==============================] - 8s 60ms/step - loss: 0.7985 - accuracy: 0.7224 - precision: 0.8436 - recall: 0.4530 - val_loss: 0.8005 - val_accuracy: 0.6690 - val_precision: 0.7616 - val_recall: 0.5208\n",
            "Epoch 13/30\n",
            "128/128 [==============================] - 7s 58ms/step - loss: 0.6439 - accuracy: 0.8004 - precision: 0.8626 - recall: 0.6616 - val_loss: 0.7190 - val_accuracy: 0.7264 - val_precision: 0.7697 - val_recall: 0.6414\n",
            "Epoch 14/30\n",
            "128/128 [==============================] - 7s 58ms/step - loss: 0.4842 - accuracy: 0.8689 - precision: 0.8990 - recall: 0.8063 - val_loss: 0.6731 - val_accuracy: 0.7580 - val_precision: 0.7811 - val_recall: 0.7185\n",
            "Epoch 15/30\n",
            "128/128 [==============================] - 7s 59ms/step - loss: 0.3476 - accuracy: 0.9160 - precision: 0.9316 - recall: 0.8945 - val_loss: 0.6941 - val_accuracy: 0.7583 - val_precision: 0.7750 - val_recall: 0.7409\n",
            "Epoch 16/30\n",
            "128/128 [==============================] - 8s 59ms/step - loss: 0.2656 - accuracy: 0.9415 - precision: 0.9494 - recall: 0.9285 - val_loss: 0.7335 - val_accuracy: 0.7588 - val_precision: 0.7749 - val_recall: 0.7467\n",
            "Mean entropy for test data: 0.39082905453425176 - Lowest entropy for test data: 0.39082905453425176\n",
            "\n",
            "Iteration :5 - Samples: 10240 \n",
            " \n",
            "Epoch 1/30\n",
            "160/160 [==============================] - 10s 56ms/step - loss: 4.0462 - accuracy: 0.3756 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 3.2692 - val_accuracy: 0.4300 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/30\n",
            "160/160 [==============================] - 8s 52ms/step - loss: 2.7318 - accuracy: 0.3781 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.2170 - val_accuracy: 0.4350 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/30\n",
            "160/160 [==============================] - 8s 49ms/step - loss: 1.9206 - accuracy: 0.3858 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.6169 - val_accuracy: 0.4339 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/30\n",
            "160/160 [==============================] - 8s 53ms/step - loss: 1.4754 - accuracy: 0.3938 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3007 - val_accuracy: 0.5145 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/30\n",
            "160/160 [==============================] - 8s 50ms/step - loss: 1.2525 - accuracy: 0.4096 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.1480 - val_accuracy: 0.4774 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 6/30\n",
            "160/160 [==============================] - 8s 52ms/step - loss: 1.1464 - accuracy: 0.4252 - precision: 1.0000 - recall: 0.0013 - val_loss: 1.0698 - val_accuracy: 0.5305 - val_precision: 1.0000 - val_recall: 7.8989e-04\n",
            "Epoch 7/30\n",
            "160/160 [==============================] - 9s 53ms/step - loss: 1.0848 - accuracy: 0.4766 - precision: 0.7667 - recall: 0.0157 - val_loss: 1.0158 - val_accuracy: 0.5387 - val_precision: 0.8190 - val_recall: 0.0453\n",
            "Epoch 8/30\n",
            "160/160 [==============================] - 8s 53ms/step - loss: 1.0276 - accuracy: 0.5196 - precision: 0.7693 - recall: 0.0632 - val_loss: 0.9585 - val_accuracy: 0.5669 - val_precision: 0.7620 - val_recall: 0.1256\n",
            "Epoch 9/30\n",
            "160/160 [==============================] - 9s 54ms/step - loss: 0.9464 - accuracy: 0.5891 - precision: 0.7954 - recall: 0.1818 - val_loss: 0.8881 - val_accuracy: 0.6272 - val_precision: 0.7443 - val_recall: 0.2852\n",
            "Epoch 10/30\n",
            "160/160 [==============================] - 9s 54ms/step - loss: 0.8145 - accuracy: 0.6889 - precision: 0.8271 - recall: 0.4037 - val_loss: 0.7870 - val_accuracy: 0.6825 - val_precision: 0.7596 - val_recall: 0.5066\n",
            "Epoch 11/30\n",
            "160/160 [==============================] - 9s 54ms/step - loss: 0.6451 - accuracy: 0.7869 - precision: 0.8606 - recall: 0.6328 - val_loss: 0.6934 - val_accuracy: 0.7209 - val_precision: 0.7687 - val_recall: 0.6459\n",
            "Epoch 12/30\n",
            "160/160 [==============================] - 8s 53ms/step - loss: 0.4889 - accuracy: 0.8505 - precision: 0.8873 - recall: 0.7878 - val_loss: 0.6415 - val_accuracy: 0.7541 - val_precision: 0.7818 - val_recall: 0.7196\n",
            "Epoch 13/30\n",
            "160/160 [==============================] - 9s 53ms/step - loss: 0.3675 - accuracy: 0.9047 - precision: 0.9198 - recall: 0.8746 - val_loss: 0.6244 - val_accuracy: 0.7722 - val_precision: 0.7891 - val_recall: 0.7517\n",
            "Epoch 14/30\n",
            "160/160 [==============================] - 8s 53ms/step - loss: 0.2800 - accuracy: 0.9322 - precision: 0.9421 - recall: 0.9182 - val_loss: 0.6395 - val_accuracy: 0.7823 - val_precision: 0.7914 - val_recall: 0.7694\n",
            "Epoch 15/30\n",
            "160/160 [==============================] - 9s 54ms/step - loss: 0.2197 - accuracy: 0.9508 - precision: 0.9573 - recall: 0.9417 - val_loss: 0.6588 - val_accuracy: 0.7883 - val_precision: 0.7970 - val_recall: 0.7775\n",
            "Mean entropy for test data: 0.35663623843514186 - Lowest entropy for test data: 0.35663623843514186\n",
            "\n",
            "Iteration :6 - Samples: 12288 \n",
            " \n",
            "Epoch 1/30\n",
            "192/192 [==============================] - 11s 52ms/step - loss: 3.9016 - accuracy: 0.3633 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 3.0161 - val_accuracy: 0.4300 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/30\n",
            "192/192 [==============================] - 9s 49ms/step - loss: 2.4567 - accuracy: 0.3826 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.9439 - val_accuracy: 0.4592 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/30\n",
            "192/192 [==============================] - 10s 50ms/step - loss: 1.6749 - accuracy: 0.3958 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.4057 - val_accuracy: 0.4921 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/30\n",
            "192/192 [==============================] - 9s 47ms/step - loss: 1.3100 - accuracy: 0.4224 - precision: 0.9091 - recall: 8.1380e-04 - val_loss: 1.1637 - val_accuracy: 0.4742 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/30\n",
            "192/192 [==============================] - 9s 49ms/step - loss: 1.1524 - accuracy: 0.4515 - precision: 0.7711 - recall: 0.0178 - val_loss: 1.0549 - val_accuracy: 0.4939 - val_precision: 0.7336 - val_recall: 0.0885\n",
            "Epoch 6/30\n",
            "192/192 [==============================] - 10s 50ms/step - loss: 1.0654 - accuracy: 0.5004 - precision: 0.7203 - recall: 0.0761 - val_loss: 0.9855 - val_accuracy: 0.5592 - val_precision: 0.7477 - val_recall: 0.1506\n",
            "Epoch 7/30\n",
            "192/192 [==============================] - 10s 50ms/step - loss: 0.9822 - accuracy: 0.5669 - precision: 0.7517 - recall: 0.1671 - val_loss: 0.9183 - val_accuracy: 0.6095 - val_precision: 0.7261 - val_recall: 0.2464\n",
            "Epoch 8/30\n",
            "192/192 [==============================] - 9s 49ms/step - loss: 0.8622 - accuracy: 0.6590 - precision: 0.8006 - recall: 0.3551 - val_loss: 0.8263 - val_accuracy: 0.6614 - val_precision: 0.7522 - val_recall: 0.4547\n",
            "Epoch 9/30\n",
            "192/192 [==============================] - 9s 49ms/step - loss: 0.7017 - accuracy: 0.7480 - precision: 0.8330 - recall: 0.5732 - val_loss: 0.7315 - val_accuracy: 0.7075 - val_precision: 0.7628 - val_recall: 0.6072\n",
            "Epoch 10/30\n",
            "192/192 [==============================] - 10s 50ms/step - loss: 0.5477 - accuracy: 0.8258 - precision: 0.8720 - recall: 0.7463 - val_loss: 0.6660 - val_accuracy: 0.7399 - val_precision: 0.7706 - val_recall: 0.6996\n",
            "Epoch 11/30\n",
            "192/192 [==============================] - 10s 50ms/step - loss: 0.4161 - accuracy: 0.8822 - precision: 0.9027 - recall: 0.8486 - val_loss: 0.6448 - val_accuracy: 0.7715 - val_precision: 0.7863 - val_recall: 0.7459\n",
            "Epoch 12/30\n",
            "192/192 [==============================] - 10s 50ms/step - loss: 0.3192 - accuracy: 0.9206 - precision: 0.9327 - recall: 0.9032 - val_loss: 0.6412 - val_accuracy: 0.7846 - val_precision: 0.7944 - val_recall: 0.7670\n",
            "Epoch 13/30\n",
            "192/192 [==============================] - 10s 51ms/step - loss: 0.2506 - accuracy: 0.9437 - precision: 0.9502 - recall: 0.9339 - val_loss: 0.6620 - val_accuracy: 0.7912 - val_precision: 0.7987 - val_recall: 0.7791\n",
            "Epoch 14/30\n",
            "192/192 [==============================] - 9s 47ms/step - loss: 0.2022 - accuracy: 0.9576 - precision: 0.9627 - recall: 0.9523 - val_loss: 0.6923 - val_accuracy: 0.7899 - val_precision: 0.7940 - val_recall: 0.7794\n",
            "Mean entropy for test data: 0.3644647264206122 - Lowest entropy for test data: 0.35663623843514186\n",
            "\n",
            "Iteration :7 - Samples: 14336 \n",
            " \n",
            "Epoch 1/30\n",
            "224/224 [==============================] - 12s 49ms/step - loss: 3.7629 - accuracy: 0.3658 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.7867 - val_accuracy: 0.4308 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/30\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 2.2144 - accuracy: 0.3800 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.7093 - val_accuracy: 0.4476 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/30\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 1.4894 - accuracy: 0.4046 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.2641 - val_accuracy: 0.5263 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/30\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 1.2095 - accuracy: 0.4326 - precision: 0.7941 - recall: 0.0019 - val_loss: 1.0979 - val_accuracy: 0.5284 - val_precision: 1.0000 - val_recall: 0.0013\n",
            "Epoch 5/30\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 1.1046 - accuracy: 0.4626 - precision: 0.7097 - recall: 0.0169 - val_loss: 1.0231 - val_accuracy: 0.5521 - val_precision: 0.8136 - val_recall: 0.0598\n",
            "Epoch 6/30\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 1.0362 - accuracy: 0.5154 - precision: 0.7217 - recall: 0.0738 - val_loss: 0.9500 - val_accuracy: 0.5837 - val_precision: 0.7576 - val_recall: 0.1630\n",
            "Epoch 7/30\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.9462 - accuracy: 0.5839 - precision: 0.7643 - recall: 0.2131 - val_loss: 0.8623 - val_accuracy: 0.6372 - val_precision: 0.7628 - val_recall: 0.3565\n",
            "Epoch 8/30\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.8067 - accuracy: 0.6858 - precision: 0.8104 - recall: 0.4251 - val_loss: 0.7570 - val_accuracy: 0.6862 - val_precision: 0.7659 - val_recall: 0.5419\n",
            "Epoch 9/30\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6254 - accuracy: 0.7931 - precision: 0.8521 - recall: 0.6597 - val_loss: 0.6508 - val_accuracy: 0.7414 - val_precision: 0.7792 - val_recall: 0.6885\n",
            "Epoch 10/30\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.4626 - accuracy: 0.8641 - precision: 0.8876 - recall: 0.8142 - val_loss: 0.5882 - val_accuracy: 0.7786 - val_precision: 0.7953 - val_recall: 0.7530\n",
            "Epoch 11/30\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.3482 - accuracy: 0.9076 - precision: 0.9185 - recall: 0.8867 - val_loss: 0.5610 - val_accuracy: 0.7949 - val_precision: 0.8067 - val_recall: 0.7812\n",
            "Epoch 12/30\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.2691 - accuracy: 0.9325 - precision: 0.9395 - recall: 0.9215 - val_loss: 0.5792 - val_accuracy: 0.7965 - val_precision: 0.8062 - val_recall: 0.7899\n",
            "Epoch 13/30\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.2125 - accuracy: 0.9500 - precision: 0.9557 - recall: 0.9430 - val_loss: 0.6001 - val_accuracy: 0.8025 - val_precision: 0.8099 - val_recall: 0.7962\n",
            "Mean entropy for test data: 0.3263614313933127 - Lowest entropy for test data: 0.3263614313933127\n",
            "\n",
            "Iteration :8 - Samples: 16384 \n",
            " \n",
            "Epoch 1/30\n",
            "256/256 [==============================] - 13s 46ms/step - loss: 3.6284 - accuracy: 0.3709 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.5722 - val_accuracy: 0.4300 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/30\n",
            "256/256 [==============================] - 11s 44ms/step - loss: 2.0038 - accuracy: 0.3822 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.5325 - val_accuracy: 0.4934 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/30\n",
            "256/256 [==============================] - 11s 45ms/step - loss: 1.3521 - accuracy: 0.3971 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.1687 - val_accuracy: 0.4966 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/30\n",
            "256/256 [==============================] - 11s 45ms/step - loss: 1.1453 - accuracy: 0.4215 - precision: 1.0000 - recall: 3.6621e-04 - val_loss: 1.0571 - val_accuracy: 0.5290 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/30\n",
            "256/256 [==============================] - 11s 44ms/step - loss: 1.0732 - accuracy: 0.4559 - precision: 0.7754 - recall: 0.0200 - val_loss: 1.0009 - val_accuracy: 0.5484 - val_precision: 0.7421 - val_recall: 0.0803\n",
            "Epoch 6/30\n",
            "256/256 [==============================] - 11s 44ms/step - loss: 1.0188 - accuracy: 0.5082 - precision: 0.7442 - recall: 0.0778 - val_loss: 0.9299 - val_accuracy: 0.5814 - val_precision: 0.7607 - val_recall: 0.2059\n",
            "Epoch 7/30\n",
            "256/256 [==============================] - 11s 45ms/step - loss: 0.9322 - accuracy: 0.5861 - precision: 0.7586 - recall: 0.2304 - val_loss: 0.8470 - val_accuracy: 0.6474 - val_precision: 0.7540 - val_recall: 0.3810\n",
            "Epoch 8/30\n",
            "256/256 [==============================] - 11s 44ms/step - loss: 0.7902 - accuracy: 0.6917 - precision: 0.8056 - recall: 0.4504 - val_loss: 0.7277 - val_accuracy: 0.7035 - val_precision: 0.7636 - val_recall: 0.5698\n",
            "Epoch 9/30\n",
            "256/256 [==============================] - 11s 44ms/step - loss: 0.6065 - accuracy: 0.7925 - precision: 0.8506 - recall: 0.6793 - val_loss: 0.6199 - val_accuracy: 0.7609 - val_precision: 0.7945 - val_recall: 0.7185\n",
            "Epoch 10/30\n",
            "256/256 [==============================] - 11s 45ms/step - loss: 0.4294 - accuracy: 0.8716 - precision: 0.8963 - recall: 0.8333 - val_loss: 0.5761 - val_accuracy: 0.7917 - val_precision: 0.8074 - val_recall: 0.7715\n",
            "Epoch 11/30\n",
            "256/256 [==============================] - 11s 44ms/step - loss: 0.3092 - accuracy: 0.9172 - precision: 0.9301 - recall: 0.9040 - val_loss: 0.5799 - val_accuracy: 0.7978 - val_precision: 0.8118 - val_recall: 0.7880\n",
            "Epoch 12/30\n",
            "256/256 [==============================] - 12s 45ms/step - loss: 0.2415 - accuracy: 0.9409 - precision: 0.9480 - recall: 0.9326 - val_loss: 0.6065 - val_accuracy: 0.8004 - val_precision: 0.8081 - val_recall: 0.7917\n",
            "Mean entropy for test data: 0.3316148360319207 - Lowest entropy for test data: 0.3263614313933127\n",
            "\n",
            "Iteration :9 - Samples: 18432 \n",
            " \n",
            "Epoch 1/30\n",
            "288/288 [==============================] - 14s 45ms/step - loss: 3.5024 - accuracy: 0.3772 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.3813 - val_accuracy: 0.4402 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/30\n",
            "288/288 [==============================] - 12s 43ms/step - loss: 1.8344 - accuracy: 0.3873 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3890 - val_accuracy: 0.4658 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/30\n",
            "288/288 [==============================] - 12s 43ms/step - loss: 1.2561 - accuracy: 0.4202 - precision: 0.7143 - recall: 0.0022 - val_loss: 1.0961 - val_accuracy: 0.5153 - val_precision: 1.0000 - val_recall: 0.0018\n",
            "Epoch 4/30\n",
            "288/288 [==============================] - 12s 43ms/step - loss: 1.0919 - accuracy: 0.4545 - precision: 0.6740 - recall: 0.0366 - val_loss: 0.9993 - val_accuracy: 0.5326 - val_precision: 0.7500 - val_recall: 0.0987\n",
            "Epoch 5/30\n",
            "288/288 [==============================] - 12s 43ms/step - loss: 1.0146 - accuracy: 0.5021 - precision: 0.6974 - recall: 0.1010 - val_loss: 0.9325 - val_accuracy: 0.5716 - val_precision: 0.7186 - val_recall: 0.1930\n",
            "Epoch 6/30\n",
            "288/288 [==============================] - 12s 43ms/step - loss: 0.9136 - accuracy: 0.5884 - precision: 0.7519 - recall: 0.2587 - val_loss: 0.8385 - val_accuracy: 0.6393 - val_precision: 0.7479 - val_recall: 0.3968\n",
            "Epoch 7/30\n",
            "288/288 [==============================] - 13s 44ms/step - loss: 0.7584 - accuracy: 0.7015 - precision: 0.7973 - recall: 0.5017 - val_loss: 0.7091 - val_accuracy: 0.7114 - val_precision: 0.7747 - val_recall: 0.6093\n",
            "Epoch 8/30\n",
            "288/288 [==============================] - 12s 43ms/step - loss: 0.5781 - accuracy: 0.8007 - precision: 0.8467 - recall: 0.7195 - val_loss: 0.6068 - val_accuracy: 0.7725 - val_precision: 0.8020 - val_recall: 0.7380\n",
            "Epoch 9/30\n",
            "288/288 [==============================] - 12s 43ms/step - loss: 0.4347 - accuracy: 0.8698 - precision: 0.8887 - recall: 0.8354 - val_loss: 0.5599 - val_accuracy: 0.7930 - val_precision: 0.8102 - val_recall: 0.7757\n",
            "Epoch 10/30\n",
            "288/288 [==============================] - 12s 43ms/step - loss: 0.3396 - accuracy: 0.9052 - precision: 0.9155 - recall: 0.8898 - val_loss: 0.5565 - val_accuracy: 0.8033 - val_precision: 0.8168 - val_recall: 0.7915\n",
            "Epoch 11/30\n",
            "288/288 [==============================] - 13s 44ms/step - loss: 0.2695 - accuracy: 0.9272 - precision: 0.9338 - recall: 0.9192 - val_loss: 0.5640 - val_accuracy: 0.8136 - val_precision: 0.8209 - val_recall: 0.8038\n",
            "Epoch 12/30\n",
            "288/288 [==============================] - 12s 41ms/step - loss: 0.2170 - accuracy: 0.9457 - precision: 0.9497 - recall: 0.9400 - val_loss: 0.5907 - val_accuracy: 0.8123 - val_precision: 0.8187 - val_recall: 0.8062\n",
            "Mean entropy for test data: 0.35907647336957715 - Lowest entropy for test data: 0.3263614313933127\n",
            "\n",
            "Iteration :10 - Samples: 20480 \n",
            " \n",
            "Epoch 1/30\n",
            "320/320 [==============================] - 15s 44ms/step - loss: 3.3779 - accuracy: 0.3823 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.2024 - val_accuracy: 0.4071 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/30\n",
            "320/320 [==============================] - 13s 42ms/step - loss: 1.6839 - accuracy: 0.3926 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.2841 - val_accuracy: 0.5055 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/30\n",
            "320/320 [==============================] - 13s 42ms/step - loss: 1.1889 - accuracy: 0.4148 - precision: 0.5455 - recall: 2.9297e-04 - val_loss: 1.0597 - val_accuracy: 0.5205 - val_precision: 1.0000 - val_recall: 7.8989e-04\n",
            "Epoch 4/30\n",
            "320/320 [==============================] - 13s 42ms/step - loss: 1.0688 - accuracy: 0.4509 - precision: 0.6853 - recall: 0.0246 - val_loss: 0.9844 - val_accuracy: 0.5403 - val_precision: 0.7694 - val_recall: 0.0861\n",
            "Epoch 5/30\n",
            "320/320 [==============================] - 13s 42ms/step - loss: 1.0021 - accuracy: 0.5016 - precision: 0.6992 - recall: 0.1128 - val_loss: 0.9135 - val_accuracy: 0.5874 - val_precision: 0.7441 - val_recall: 0.2320\n",
            "Epoch 6/30\n",
            "320/320 [==============================] - 13s 42ms/step - loss: 0.9080 - accuracy: 0.5874 - precision: 0.7342 - recall: 0.2808 - val_loss: 0.8187 - val_accuracy: 0.6511 - val_precision: 0.7615 - val_recall: 0.4279\n",
            "Epoch 7/30\n",
            "320/320 [==============================] - 13s 42ms/step - loss: 0.7565 - accuracy: 0.7023 - precision: 0.7930 - recall: 0.5125 - val_loss: 0.6907 - val_accuracy: 0.7325 - val_precision: 0.7764 - val_recall: 0.6209\n",
            "Epoch 8/30\n",
            "320/320 [==============================] - 14s 42ms/step - loss: 0.5742 - accuracy: 0.8078 - precision: 0.8457 - recall: 0.7326 - val_loss: 0.5763 - val_accuracy: 0.7862 - val_precision: 0.8104 - val_recall: 0.7486\n",
            "Epoch 9/30\n",
            "320/320 [==============================] - 13s 42ms/step - loss: 0.4233 - accuracy: 0.8716 - precision: 0.8885 - recall: 0.8439 - val_loss: 0.5327 - val_accuracy: 0.8094 - val_precision: 0.8223 - val_recall: 0.7907\n",
            "Epoch 10/30\n",
            "320/320 [==============================] - 14s 43ms/step - loss: 0.3174 - accuracy: 0.9091 - precision: 0.9203 - recall: 0.8976 - val_loss: 0.5355 - val_accuracy: 0.8117 - val_precision: 0.8222 - val_recall: 0.7999\n",
            "Epoch 11/30\n",
            "320/320 [==============================] - 13s 40ms/step - loss: 0.2509 - accuracy: 0.9351 - precision: 0.9426 - recall: 0.9268 - val_loss: 0.5656 - val_accuracy: 0.8104 - val_precision: 0.8191 - val_recall: 0.8033\n",
            "Mean entropy for test data: 0.39702330217740867 - Lowest entropy for test data: 0.3263614313933127\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU1fn48c+TPYEkZGNLgIDsi4JGRIWKdcMVtLbi0qpdbGutfm2txbZWS9tv/XXftJX6dWmrUouKtKKICoorBEHZ9y2BQEISErKQZZ7fH+cGhjjAAJncLM/79ZrXzL333JlnJnCfe8659xxRVYwxxpjmovwOwBhjTNtkCcIYY0xIliCMMcaEZAnCGGNMSJYgjDHGhGQJwhhjTEiWIEy7IiJPisjPwiy7VUQujHRMxnRUliCM6cBE5AIRWSsi1SKyQET6HaXsaBFZJCL7RKRARO4P2hYnIrO8pKsiMrHZvneLyGYRqRCRnSLyOxGJaVbmLhHZIiJVIrJGRAa3+Bc2LcoShDE+aH7wjNBnZAIvAPcD6UA+8K+j7PIM8LZX9jzgdhG5Kmj7O8BNQFGIfecAp6tqCjASOA24MyiWrwJfAS4HugJXACUn9MVMq7EEYVqcd5b5PRH5xDtb/D8R6SEir4hIpYi8LiJpQeWvEpFVIlIuIgtFZFjQtjEi8pG337+AhGafdYWILPf2fU9ETg0zxstFZJl3xrtDRB5stn28937l3vZbvPWJIvIbEdnmnWm/462bKCIFIX6HC73XD3pn4P8UkQrgFhEZKyLve5+xS0T+LCJxQfuPEJH5IlIqIrtF5Aci0tOrDWQElTtdRIpFJLbZ17wGWKWq/1bVWuBB4DQRGXqEnyUXeFpVG1V1Ey4hjABQ1TpV/b2qvgM0Nt9RVTepanlTSEAAGOjFFwU8ANytqqvV2aSqpUeIw7QRliBMpHwOuAgYDFwJvAL8AMjC/bu7E8BrZngW+B9v21zgP16TRhwwG/gH7qz239774u07Bngc+DqQATwKzBGR+DDiqwK+BHTDndV+U0SmeO/bz4v3T15Mo4Hl3n6/Bs4AzvFiuhd3MAzHZGCW95lP4w60dwOZwNnABcDtXgzJwOvAq0Bv3MH2DVUtAhYCXwh63y8CM1W13ks24731I4CPmwqpahWwyVsfyu+BL4lIrIgM8WJ6Pczvhojc4CW/ElwN4lFvU473GOkl2y0i8hMvcZg2zP5AJlL+pKq7VbUQWAR8qKrLvDPZF4ExXrnrgJdVdb6q1uMOwIm4A/A4IBb4varWq+osYEnQZ9wGPKqqH3pnvU8BB7z9jkpVF6rqClUNqOonuCR1nrf5BuB1VX3W+9y9qrrcO6B9GbhLVQu9z3xPVQ+E+Zu8r6qzvc+sUdWlqvqBqjao6lbcAbUphiuAIlX9jarWqmqlqn7obXsK19SDiEQD1+OSKKrazTvLB9eUs69ZDPuA5CPE91/gWqAGWAv8n6ouOULZT1HVZ7wmpsHAX4Hd3qYc7/liYBRwvhfzV8J9b+MPSxAmUnYHva4JsdzVe90b2Na0QVUDwA4g29tWqIePKLkt6HU/4LveWXO5iJQDfbz9jkpEzvI6bYtFZB/wDdyZPN57bAqxWyauiSvUtnDsaBbDYBH5r4gUeWfe/xtGDAAvAcNFpD+ulrZPVReHKLcfSGm2LgWobF5QRNJxtZXpuO/YB7hERG4P65sFUdUNwCrgEW9Vjff8S1UtD0qGlx3ve5vWZQnC+G0n7kAPgIgI7uBUCOwCsr11TfoGvd4B/Nw7a256JKnqs2F87jO4jtU+qpqKO+Nt+pwdwCkh9ikBao+wrQpICvoe0bjmqWDNh07+C+5MfZB35v2DZjEMCBW4Vwt7DleL+CJe7SGEVbimnqaYunixrwpRdgDQqKp/92o0BcBMTvwgHsOh32kdUMfh39+GkW4HLEEYvz0HXC7ucsxY4Lu4ZqL3gPeBBuBOr138GmBs0L5/A77h1QZERLp4nc9HakIJlgyUqmqtiIzFNSs1eRq4UES+ICIxIpIhIqO92s3jwG9FpLeIRIvI2V6fx3ogwfv8WOBHwLH6QpKBCmC/13H8zaBt/wV6icj/iEi8iCSLyFlB2/8O3AJcxZETxIu4dv/PiUgC8GPgE1VdG6Lselx+vkFEokSkJ67575OmAl4cTRcJxIlIQlPyFpGvikh37/Vw4D7gDQBVrcZdPXWv9z1ycM2D/z3G72N8ZgnC+EpV1+HOhP+EO0O/ErjSu2qmDnclzi1AKe6A9ULQvvnA14A/A2XARq9sOG4HpotIJe7A+VzQ+27HnTl/1/vc5Rw6E78HWIHrCykF/h8Qpar7vPd8DFf7qQIOu6ophHtwiakSl+wOXoKqqpW45qMrcZeVbsC13TdtfxfXOf6Rqh5sdhOR/SIywStTjOvU/znu9zkLmBpU9q8i8levbAXut77bK7scWAkE35S4DtdclA3M81431f7OBVaISBXuQoO5uBpRkztwTV47cYn/GVyyNW2Y2IRBxrRPIvIm8IyqPuZ3LKZjsgRhTDskImcC83F9KJ/qdDamJVgTkzHtjIg8hbs/4X8sOZhIshqEMcaYkKwGYYwxJqSIDxjWWjIzMzU3N9fvMIwxpl1ZunRpiao2v2cH6EAJIjc3l/z8fL/DMMaYdkVEth1pmzUxGWOMCckShDHGmJAsQRhjjAmpw/RBhFJfX09BQQG1tbV+hxJxCQkJ5OTkEBvbfM4YY4w5MR06QRQUFJCcnExubi6HDwjasagqe/fupaCggP79+/sdjjGmg+jQTUy1tbVkZGR06OQAICJkZGR0ipqSMab1RDRBiMgkEVknIhtFZFqI7f1E5A1xcxcv9IYBbtrWKG6u4eUiMuckYjjRXduVzvI9jTGtJ2JNTN6EKQ/jhiwuAJaIyBxVXR1U7NfA31X1KRH5LPAL3AQoADWqOjpS8RlzNKVVdazZVcGaXRWICLkZSfTLSCInLYmE2Gi/wzOmVUSyD2IssFFVNwOIyEzcpO3BCWI48B3v9QLcBPUdSnl5Oc888wy33358MzdedtllPPPMM3Tr1i1CkRmA+sYAW0qqWLOrgtW7Kli7q5I1uyrYUxl6mmkR6J2aSD8vYfTL6OIljy70y0giKa59d+s1BpQ9lbUUlNVQWFZDQVk1BWU1brm8hj0VtWSnJTK0ZwpDeyUztGcyQ3um0Cs1wWqxHVAk/zVnc/gcvAW4CUuCfYybpOQPwNVAsohkqOpe3Oxc+bgZxR5S1U8lDxG5DTczFX379m2+uU0oLy/nkUce+VSCaGhoICbmyD//3LlzIx1ap1Pm1QpW76pgbZFLBBt276euMQBAbLQwsHsy4wdmMqxX0wEwhegoYeveKrbtrWLb3mq27a1m694q5q3aTWlV3WGfkZUcfyhhpCfRL/NQAklN9P8Ks8aAUlRRS0FpNYXlNd7Bv/pgAthZXkN94+EDeGZ2jSM7LYnhvVM4b3AWO0qrWbqtjDkf7zxYJiUhJihppDCkZzJDeibTNb59J8zOzu+/3j3An0XkFuBt3Excjd62fqpaKCIDgDdFZIWqHjaJu6rOAGYA5OXltclhaadNm8amTZsYPXo0sbGxJCQkkJaWxtq1a1m/fj1Tpkxhx44d1NbWcuttt3Pzl79KTJQwevhgFr3/AbXVVUy+8grGjx/Pe++9R3Z2Ni+99BKJiYl+f7U2q8GrFQQngjW7KthdcahWkNk1nmG9krnl3FyGeQe1U7K6EhcTulsuvUscp/dN+9T6itp6tnsJwyWPKrburWbRhmJmVRxeC0lLiqVvUI0jN6gWktElrkXOwBsaA+zaV/vpg39ZDQXl1ewqr6UhcPh/lazkeHLSEjk1pxuXjepFdrdEctISyUlLIrtbIolxoZvU9tXUs353JWuLKlnr/dYvfFTI/gOHRm7om57k1TKSGdorhaE9k+mX0YXoKKtttAcRG+5bRM4GHlTVS7zl+wBU9RdHKN8VWKuqOSG2PQn8V1VnHenz8vLytPlYTGvWrGHYsGEA/OQ/q1i9s+LEvswRDO+dwgNXjjhqma1bt3LFFVewcuVKFi5cyOWXX87KlSsPXo5aWlpKeno6G3eVMmniuTw+62W6paVz6dmn8szLC6iuquLKCaczc+4CRow6je9+/RYunHQZ1069gZgoISZKiI4SYqKFLRvX07VHP9KS4uiWFNcp/hOWV9exelcFa3a5g9SaogrW795PXYOrFcRECQO7d2VYrxSG9Up2NYOeKWQlH2u66JNXU9fI9tLqg7WPrXurDyaTneU1BB+nu8bHHLHZqkdyAlHe37K+McCu8loKyg81/RSUVXvNQTUUVdTSGPTGItA9OZ6ctCTvoJ9IdrdDr3t3S2zRPhVVpaCshrVFlawrqmCNlzy2lFQd/L4JsVEM7uGSxpCeKQzzkkd6l7gWi8OET0SWqmpeqG2RrEEsAQaJSH9czWAqh08Mj4hk4iaOD+AmOX/cW58GVKvqAa/MucAvIxhrqxk7duxh9yr88Y9/5PkXXqCuIcCeXTuJqihiyNB+xERF0TctiX0xDfTtl8u5Y/NoCCijRo9hx/btVNc10NioNAYl+OLKOqY8/TbgDgzdEmNJ7xIX4hFPepdY95wUR3rXONKT4o54ptgWNDQG2Lq3itVNicBLCkUVhy7tzewax7BeKdx8dr+DiWBg9yPXCiItMS76YFNLcwcaGikoqzms9rF1bxVrd1Xy2qrdh53lx8dE0Sc9ieoDDRRV1B6WWESgV0oC2WmJjO2f7iWAxIMJoVe3BOJjWu/vKiL0SU+iT3oSFw3vcXB9bX0jG/fsZ41X01hXVMkba/bwXP6habuzkuMZ2tMl8SE9khnaK5mB3bu2avzmcBFLEKraICJ34CY3jwYeV9VVIjIdyFfVOcBE4Bciorgmpm95uw8DHhWRAO5S3IeaXf103I51pt9aunTpcvD1woULef311/nn7PnEJyby9euvoqG+jvjYaEQgOTEWaYwnKTGBXt1ck1JWSiL79zcytGcKAIGA0hhQGgIBGkvj+MPU0ZRV1VFaVUdptfdcVceWkiqWbiunrLrusDPMYImx0aR3iSMlMfZgzeTgQ1wtJUpcrSUq6vDnaGlWPmi/6Ghv/xD7Nb2fKx9FdBQHn8ur6w8mgvW7KznQrFZw9ikZBw8oQ3sl0z05IcJ/vZYTHxPNKVldOSWr66e2NTUTbT1Y66hie2k1XeJj3IE/qAmoZ2qCbwnweCTERjMyO5WR2amHrS+uPMC6okrWFrm/87rdFTz53taDNcDoKGFAZpeDzVNNTVW9rVO8VUS0D0JV5wJzm637cdDrWcCnmo1U9T1gVCRjay3JyclUVoaeFXLfvn0kJacicfHs372NDz/44LjfP8o72MYSRUJsNJOHZR+1fCCgVNY2sLfqAGXVdezdX+eeq+ooq3LPFTUNNAYCNASUgCoNjS4BHWhwyajRWxdQdWUCzZ699Y3NHs3bvsOR0cXVCr44rp/XTJTCKd27dOizypjoqINn4RMG+R1NZGUlx5OVHM/4QZkH1zXVFl3fhksey7aX8Z+gTvHUxFjOHpDB+EGZfGZQFn0zkvwIv8Pzu5O6w8vIyODcc89l5MiRJCYm0qPHoWr3xAsu4le//zPXfHYcI4cPZdy4cRGPJypKSE2KJTXJnytqAl6CCU4azRNL03KX+GiyusbbmWInExMdxcDuyQzsnswVpx5aX1Fbz/oi1yn+8Y5y3t1YwqurigDXGT5+UCYTBmZyzimZvv377mg6zJzUx+qkbmtUlc3FVdQ2NDK4RzKx0SffTNCWv68xLU1V2VxSxTsbSli0oYQPNu9l/4EGogROzenGhEGZjB+YyZi+ae2iGc4vfnVSm6Moraqjqq6BnLSkFkkOxnQ2InKwH+fmc3KpbwywfEc5izaU8M6GYh5ZuIk/vbmRpLhoxg3IYPzATCYMymRg965WKw2TJQgf1DcEKNpXS9f4GNKsKmxMi4iNjuLM3HTOzE3nOxcNpqK2nvc37WXRhmLe2VDCm2v3ANAzJcE1Rw3K5NyBmWR2jfwlz+2VJYhWpqoUltegQHZaop3JGBMhKQmxXDKiJ5eM6AnAjtJq3tlYwjsbSnh9zW5mLXWX2A7rlXKwOWps/3QbayuIJYhWVlFTT0VtPb1SW/f6dGM6uz7pSVw/ti/Xj+1LY0BZWbiPdzaWsGhDMU+8u4UZb28mLiaKsbnpjPcSxvBeKQdvUuyMLEG0ooZAgMJ9tSTGRlu11hgfRUcJp/Xpxml9uvGt8wdSXdfAh1tKvQ7vYh56ZS3gLrM+x+u7mDAok16pnWuIG0sQrahoXy2NjUr/jCRrWjKmDUmKi+H8Id05f0h3AHZX1PLOhhKvhlFy8B6MU7K6MGFQFuMHZjLulIwOPxhhx/52bcj+Aw2UVtWRlRxP4lGGhO7atSv79+9vxciMMc31SEngc2fk8LkzclBV1u2uZNH6EhZtLGHmku08+d5WYqKEvNw0rjotm8tH9eqQ915YgmgFgYBSWFZDXEwUPdrRcBDGGHc57dCebmyvr31mALX1jXy0rYxFG0uYt6qIH7y4ggfnrGLikCymjMnms0O7d5iObksQETZt2jRSM3ty+dRb6J/ZhenTf0JMTAwLFiygrKyM+vp6fvaznzF58mS/QzXGhCEhNppzBmZyzsBM7r1kCCsLK5i9vJA5H+/ktdW7SY6PYdLInlw9JpuzBmS061GVO8+d1K9Mg6IVLfuhPUfBpQ8dtcj7i/P59p138eLc+fRJT2L48OHMmzeP1NRUUlJSKCkpYdy4cWzYsAEROakmJruT2hj/NAaU9zaVMHvZTuatKmL/gQZ6pMRz1Wm9mTw6mxG9U9pk36PdSe0TVSUrdyhle0vQqlI+3rGBtLQ0evbsyd13383bb79NVFQUhYWF7N69m549e/odsjHmBEVHCRMGZTFhUBY/rx/J62t2M3vZTp54dyt/W7SFQd27MmVMNled1ps+6e1jcMHOkyCOcaYfCXur6qiua+Bz117L7BdfoKioiOuuu46nn36a4uJili5dSmxsLLm5udTW1h77DY0x7UJCbDRXnNqbK07tTVlVHS+v2MVLywv51bx1/GreOvL6pTF5TDZXjOpFWhueKKnzJIhWVtfQSNG+WpITYvnSjddz2223UVJSwltvvcVzzz1H9+7diY2NZcGCBWzbtu3Yb2iMaZfSusRx07h+3DSuHztKq5nz8U5mLyvk/tkr+YnXuT15dDYXDuvR5ibtsgQRAW44DVcjyO6WQFzmSCorK8nOzqZXr17ceOONXHnllYwaNYq8vDyGDh3qc8TGmNbQJz2Jb50/kNsnnsLqXRW8tHwnLy0v5PU1e+gSF82kkb2YMqY355yS2SY6ty1BRMC+mnoqa+vplZpInDecxooVhzrIMzMzef/990Pua/dAGNPxiQgjeqcyoncq3580lA8372X28kJeWVHE8x8VkJUcz5Wn9ubqMdmMzPavc9sSRAtraAyws7yWpLhoMru23bZFY0zbEB0lBy+bnT55JAvW7uHFZYX884NtPP7uFgZkdWHK6GymjM5u9ZnzLEG0sF37amkMKNndurTJS9qMMW1XQmw0l47qxaWjerGvup65K3cxe1khv52/nt/OX8+Yvt24eoy7czujFcZz6/AJQlVb7UBdWVtPWXUd3ZPjW72zqaPcz2KMcVKTYg+OPltYXsMcr7/ixy+t4if/Wc1nBmUyZUw2Fw3vQdJRhu85GR36RrktW7aQnJxMRkZGxJNEIKCs31OJIAzq3rVVhwhWVfbu3UtlZSX9+/dvtc81xrS+tUUVzF62kznLC9m5zzVnTxmTzf9ePeqE3q/T3iiXk5NDQUEBxcXFEf8s1zHdQFZyHOvKWv9StYSEBHJyclr9c40xrWtozxSmXZrCvZcMYfHWUl5aXkikzkc7dIKIjY1tlTPqlYX7uPEf7/L5M3J46JyREf88Y4yJihLGDchg3ICMyH1GxN65k2hoDPD95z8hLSmO+y61cZCMMR1HRBOEiEwSkXUislFEpoXY3k9E3hCRT0RkoYjkBG27WUQ2eI+bIxnnyXj83S2s2lnB9MkjOuR48MaYzitiCUJEooGHgUuB4cD1IjK8WbFfA39X1VOB6cAvvH3TgQeAs4CxwAMikhapWE/U9r3V/Hb+ei4c1oNLR9pAe8aYjiWSNYixwEZV3ayqdcBMoPmkB8OBN73XC4K2XwLMV9VSVS0D5gOTIhjrcVNVfvDiCmKiovjplBF2z4MxpsOJZILIBnYELRd464J9DFzjvb4aSBaRjDD3RURuE5F8EclvjSuVgr3wUSHvbCzh+5OGdLqJzI0xnYPfndT3AOeJyDLgPKAQaAx3Z1Wdoap5qpqXlZUVqRg/pWT/AX768mrO6JfGjWf1a7XPNcaY1hTJy1wLgT5ByzneuoNUdSdeDUJEugKfU9VyESkEJjbbd2EEYz0uP/3vaqoONPDQNaNa9YY4Y4xpTZGsQSwBBolIfxGJA6YCc4ILiEimiDTFcB/wuPd6HnCxiKR5ndMXe+t8t2DdHl5avpPbJw5kUI9kv8MxxpiIiViCUNUG4A7cgX0N8JyqrhKR6SJylVdsIrBORNYDPYCfe/uWAj/FJZklwHRvna+qDjTwoxdXMrB7V24//xS/wzHGmIiK6J3UqjoXmNts3Y+DXs8CZh1h38c5VKNoE3792joKy2uY9Y2ziY9pWzM/GWNMS/O7k7rdWLa9jCff28oXx/UjLzfd73CMMSbiLEGEob4xwH0vrKBHcgL3ThridzjGGNMqOvRgfS1lxtubWVtUyd++lEdygg2nYYzpHKwGcQybi/fzhzc2cNmonlw0vIff4RhjTKuxBHEUgYBy3wsriI+J4sErR/gdjjHGtCpLEEfxXP4OPtxSyg8vG0b3lAS/wzHGmFZlCeII9lTU8vO5azirfzrXndnn2DsYY0wHYwniCB78zyoONAT4xTWjbKRWY0ynZAkihNdWFTF3RRF3XTCIAVld/Q7HGGN8YQmimYraeu5/aSVDeyZz22cG+B2OMcb4xu6DaOaXr65lT+UBHv1iHrHRlj+NMZ2XHQGD5G8t5Z8fbOeWc3IZ3aeb3+EYY4yvLEF4DjQ0Mu2FFWR3S+Sei204DWOMsSYmzyMLNrFxz36euPVMusTbz2KMMVaDADbsruSRhRuZPLo35w/p7nc4xhjTJnT6BBEIKNNeWEGX+Bjuv2K43+EYY0yb0ekTxLbSaraWVHH/5cPJ7BrvdzjGGNNmdPrG9v6ZXXjznomkJHT6n8IYYw5jR0UgNdHmeDDGmOY6fROTMcaY0CxBGGOMCckShDHGmJAsQRhjjAnJEoQxxpiQIpogRGSSiKwTkY0iMi3E9r4iskBElonIJyJymbc+V0RqRGS59/hrJOM0xhjzaRG7zFVEooGHgYuAAmCJiMxR1dVBxX4EPKeqfxGR4cBcINfbtklVR0cqPmOMMUcXyRrEWGCjqm5W1TpgJjC5WRkFUrzXqcDOCMZjjDHmOEQyQWQDO4KWC7x1wR4EbhKRAlzt4dtB2/p7TU9viciEUB8gIreJSL6I5BcXF7dg6MYYY/zupL4eeFJVc4DLgH+ISBSwC+irqmOA7wDPiEhK851VdYaq5qlqXlZWVqsGbowxHV0kE0Qh0CdoOcdbF+wrwHMAqvo+kABkquoBVd3rrV8KbAIGRzBWY4wxzUQyQSwBBolIfxGJA6YCc5qV2Q5cACAiw3AJolhEsrxObkRkADAI2BzBWI0xxjQTsauYVLVBRO4A5gHRwOOqukpEpgP5qjoH+C7wNxG5G9dhfYuqqoh8BpguIvVAAPiGqpZGKlZjjDGfJqrqdwwtIi8vT/Pz8/0Owxhj2hURWaqqeaG2+d1JbYwxpo2yBGGMMSYkSxDGGGNCsgRhjDEmJEsQxhhjQrIEYYwxJqSwEoSIvCAil3vDYBhjjOkEwj3gPwLcAGwQkYdEZEgEYzLGGNMGhJUgVPV1Vb0ROB3YCrwuIu+JyK0iEhvJAI0xxvgj7CYjEckAbgG+CiwD/oBLGPMjEpkxxhhfhTUWk4i8CAwB/gFcqaq7vE3/EhEb38IYYzqgcAfr+6OqLgi14UhjeBhjjGnfwm1iGi4i3ZoWRCRNRG6PUEzGGGPagHATxNdUtbxpQVXLgK9FJiRjjDFtQbgJIlpEpGnBm8wnLjIhGWOMaQvC7YN4Fdch/ai3/HVvnTHGmA4q3ATxfVxS+Ka3PB94LCIRGWOMaRPCShCqGgD+4j2MMcbU7oOybRCTALEJEJMIsd4jKtrv6FpEuPdBDAJ+AQwHEprWq+qACMVljDFtS30NbP8AtrztHjs/Ag2ELhsVC7FJXuJIOJQ4YhLdutik0IklJiFov+Zlg8sElY2Jh0NdxC0q3CamJ4AHgN8B5wO3YiPBGmM6ssZ6KPwItrzlEsKOD6GxDqJiIPsMmHAP9BzpytXXuEdDDdTXes9N62qhvtpbXwu1FVC5+1DZ+mq3vqH2BAMV6DsOvtzy3cLhJohEVX1DRERVtwEPishS4MctHpExxvghEIDdK1wy2PwWbHsP6qsAgZ6jYOxtMGCiOxjHJ0fm85sSxacSS/OE0ywhde3e8vEQfoI44A31vUFE7gAKga4RicgYY1qDKpRsOFRD2LoIasrctszBMPp66P8ZyJ0ASemRjycqCuKS3KONCDdB3AUkAXcCP8U1M90cqaCMMSYiynccSghb3oZKb1i51D4w5HKXEPpPgJTe/sbZRhwzQXg3xV2nqvcA+3H9D8YY0/btL4atXpPRlrehbItbn5TpksGA89xzWv+IdfS2Z8dMEKraKCLjT+TNRWQSbljwaOAxVX2o2fa+wFNAN6/MNFWd6227D/gK0AjcqarzTiQGY0wnUlPu+g6aagl7Vrv18SmQOx7O+jr0Pw+6D7OEEIZwm5iWicgc4N9AVdNKVX3hSDt4NY+HgYuAAmCJiMxR1dVBxX4EPKeqfxGR4cBcINd7PRUYAfTGTVA0WFUbj+O7GWNOVqARitfCjsVQsASKPgHEXV4Zl+RdknmU1+GUiz6JOcfqqmHHB4c6lnctd5eexiS6zuRRn3e1hJ6nQXS4h8CYoh8AABWuSURBVDvTJNxfLAHYC3w2aJ0CR0wQwFhgo6puBhCRmcBkIDhBKJDivU4FdnqvJwMzVfUAsEVENnrv936Y8RpjTkRNGRTkewlhMRQshbpKty0pA3qPcZd51lV5l2sWuStt6qq95yrcf+vjEBV7/Amm4YCrKRQsPnTpac6Z8JnvuSajnDPd/QHmpIR7J/WJ9DtkAzuClguAs5qVeRB4TUS+DXQBLgza94Nm+2afQAzGmCMJBFztoGAx7FjinkvWu20SBT1GwKlfgD5j3QE3fcCxm2VU3cG7vvrwxNH89dG2HUw2+2H/nqB9alwC0kZAoNepcNY3XJNR33EQbxdWtrRw76R+ghCnBar65ZP8/OuBJ1X1NyJyNvAPERkZ7s4ichtwG0Dfvn1PMhRjOriaclc7KFjsagiFS+FAhduWmO4SwanXuefep5/YAVfEu/s3AYjApaGqrsagAXc3sYmocJuY/hv0OgG4mkPNQUdSCPQJWs7x1gX7CjAJQFXfF5EEIDPMfVHVGcAMgLy8vOOs1xrTgQUCrjbQlAx2LIaSdW6bREH34TDqWsgZ6xJCOLWDtkDEmo5aUbhNTM8HL4vIs8A7x9htCTBIRPrjDu5TgRualdkOXAA8KSLDcMmnGJgDPCMiv8V1Ug8CFocTqzGdUu0+r3awxKsd5Lt1AIlprolo1Oehz5lumIhI3AlsOpwT7dYfBBz13m5VbfDuup6Hu4T1cVVdJSLTgXxVnQN8F/ibiNyNa8K6RVUVWCUiz+E6tBuAb9kVTMZ4AgHYu+FQR/KOJa4vAQXE1Q5GXH2odpAxsH3UDkybI+54fIxCIpUc3gdRBNzXvGbhp7y8PM3Pz/c7DGNOXmODu3KotgIOVLp+gppyKFrhXVm05FDtIKGbqx00dSRnnwEJKUd/f2OCiMhSVc0LtS3cJiarjxpzLKruSpsD3oG9tgIO7At67R3sayu8MsHlgl7XVx3hA8Td4DV8ipcQvNpBlA2sbCIj3KuYrgbeVNV93nI3YKKqzo5kcMb4riAfdi47dHAPdUAPPtgHGo79nrFd3Fl+fLK7wzc+GVKzvdcph287+DoVMgdCQmrkv7MxnnD7IB5Q1RebFlS1XEQeACxBmI6pfDu8dj+sDvonHhXz6YN2ao637K0L3nZwOWhbXLLd0WvajXD/pYaqw9q/ctPx1FXDu7+Hd/8ACEy8D06/GRK7uRm8rLPXdCLhHuTzvUtOH/aWvwUsjUxIxvhAFVa9AK/9GCoKYMQ1cNF06Nbn2Psa00GFmyC+DdwP/At3NdN8XJIwpv3b9TG8Mg22v+dmDrtmBuSe63dUxvgu3KuYqoBpEY7FmNZVVQJv/hSWPuVmDLvi93D6lyAq2u/IjGkTwr2KaT7weVUt95bTcKOtXhLJ4IyJiMZ6WPw3WPiQu6R03DfhvHvdHcfGmIPCbWLKbEoOAKpaJiKRmSXbmEja+Dq8ep8bp+iUz8KkhyBriN9RGdMmhZsgAiLSV1W3A4hILsc96LsxPtq7Ceb9ENa/4gamu34mDJ5kVyUZcxThJogfAu+IyFuAABPwhtk2pk2rrYBFv4b3H3GjgF74E9ekZCOCGnNM4XZSvyoiebiksAx3g1xNJAMz5qQEAvDxs/DGT2D/bhh9I1zwACT38DsyY9qNcDupvwrchZuXYTkwDjf952ePtp8xvtixBF65F3Z+5Aawu/5ZN4idMea4hNvEdBdwJvCBqp4vIkOB/41cWMacgIpd8PqD8MlM6NoTrn4URn3BBrMz5gSFmyBqVbVWRBCReFVdKyJ26YdpG+pr4YOH4e3fQKAexn8HJnzX5ig25iSFmyAKvBFcZwPzRaQM2Ba5sIwJgyqsfRle+yGUbYWhV8DFP4P0/n5HZkyHEG4n9dXeywdFZAGQCrwasaiMOZY9a+CV78OWtyBrGHxxNpxyvt9RGdOhHPeIrKr6ViQCMSYs1aWw8Bew5P/cENqX/gryvmxDaBsTAfa/yrQPjQ3w0ZPw5s+hthzOuBXO/yF0yfA7MmM6LEsQpu3bsghenQa7V0LuBDc8Rs+RfkdlTIdnCcK0XWXbYP79sPolSO0LX/g7DLvKhscwppVYgjBtz95NsOQxyH8cENeUdM63ITbR78iM6VQsQZi2IdDoRlpdPMM9R8XAyGvhgvvdvM/GmFZnCcL4q7oUlv3DXZVUvs3dAT3xB3DGzZDc0+/ojOnULEEYf+xcBosfg5WzoKEW+o2Hi37ibnaLjvU7OmMMEU4QIjIJ+AMQDTymqg812/47oOnupiSgu6p287Y1Aiu8bdtV9apIxmpaQcMBWDUblvwNCpZAbBcYfQOc+VXoMcLv6IwxzUQsQYhINPAwcBFQACwRkTmqurqpjKreHVT+28CYoLeoUdXRkYrPtKJ9Ba7DeelTUF0CGQNh0v+D0ddDQqrf0RljjiCSNYixwEZV3QwgIjOBycDqI5S/HngggvGY1qTqhsFY/DdYN9etG3wpjP0q9J9oI6wa0w5EMkFkAzuClguAs0IVFJF+QH/gzaDVCSKSDzQAD6nq7BD73YY3s13fvn1bKGxzUmor4OOZ7jLVknWQmA7n3uWGw+hmfyNj2pO20kk9FZilqo1B6/qpaqGIDADeFJEVqropeCdVnQHMAMjLy7M5sv20Z63rW/h4JtTth96nw5S/woirITbB7+iMMScgkgmiEOgTtJzjrQtlKvCt4BWqWug9bxaRhbj+iU2f3tX4prEB1r3smpG2LoLoeBh5DZz5NcixGdyMae8imSCWAINEpD8uMUwFbmheyJudLg03hWnTujSgWlUPiEgmcC7wywjGao7H/j2uw3npE1BRCKl94MIHYcyXbPA8YzqQiCUIVW0QkTuAebjLXB9X1VUiMh3IV9U5XtGpwExVDW4iGgY8KiIBIArXB3Gkzm3TGlTdpamLZ7hLVQP1MOB8uOzXMPgSiIr2O0JjTAuTw4/L7VdeXp7m5+f7HUbHU1ftbmZb/Dco+gTiUw7du5A5yO/ojDEnSUSWqmpeqG1tpZPatDWlm93wF8v+6eZf6D4crvgdjPqCzfVsTCdhCcIcbtcn8OZPYcN8kCgYdiWMvQ36nWPDbBvTyViCME5jA7zzO3jrIUjoBufdC2fcAim9/Y7MGOMTSxAGitfD7G9A4VI3xPZlv4KkdL+jMsb4zBJEZxYIwOJH4fUH3WQ81z7h7mMwxhgsQXRe5dth9u3uBrdBl8BVf7T5F4wxh7EE0dmouiuTXr0PULjqzzDmJuuANsZ8iiWIzqRyN/znTlj/qpugZ8ojkNbP76iMMW2UJYjOYtVs+O/dUF8Nl/wCzvqGDbltjDkqSxAdXXUpvHIvrPi3G2H16r9C1hC/ozLGtAOWIDqyDa/DnDugqhjO/yGM/w5E25/cGBMeO1p0RAf2w2s/cqOtZg2F62dCb5u91RhzfCxBdDTb3oPZ34SybXDOna7mYBP2GGNOgCWIjqK+Fhb8DN77s7sy6da5bvwkY4w5QZYgOoKdy+HFb0DxGjjjVrj4ZzbiqjHmpFmCaM8a62HRb+HtX0KXLLjxeRh0od9RGWM6CEsQ7VXxOnjx67BzGYz6vBtgLzHN76iMMR2IJYj2JhCAD/8Kb/wEYpPg80/BiCl+R2WM6YAsQbQnZdvcAHvb3oHBk+DKP0JyD7+jMsZ0UJYg2gNVWPYPb4A9sQH2jDGtwhJEW1dZBHPuhA3zIHeCG2CvW1+/ozLGdAKWINqylS/Ay9+B+hqY9BCM/boNsGeMaTWWINqi6lKYew+sfB6yz4Apf4WswX5HZYzpZCxBtDUb5sNLd0B1CZz/Ixh/tw2wZ4zxRUTbK0RkkoisE5GNIjItxPbfichy77FeRMqDtt0sIhu8x82RjLNNqK+B/9wFT18LSenwtTfhvO9ZcjDG+CZiRx8RiQYeBi4CCoAlIjJHVVc3lVHVu4PKfxsY471OBx4A8gAFlnr7lkUqXl/VVcGzU2HLIjj3LjfAXky831EZYzq5SNYgxgIbVXWzqtYBM4HJRyl/PfCs9/oSYL6qlnpJYT4wKYKx+qe2Av5xDWx9B65+FC6absnBGNMmRDJBZAM7gpYLvHWfIiL9gP7Am8ezr4jcJiL5IpJfXFzcIkG3qupS+PtkKMyHa5+A067zOyJjjDmorVwzORWYpaqNx7OTqs5Q1TxVzcvKyopQaBFSVQJPXQW7V8J1/7ThMowxbU4kE0Qh0CdoOcdbF8pUDjUvHe++7U9lETxxGezdCDf8C4Zc6ndExhjzKZFMEEuAQSLSX0TicElgTvNCIjIUSAPeD1o9D7hYRNJEJA242FvX/pXvgCcuhYpCuGkWnPJZvyMyxpiQInYVk6o2iMgduAN7NPC4qq4SkelAvqo2JYupwExV1aB9S0Xkp7gkAzBdVUsjFWurKd3impVq98EXX4Q+Y/2OyBhjjkiCjsvtWl5enubn5/sdxpGVbICnroSGAy459B7td0TGGIOILFXVvFDb7C6s1rB7lbtaCeCWl6HHcH/jMcaYMLSVq5g6rp3L4cnLISoWbn3FkoMxpt2wBBFJOxa7Pof4ZLh1LmQO8jsiY4wJmyWISNn6DvzjauiS4WoO6f39jsgYY46LJYhI2PgG/PNaSM1xySE1x++IjDHmuFmCaGnrXnED72UMdB3SyT39jsgYY06IJYiWtOpF+NdN0GMk3DwHumT6HZExxpwwSxAt5eN/wawvQ86Z8KWX3JwOxhjTjlmCaAlLn4IXvw654+Gm5yEhxe+IjDHmpFmCOFkfzoD/3AkDL4QbnoO4Ln5HZIwxLcISxMl49w/wyvdg6BUw9WmITfQ7ImOMaTE21MaJUIW3/h8s/AWM/JybCS461u+ojDGmRVmCOF6q8PqD8O7vYfSNcNWfICra76iMMabFWYI4Hqrw6jT48K+Q9xW47NcQZa10xpiOyRJEuAIBePluWPokjPsWXPJzEPE7KmOMiRhLEOFobICXvgWfzIQJ98Bnf2TJwRjT4VmCOJbGenj+q7B6tksMn/me3xEZY0yrsARxNA0H4N+3wLq5cPHP4Zw7/I7IGGNajSWII6mrhn/dCJvehMt/A2d+1e+IjDGmVVmCCOXAfjci69Z3YPLDMOYmvyMyxphWZwmiudp9bi6HwqXwucdg1LV+R2SMMb6wBBGsutTNArd7FXzhKRh2pd8RGWOMbyxBNNm/B/4+BfZuhKnPwOCL/Y7IGGN8ZQkCoGIn/H0y7CuAG5+DARP9jsgYY3wX0XEiRGSSiKwTkY0iMu0IZb4gIqtFZJWIPBO0vlFElnuPORELcl8hPHEpVOxyczkMmBixjzLGmPYkYjUIEYkGHgYuAgqAJSIyR1VXB5UZBNwHnKuqZSLSPegtalR1dKTiOyghFbKGwufuhZwzIv5xxhjTXkSyiWkssFFVNwOIyExgMrA6qMzXgIdVtQxAVfdEMJ7Q4rvCDf9q9Y81xpi2LpJNTNnAjqDlAm9dsMHAYBF5V0Q+EJFJQdsSRCTfWz8l1AeIyG1emfzi4uKWjd4YYzo5vzupY4BBwEQgB3hbREapajnQT1ULRWQA8KaIrFDVTcE7q+oMYAZAXl6etm7oxhjTsUWyBlEI9AlazvHWBSsA5qhqvapuAdbjEgaqWug9bwYWAmMiGKsxxphmIpkglgCDRKS/iMQBU4HmVyPNxtUeEJFMXJPTZhFJE5H4oPXncnjfhTHGmAiLWBOTqjaIyB3APCAaeFxVV4nIdCBfVed42y4WkdVAI/A9Vd0rIucAj4pIAJfEHgq++skYY0zkiWrHaLrPy8vT/Px8v8Mwxph2RUSWqmpeqG02obIxxpiQLEEYY4wJqcM0MYlIMbDtJN4iEyhpoXDaO/stDme/x+Hs9zikI/wW/VQ1K9SGDpMgTpaI5B+pHa6zsd/icPZ7HM5+j0M6+m9hTUzGGGNCsgRhjDEmJEsQh8zwO4A2xH6Lw9nvcTj7PQ7p0L+F9UEYY4wJyWoQxhhjQrIEYYwxJqROnyDCmRa1sxCRPiKyIGgK2Lv8jslvIhItIstE5L9+x+I3EekmIrNEZK2IrBGRs/2OyU8icrf3/2SliDwrIgl+x9TSOnWCCJoW9VJgOHC9iAz3NypfNQDfVdXhwDjgW5389wC4C1jjdxBtxB+AV1V1KHAanfh3EZFs4E4gT1VH4gYknepvVC2vUycIgqZFVdU6oGla1E5JVXep6kfe60rcAaD5LICdhojkAJcDj/kdi99EJBX4DPB/AKpa503s1ZnFAIkiEgMkATt9jqfFdfYEEc60qJ2SiOTiJmn60N9IfPV74F4g4HcgbUB/oBh4wmtye0xEuvgdlF+8Cc1+DWwHdgH7VPU1f6NqeZ09QZgQRKQr8DzwP6pa4Xc8fhCRK4A9qrrU71jaiBjgdOAvqjoGqAI6bZ+diKThWhv6A72BLiJyk79RtbzOniDCmRa1UxGRWFxyeFpVX/A7Hh+dC1wlIltxTY+fFZF/+huSrwqAAlVtqlHOwiWMzupCYIuqFqtqPfACcI7PMbW4zp4gwpkWtdMQEcG1Ma9R1d/6HY+fVPU+Vc1R1Vzcv4s3VbXDnSGGS1WLgB0iMsRbdQGdexrg7cA4EUny/t9cQAfstI/YlKPtwZGmRfU5LD+dC3wRWCEiy711P1DVuT7GZNqObwNPeydTm4FbfY7HN6r6oYjMAj7CXf23jA447IYNtWGMMSakzt7EZIwx5ggsQRhjjAnJEoQxxpiQLEEYY4wJyRKEMcaYkCxBGBOCiOz3nnNF5IYWfu8fNFt+ryXf35iWYgnCmKPLBY4rQXiDtx3NYQlCVTvcHbimY7AEYczRPQRMEJHl3vj/0SLyKxFZIiKfiMjXAURkoogsEpE5eHcYi8hsEVnqzRlwm7fuIdwIoMtF5GlvXVNtRbz3XikiK0TkuqD3Xhg0F8PT3t27xkRUp76T2pgwTAPuUdUrALwD/T5VPVNE4oF3RaRpFM/TgZGqusVb/rKqlopIIrBERJ5X1Wkicoeqjg7xWdcAo3FzLWR6+7ztbRsDjMANKf0u7q73d1r+6xpziNUgjDk+FwNf8oYi+RDIAAZ52xYHJQeAO0XkY+AD3KCQgzi68cCzqtqoqruBt4Azg967QFUDwHJc05cxEWU1CGOOjwDfVtV5h60UmYgbAjt4+ULgbFWtFpGFwMlMSXkg6HUj9n/XtAKrQRhzdJVActDyPOCb3rDoiMjgI0yckwqUeclhKG4K1yb1Tfs3swi4zuvnyMLN4La4Rb6FMSfAzkKMObpPgEavqehJ3LzMucBHXkdxMTAlxH6vAt8QkTXAOlwzU5MZwCci8pGq3hi0/kXgbOBjQIF7VbXISzDGtDobzdUYY0xI1sRkjDEmJEsQxhhjQrIEYYwxJiRLEMYYY0KyBGGMMSYkSxDGGGNCsgRhjDEmpP8P1z+nSqsdaEYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import asarray\n",
        "from numpy import savetxt\n",
        "from google.colab import files\n",
        "'''\n",
        "Saving the learning curve in csv file for later inspection.\n",
        "'''\n",
        "\n",
        "savetxt('val_accuracies.csv', np.round(val_accuracies, decimals= 4) , delimiter=',')\n",
        "\n",
        "savetxt('train_accuracies.csv', np.round(train_accuracies, decimals= 4) , delimiter=',')\n",
        "\n",
        "files.download('train_accuracies.csv')\n",
        "files.download('val_accuracies.csv')\n",
        "\n",
        "print(val_accuracies)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Ns-U2F9aSoCY",
        "outputId": "6a5b3f88-c0a4-46bd-d0c1-747ff88e461d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_eedc8668-af2f-4cb6-969e-29e1417406dd\", \"train_accuracies.csv\", 250)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_af69ee30-036c-4342-a365-7e6633230b01\", \"val_accuracies.csv\", 250)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.6537651419639587, 0.7027382850646973, 0.7374933958053589, 0.7588204145431519, 0.788309633731842, 0.791205883026123, 0.802527666091919, 0.8004212975502014, 0.8135861158370972, 0.8117430210113525]\n"
          ]
        }
      ]
    }
  ]
}