{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Random_Sampling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOfqH/bXLR5swdWqYzQPL23",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Roho1999/Bachelor_Thesis/blob/main/Random_Sampling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning the data"
      ],
      "metadata": {
        "id": "dxC4c4qXbSo4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOzQSWL2a9Di",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a74a548-5b11-4084-a7fa-3c6d725be87d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Created on Thu Sep 16 16:18:51 2021\n",
        "@author: Robin Feldmann\n",
        "\"\"\"\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "%load_ext tensorboard\n",
        "\n",
        "#Simplifies the Data to 3 labels Positive, Negative and Neutral\n",
        "def convert_Sentiment(sentiment):\n",
        "    if sentiment == \"Extremely Positive\":\n",
        "        return 2\n",
        "    elif sentiment == \"Extremely Negative\":\n",
        "        return 0\n",
        "    elif sentiment == \"Positive\":\n",
        "        return 2\n",
        "    elif sentiment == \"Negative\":\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "#removes all URLs \n",
        "def remove_URL(text):\n",
        "    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
        "    return url.sub(r\"\", text)\n",
        "\n",
        "#removes hashtags\n",
        "def remove_hashtags(text):\n",
        "   \n",
        "    text =  re.sub(r\"#\\w+\", \" \", text)\n",
        "    return text\n",
        "\n",
        "#deletes all numbers\n",
        "#this doesn't increases performance and results, so it isn't used\n",
        "def remove_numbers(text):\n",
        "    text = re.sub(r\"\\d+\", \" \", text)\n",
        "    return text\n",
        "\n",
        "#removes words with 2 or less letters\n",
        "#this doesn't increases performance and results, so it isn't used\n",
        "def remove_short_words(text):\n",
        "    \n",
        "    text = re.sub(r'\\b\\w{1,2}\\b', \" \", text)\n",
        "    \n",
        "    return text\n",
        "\n",
        "\n",
        "#removes all mentions in form of @name\n",
        "#names are not important for the sentiment and bad for tokenization and\n",
        "#embedding\n",
        "#led to a relatively strong increase in performance\n",
        "def remove_mentions(text):\n",
        "    text = re.sub(r\"@([a-zA-Z0-9_.-]{1,100})\", \" \", text)\n",
        "    return text\n",
        "\n",
        "#removes contractions\n",
        "#this doesn't increases performance and results, so it isn't used\n",
        "def decontraction(text):\n",
        "    text = re.sub(r\"won\\'t\", \" will not\", text)\n",
        "    text = re.sub(r\"won\\'t've\", \" will not have\", text)\n",
        "    text = re.sub(r\"can\\'t\", \" can not\", text)\n",
        "    text = re.sub(r\"don\\'t\", \" do not\", text)\n",
        "    \n",
        "    text = re.sub(r\"can\\'t've\", \" can not have\", text)\n",
        "    text = re.sub(r\"ma\\'am\", \" madam\", text)\n",
        "    text = re.sub(r\"let\\'s\", \" let us\", text)\n",
        "    text = re.sub(r\"ain\\'t\", \" am not\", text)\n",
        "    text = re.sub(r\"shan\\'t\", \" shall not\", text)\n",
        "    text = re.sub(r\"sha\\n't\", \" shall not\", text)\n",
        "    text = re.sub(r\"o\\'clock\", \" of the clock\", text)\n",
        "    text = re.sub(r\"y\\'all\", \" you all\", text)\n",
        "    \n",
        "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
        "    text = re.sub(r\"n\\'t've\", \" not have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'s\", \" is\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'d've\", \" would have\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ll've\", \" will have\", text)\n",
        "    text = re.sub(r\"\\'t\", \" not\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'m\", \" am\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    return text   \n",
        "\n",
        "\n",
        "\n",
        "def remove_punct(text):\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(translator)\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "stop = set(stopwords.words(\"english\"))\n",
        "#remove all stopwords from text and seperate every lowers words with space\n",
        "def remove_stopwords(text):\n",
        "    \n",
        "    filtered_words = [word.lower() for word in text.split() if word.lower() not in stop]\n",
        "    \n",
        "    return \" \".join(filtered_words)\n",
        "\n",
        "\n",
        "\n",
        "#takes chars that are more then doubled and presents them as double chars\n",
        "#Example \"Helllllo\" --> \"Hello\"\n",
        "#helps to identify identical words with misspellings\n",
        "def repeated_char(text):\n",
        "    rchar = text.group(0) \n",
        "    \n",
        "    if len(rchar) > 1:\n",
        "        return rchar[0:2] \n",
        "\n",
        "#helper function for repeated_char\n",
        "def unique_char(rep, text):\n",
        "    substitute = re.sub(r'(\\w)\\1+', rep, text)\n",
        "    return substitute\n",
        "\n",
        "\n",
        "\n",
        "#returns text as lower case\n",
        "def to_lower(text):\n",
        "    \n",
        "    return text.lower()\n",
        "\n",
        "\n",
        "#removes articles from data, leads to less complex data and no hit in accuracy\n",
        "def remove_articles(text):\n",
        "\n",
        "\n",
        "  articles = {'a': '', 'an':'', 'and':'', 'the':''}\n",
        "  rest = []\n",
        "  for word in text.split():\n",
        "    if word not in articles:\n",
        "      rest.append(word)\n",
        "  return ' '.join(rest)\n",
        "\n",
        "#counts how often a word is used and determines the number of unique words\n",
        "def count_words(text_col):\n",
        "   \n",
        "    \n",
        "    count = Counter()\n",
        "    for text in text_col.values:\n",
        "        for word in text.split():\n",
        "            count[word] += 1\n",
        "    number_unique_words = len(count)\n",
        "    return count, number_unique_words\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#tokenize the words \n",
        "def tokenize(train_sentences, val_sentences, num_unique_words):\n",
        "   \n",
        "\n",
        "    #vectorize a text corpus by turning each text into a sequence of integers\n",
        "    tokenizer = Tokenizer(num_words=num_unique_words)\n",
        "    \n",
        "    tokenizer.fit_on_texts(np.append(train_sentences, val_sentences))\n",
        "    \n",
        "    word_index = tokenizer.word_index\n",
        "    return tokenizer, word_index\n",
        "\n",
        "#shows a simple barplot of given data \n",
        "def simple_barplot(data,names=\"\",color=['green','red', 'black', 'blue', 'violet'],\n",
        "                   title=\"\", labelx=\"\", labely=\"\"):\n",
        "    \n",
        "    f = plt.figure()\n",
        "    plt.bar(x = data, height = names, color=color)\n",
        "    f.set_figwidth(10)\n",
        "    plt.title(title, fontweight=\"heavy\")\n",
        "    plt.ylabel(labely, fontweight=\"heavy\")\n",
        "    plt.xlabel(labelx, fontweight=\"heavy\")\n",
        "    plt.show()\n",
        "    #f.savefig(f\"barplot_{color[-1]}.png\")\n",
        "    #files.download(f\"barplot_{color[-1]}.png\")\n",
        "\n",
        "#no hugh difference for the length to cut outliers: so not used\n",
        "#maybe because tweets are inherently limited to a certain amount of letters\n",
        "#sequences are just padded to the longest sequence\n",
        "#twitter only allows for 280 characters\n",
        "def show_length_distribution(train_sequences):\n",
        "    \n",
        "    #statistic of tweet length\n",
        "    tweet_len=[len(item) for item in train_sequences]\n",
        "    \n",
        "    #important stats\n",
        "    max_len = len(max(train_sequences, key=len))\n",
        "    print(f\"Length longest squence: {max_len}\")\n",
        "    quantile = np.quantile(tweet_len, 0.99)\n",
        "    print(f\"99% quantile: {quantile}\")\n",
        "    '''\n",
        "    #plot\n",
        "    dist = plt.figure()\n",
        "    plt.scatter(np.arange(len(tweet_len))[::100], tweet_len[::100], color = \"b\")\n",
        "    plt.axhline(quantile, color = \"r\")\n",
        "    plt.title('Tweet-length')\n",
        "    plt.ylabel('Length')\n",
        "    plt.xlabel('Tweet-index')\n",
        "    plt.legend(['99% Quantile', 'Tweets'], loc='upper right')\n",
        "    plt.show()\n",
        "    '''\n",
        "    #dist.savefig(\"Length_distribution.png\")\n",
        "    #files.download(\"Length_distribution.png\")\n",
        "    #return int(quantile)\n",
        "    return int(max_len)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To use the code, download the train and test data from https://www.kaggle.com/datatattle/covid-19-nlp-text-classification and upload it to colab named corona_tweets_train.csv and corona_tweets_test.csv respectively"
      ],
      "metadata": {
        "id": "1j-HsqBnQq60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\"\"\"\n",
        "loading the corona tweets training and test data\n",
        "from https://www.kaggle.com/datatattle/covid-19-nlp-text-classification\n",
        "\n",
        "\"\"\"\n",
        "df = pd.read_csv(\"corona_tweets_train.csv\", encoding='latin1')\n",
        "df_test = pd.read_csv(\"corona_tweets_test.csv\", encoding='latin1')\n",
        "\n",
        "\n",
        "#append the test data to the df to do the cleaning on both, after that split \n",
        "#split again\n",
        "df = df.append(df_test)\n",
        "#print(df.head())\n",
        "\n",
        "#remove unnessessary colums \n",
        "df = df.drop(['Location','TweetAt','ScreenName'], axis=1)\n",
        "\n",
        "#print(df.head())\n",
        "\n",
        "\"\"\"\n",
        "Process the target sentiments help:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#show the distribution of sentiments\n",
        "#simple_barplot(df.Sentiment.value_counts().index, df.Sentiment.value_counts().values,\n",
        "#                   title=\"Original Data Distribution\", labelx=\"Labels\", labely=\"Samples\") \n",
        "\n",
        "#simplify to positive,neutral and negative\n",
        "\n",
        "df.Sentiment = df.Sentiment.apply(lambda x : convert_Sentiment(x))\n",
        "\n",
        "\n",
        "\n",
        "#show new labels df.Sentiment.value_counts().index\n",
        "#simple_barplot([\"2 (Positive)\",\"0 (Negative)\",\"1 (Neutral)\"], df.Sentiment.value_counts().values\n",
        "#               ,title=\"Simplified Data Distribution\", labelx=\"Labels\", labely=\"Samples\",\n",
        "#               color = [\"green\", \"red\", \"black\"]) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Clean and preprocess the data, removing duplicates, punctation, URLs\n",
        "and stopwords. Functions from utils.\n",
        "\"\"\"\n",
        "df = df.drop_duplicates()\n",
        "print(\"\\nOriginal Shape: \", df.shape, \"\\n\")\n",
        "print(\"No preprocessing:\\n\",df[\"OriginalTweet\"].head(5))\n",
        "\n",
        "#removes URLs, mentions, hashtags and short words in dataset   \n",
        "df[\"OriginalTweet\"] = df.OriginalTweet.map(remove_URL)\n",
        "\n",
        "df[\"OriginalTweet\"] = df.OriginalTweet.map(remove_mentions)\n",
        "\n",
        "#does not increase accuracy or performance\n",
        "#df[\"OriginalTweet\"] = df.OriginalTweet.map(decontraction)\n",
        "\n",
        "#increases accuracy and performance\n",
        "df[\"OriginalTweet\"] = df.OriginalTweet.map(remove_hashtags)\n",
        "\n",
        "\n",
        "#removing short words does not lead to better accuracy\n",
        "#df[\"OriginalTweet\"] = df.OriginalTweet.map(remove_short_words)\n",
        "\n",
        "\n",
        "#removing numbers does not changes accuracy by a lot but decreases runtime\n",
        "df[\"OriginalTweet\"] = df.OriginalTweet.map(remove_numbers)\n",
        "\n",
        "\n",
        "#removes punctation dataset\n",
        "df[\"OriginalTweet\"] = df.OriginalTweet.map(remove_punct)\n",
        "\n",
        "\n",
        "#removes stopwords dataset, and also removes multible consecutive spaces\n",
        "#decreases complexity of the samples but also decreases accuracy significantly \n",
        "df[\"OriginalTweet\"] = df.OriginalTweet.map(remove_stopwords)\n",
        "\n",
        "#remove double words\n",
        "df['OriginalTweet'] = df['OriginalTweet'].apply(lambda x : unique_char(repeated_char,x))\n",
        "\n",
        "#all to lower case\n",
        "df[\"OriginalTweet\"] = df.OriginalTweet.map(to_lower)\n",
        "\n",
        "df[\"OriginalTweet\"] = df.OriginalTweet.map(remove_articles)\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nAfter prep.:\\n \", df[\"OriginalTweet\"].head(5))\n",
        "\n",
        "#gets a list of word counts \"counter\" and the number of unique words\n",
        "counter, num_unique_words = count_words(df.OriginalTweet)\n",
        "\n",
        "print(\"\\n\", \"The number of unique words in the OriginalTweet column: \", \n",
        "      num_unique_words, \"\\n\")\n",
        "\n",
        "'''\n",
        "\n",
        "split dataset into original training and validation set \n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "train_df = df[:41157]\n",
        "\n",
        "val_df = df[41157:]\n",
        "\n",
        "#split text and labels into numpy arrays\n",
        "train_sentences = train_df.OriginalTweet.to_numpy()\n",
        "train_labels = train_df.Sentiment.to_numpy()\n",
        "val_sentences = val_df.OriginalTweet.to_numpy()\n",
        "val_labels = val_df.Sentiment.to_numpy()\n",
        "\n",
        "\n",
        "# Convert labels to categorical \n",
        "train_labels = to_categorical(train_labels, 3)\n",
        "val_labels  = to_categorical(val_labels, 3)\n",
        "print(f\"The shape of the training and validation data: \\n\",\n",
        "f\"{train_sentences.shape} \\n {val_sentences.shape} \\n\")\n",
        "#create a tokenizer on the whole dataset and the respective word_index\n",
        "tokenizer, word_index = tokenize(train_sentences, val_sentences, num_unique_words)\n",
        "\n",
        "\"\"\"\n",
        "tokenize the validation and trainings data \n",
        "\"\"\"\n",
        "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "val_sequences = tokenizer.texts_to_sequences(val_sentences)\n",
        "                                           \n",
        "print(f\"Sequence example: {train_sequences[0]}\")\n",
        "\n",
        "'''\n",
        "show the distribution of the tweet-length and the maximal length\n",
        "to find a good pad-size\n",
        "'''\n",
        "\n",
        "pad_size = show_length_distribution(train_sequences)\n",
        "\n",
        "\n",
        "'''\n",
        "pads the squences length of the longest tweet \n",
        "to simplify training\n",
        "'''\n",
        "\n",
        "train_padded = pad_sequences(train_sequences, maxlen = pad_size, \n",
        "                             padding=\"post\", truncating=\"post\")\n",
        "\n",
        "val_padded = pad_sequences(val_sequences, maxlen = pad_size,\n",
        "                             padding=\"post\", truncating=\"post\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "O4mgGzSGbc3p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8c5a0a8-3e90-4752-d44a-835139b8b62e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original Shape:  (44955, 3) \n",
            "\n",
            "No preprocessing:\n",
            " 0    @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...\n",
            "1    advice Talk to your neighbours family to excha...\n",
            "2    Coronavirus Australia: Woolworths to give elde...\n",
            "3    My food stock is not the only one which is emp...\n",
            "4    Me, ready to go at supermarket during the #COV...\n",
            "Name: OriginalTweet, dtype: object\n",
            "\n",
            "After prep.:\n",
            "  0                                                     \n",
            "1    advice talk neighbours family exchange phone n...\n",
            "2    coronavirus australia woolworths give elderly ...\n",
            "3    food stock one empty please dont panic enough ...\n",
            "4    ready go supermarket outbreak im paranoid food...\n",
            "Name: OriginalTweet, dtype: object\n",
            "\n",
            " The number of unique words in the OriginalTweet column:  40974 \n",
            "\n",
            "The shape of the training and validation data: \n",
            " (41157,) \n",
            " (3798,) \n",
            "\n",
            "Sequence example: []\n",
            "Length longest squence: 41\n",
            "99% quantile: 30.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Model"
      ],
      "metadata": {
        "id": "Ify5rH1AbjQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "create LSTM model \n",
        "'''\n",
        "name = \"Bothreguse4_12_16\"\n",
        "from tensorflow.keras import layers, regularizers\n",
        "\n",
        "#regularizers and constraints\n",
        "l1 = regularizers.l1(1e-4)\n",
        "constraint = tf.keras.constraints.max_norm(3.)\n",
        "\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "\n",
        "model.add(layers.Embedding(num_unique_words, 512 , input_length = pad_size))\n",
        "\n",
        "#adding a convolutional layer didn't help the learning prozess, contrary to the paper \n",
        "#\"Evaluation of Deep Learning Techniques in\n",
        "#Sentiment Analysis from Twitter Data\"\n",
        "\n",
        "#breaks down the dimesionality of data and speeds up the learning significantly\n",
        "\n",
        "#bidirectional because it is works well for language processing, since language is recursive by nature\n",
        "#dropout to reduce overfitting on small details \n",
        "#regularizer to reduce exploding gradients\n",
        "#\n",
        "# with 512 dimensions best result but slower\n",
        "model.add(layers.Bidirectional(\n",
        "    layers.LSTM(512, return_sequences=True,\n",
        "                recurrent_regularizer =l1,\n",
        "                dropout =0.2,\n",
        "                kernel_constraint= constraint,\n",
        "                bias_constraint = constraint\n",
        "                )))\n",
        "\n",
        "#MaxPool worked better than global average pooling\n",
        "model.add(layers.GlobalMaxPool1D())\n",
        "\n",
        "\n",
        "\n",
        "model.add(layers.Dense(32, activation= \"relu\", \n",
        "                       bias_regularizer = l1))\n",
        "                      # bias_constraint = constraint ))\n",
        "\n",
        "model.add(layers.Dropout(0.2))\n",
        "\n",
        "\n",
        "\n",
        "model.add(layers.Dense(3, activation= \"softmax\"))\n",
        "\n",
        "print(model.summary())\n"
      ],
      "metadata": {
        "id": "f99-xGZZbcyr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ba7a943-828f-4dcd-e7aa-480efd359f1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 41, 512)           20978688  \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 41, 1024)         4198400   \n",
            " l)                                                              \n",
            "                                                                 \n",
            " global_max_pooling1d (Globa  (None, 1024)             0         \n",
            " lMaxPooling1D)                                                  \n",
            "                                                                 \n",
            " dense (Dense)               (None, 32)                32800     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 32)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3)                 99        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,209,987\n",
            "Trainable params: 25,209,987\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creates a new model with the same architecture but new weights for experiment \n",
        "#with Uncertainty Sampling\n",
        "model_entro = tf.keras.models.clone_model(model)\n",
        "\n",
        "\n",
        "#compile the model\n",
        "loss= keras.losses.CategoricalCrossentropy(from_logits = False)\n",
        "#small learning rate to get a clear understanding of the learning curve and\n",
        "#better comparability\n",
        "optim = keras.optimizers.Adam(learning_rate=0.00006)\n",
        "metrics = [\"accuracy\",tf.keras.metrics.Precision(),tf.keras.metrics.Recall()]\n",
        "model_entro.compile(loss=loss, optimizer=optim, metrics=metrics)\n"
      ],
      "metadata": {
        "id": "bX-3xN-_Bd9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Different Implementations for calculating the entropy"
      ],
      "metadata": {
        "id": "H3tysFGQb58g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The fastest implementation with simple numpy functions.\n",
        "this implementation (list_entropys3) is faster than the list_entropys1 function (with scpy.stats.entropy) by around 88.9006%:\n",
        "\n",
        "(100/0.0946) * 0.0105 - 100 = -88.9006 %"
      ],
      "metadata": {
        "id": "Asfxx3-6cUEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "fastest implementation with numpy log\n",
        "gets the prediction of the model over the whole set of samples then computes\n",
        "entropy\n",
        "\n",
        "in a trial with 640 samples it took 6.734 seconds, 0.0105s per sample\n",
        "'''\n",
        "def list_entropys3(model, training_pool):\n",
        "\n",
        "  \"\"\" Computes entropy of label distribution. \"\"\"\n",
        "  entropys = []\n",
        "  probs = model.predict(training_pool)\n",
        "  for i in range(0,len(probs)):\n",
        "    entro = 0.\n",
        "   \n",
        "    prob = probs[i]\n",
        "    # Compute entropy\n",
        "   \n",
        "    for j in prob:\n",
        "      entro -= j * np.log(j)\n",
        "\n",
        "    entropys.append(entro)\n",
        "  \n",
        "  return entropys"
      ],
      "metadata": {
        "id": "pLYMRpGnb9um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two slower functions to compute the entropy for the whole dataset."
      ],
      "metadata": {
        "id": "oH0nOyaRcR8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from scipy.stats import entropy\n",
        "from numpy import e,log\n",
        "\n",
        "\n",
        "#the two functions give back the list of the list of entropies of the training_pool in respect to\n",
        "#the model\n",
        "\n",
        "\n",
        "#slower implementation with scipy.stats.entropy \n",
        "#in a trial with 640 samples it took 60.56 seconds, 0.0946s per sample\n",
        "def list_entropys(model, training_pool):\n",
        "  entropys = []\n",
        " \n",
        "  for i in range(0,len(training_pool)):\n",
        "  \n",
        "    entro = entropy(model.predict(training_pool[[i][:]]).reshape(3,))\n",
        "    entropys.append(entro)\n",
        "  return entropys\n",
        "\n",
        "#faster implementation with numpy log\n",
        "#in a trial with 640 samples it took 50.07 seconds, 0.0782s per sample\n",
        "def list_entropys2(model, training_pool):\n",
        "\n",
        "  \"\"\" Computes entropy of label distribution. \"\"\"\n",
        "  entropys = []\n",
        "  for i in range(0,len(training_pool)):\n",
        "    entro = 0.\n",
        "   \n",
        "    probs = model.predict(training_pool[[i][:]]).reshape(3,)\n",
        "    # Compute entropy\n",
        "   \n",
        "    for i in probs:\n",
        "      entro -= i * np.log(i)\n",
        "\n",
        "    entropys.append(entro)\n",
        "  \n",
        "  return entropys\n",
        "\n"
      ],
      "metadata": {
        "id": "IlGVymEMbvYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing execution speed"
      ],
      "metadata": {
        "id": "tXS4EXBPiHcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# '''\n",
        "# Testing 1. implementation of entropy\n",
        "# '''\n",
        "# import copy\n",
        "# training_pool = copy.deepcopy(train_padded)\n",
        "# training_label = copy.deepcopy(train_labels)\n",
        "# #select\n",
        "# training_pool = training_pool[:,:] \n",
        "# training_label = training_label[:,:]\n",
        "\n",
        "# import time\n",
        "# start_time = time.time()\n",
        "# entros = np.array(list_entropys(model_entro, training_pool))\n",
        "# print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "# #entros = np.array(list_entropys(model_entro, training_pool))\n",
        "# print(entros)\n",
        "# #print(training_pool)\n",
        "\n",
        "# #sortes the training pool samples by their entropy value in descending order\n",
        "# arr1inds = entros.argsort()\n",
        "# sorted_arr1 = entros[arr1inds[::-1]]\n",
        "# sorted_arr2 = training_pool[arr1inds[::-1]]\n",
        "\n",
        "# print(sorted_arr1)\n",
        "# print(sorted_arr2)"
      ],
      "metadata": {
        "id": "bURxNJDRh9-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# '''\n",
        "# Testing 2. implementation of entropy\n",
        "# '''\n",
        "\n",
        "# start_time = time.time()\n",
        "# entros2 = np.array(list_entropys2(model_entro, training_pool))\n",
        "# print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "\n",
        "# #print(entros2)\n",
        "\n",
        "# arr1inds = entros2.argsort()\n",
        "# sorted_entropys = entros2[arr1inds[::-1]]\n",
        "# sorted_samples = training_pool[arr1inds[::-1]]\n",
        "\n",
        "# print(sorted_entropys)\n",
        "# print(sorted_samples)\n"
      ],
      "metadata": {
        "id": "Xxj4_SBLiBEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# '''\n",
        "# Testing 3. implementation of entropy test: \n",
        "# '''\n",
        "\n",
        "# start_time = time.time()\n",
        "# entros3 = np.array(list_entropys3(model_entro, training_pool))\n",
        "# print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "\n",
        "# #print(entros2)\n",
        "\n",
        "# arr1inds = entros3.argsort()\n",
        "# sorted_entropys = entros3[arr1inds[::-1]]\n",
        "# sorted_samples = training_pool[arr1inds[::-1]]\n",
        "\n",
        "# print(sorted_entropys)\n",
        "# print(sorted_samples)"
      ],
      "metadata": {
        "id": "5sOQcYvziEz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model "
      ],
      "metadata": {
        "id": "wiQqX6_oimpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "train the model and return the accuracy and val_accuracy\n",
        "\"\"\"\n",
        "\n",
        "def train_model(model, train_padded, train_labels, val_padded, val_labels):\n",
        "\n",
        "  import datetime\n",
        " \n",
        "  # Clear any logs from previous runs\n",
        "  !rm -rf /logs/\n",
        "\n",
        "  #clearing memory to avoid clutter from old runs\n",
        "  tf.keras.backend.clear_session()\n",
        " \n",
        "  ## Hyperparameters\n",
        "  epochs = 30\n",
        "  batchsize = 64\n",
        "  early_stopping = keras.callbacks.EarlyStopping(monitor =\"val_loss\", \n",
        "                                          mode =\"min\", patience=2)\n",
        "\n",
        "  #create the log file for tensorboard\n",
        "  log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "  \n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        " \n",
        "\n",
        "\n",
        "  checkpoint_filepath = '/tmp/checkpoint'\n",
        "  model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "  filepath=checkpoint_filepath,\n",
        "  save_weights_only=True,\n",
        "  monitor='val_accuracy',\n",
        "  mode='max',\n",
        "  save_best_only=True)\n",
        "   \n",
        "  #shuffels dataset before training so that the order isn't always the same and\n",
        "  #model isn't bias towards the first samples more\n",
        "  #then trains with given parameters\n",
        "  history = model.fit(train_padded,\n",
        "                      train_labels,\n",
        "                      epochs=epochs,\n",
        "                      batch_size = batchsize,\n",
        "                      validation_data=(val_padded, val_labels),\n",
        "                      callbacks=[early_stopping,tensorboard_callback,\n",
        "                        model_checkpoint_callback],\n",
        "                      verbose=1, shuffle=True)\n",
        "\n",
        "  model.load_weights(checkpoint_filepath)\n",
        "\n",
        "  return model, history.history['accuracy'][:], history.history['val_accuracy'][:]"
      ],
      "metadata": {
        "id": "0QiMr271i0fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Training-Loop"
      ],
      "metadata": {
        "id": "A_GEPPKJjJ8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#helper function to reset weights after each loop\n",
        "weights = model_entro.get_weights()\n",
        "reset_model = lambda model: model.set_weights(weights)"
      ],
      "metadata": {
        "id": "ObsuPXLNjUmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "###HYPERPARAMETERS\n",
        "x= 2048\n",
        "loops = 10\n",
        "patience = 10\n",
        "\n",
        "\n",
        "# copy the padded training data and training labels for future use\n",
        "training_pool = copy.deepcopy(train_padded)\n",
        "training_label = copy.deepcopy(train_labels)\n",
        "#select\n",
        "training_pool = training_pool[:,:] \n",
        "training_label = training_label[:,:]\n",
        "\n",
        "\n",
        "\n",
        "selected_samples = np.zeros( shape =(0,pad_size), dtype=np.int32)\n",
        "selected_labels  = np.zeros( shape =(0,3), dtype=np.int32)\n",
        "\n",
        "#take the first x random samples from the training pool and the labels to train\n",
        "#the model \n",
        "#also delete the used samples from original training_pool and training_label\n",
        "\n",
        "#512*32= 16384 around half of the samples used for the orginial model\n",
        "#x are the amount of samples that are added in each iteration\n",
        "\n",
        "\n",
        "selected_samples = np.append(selected_samples, training_pool[:x][:], axis=0)\n",
        "training_pool = np.delete(training_pool,slice(0,x) , axis=0)\n",
        "\n",
        "selected_labels = np.append(selected_labels, training_label[:x][:], axis=0)\n",
        "training_label = np.delete(training_label,slice(0,x) , axis=0)\n",
        "\n",
        "\n",
        "'''\n",
        "repeat loops times:\n",
        "  train the model for one loop and calculate the entropy in respect to the trained\n",
        "  model, select 1/2x new samples with highest entropy and 1/2x random samples, reset model and relearn on the \n",
        "  new + old samples\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "val_accuracies = []\n",
        "train_accuracies = []\n",
        "wait = 0\n",
        "best = 1\n",
        "\n",
        "for i in range(loops):\n",
        "\n",
        "  print(f\"\\nIteration :{i+1} - Samples: {(i+1)*x} \\n \")\n",
        "  \n",
        "  #clearing the old model graph and reset all the weights\n",
        "  keras.backend.clear_session()\n",
        "  reset_model(model_entro)\n",
        "  \n",
        "  \n",
        "  model_entro, train_accuracy, val_accuracy = train_model(model_entro,\n",
        "                                                          selected_samples,\n",
        "                                                          selected_labels,\n",
        "                                                          val_padded, val_labels)\n",
        "  \n",
        "  #adding max accuracy from each iteration to the accuracy lists\n",
        "  max_index = np.argmax(val_accuracy)\n",
        "  val_accuracies.append(val_accuracy[max_index])\n",
        "  train_accuracies.append(train_accuracy[max_index])\n",
        "  \n",
        "\n",
        "  \n",
        " \n",
        "\n",
        "\n",
        "  #add the x random samples\n",
        "  random_idx = np.random.randint(np.size(training_pool, axis = 0), size=x)\n",
        "  selected_samples = np.append(selected_samples, training_pool[random_idx][:], axis=0)\n",
        "  selected_labels = np.append(selected_labels, training_label[random_idx][:], axis=0)\n",
        "\n",
        "  #delete the selected samples and labels from original pool\n",
        "  training_pool = np.delete(training_pool,random_idx , axis=0)\n",
        "  training_label = np.delete(training_label,random_idx , axis=0)\n",
        "\n",
        "\n",
        "  '''\n",
        "  early stopping method that doesn't need labeled test data:\n",
        "  if the uncertainty for the test data does not \n",
        "  decrease for a certain amount of iterations the training stops\n",
        "  '''\n",
        "  val = val_accuracy[max_index]\n",
        "\n",
        " \n",
        "  mean_entropy = np.mean((list_entropys3(model_entro, val_padded)))\n",
        "\n",
        "  \n",
        "  wait += 1\n",
        "  if mean_entropy < best:\n",
        "    best = mean_entropy\n",
        "    wait = 0\n",
        "  if wait >= patience:\n",
        "    break\n",
        "\n",
        "  \n",
        "  print(f\"Mean entropy for test data: {mean_entropy} - Lowest entropy for test data: {best}\")\n",
        "\n",
        "\n",
        "'''\n",
        "visualize training\n",
        "'''\n",
        "test = plt.figure()\n",
        "plt.plot(train_accuracies)\n",
        "plt.plot(val_accuracies)\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('Iteration')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "\n",
        "result = np.round(np.amax(val_accuracies), decimals = 4)\n",
        "plt.title(f'model accuracy:{result}' )\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VsnSpK-CjVYy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9388aa44-2b74-4cb3-e08c-aaaa53d30d4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration :1 - Samples: 2048 \n",
            " \n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 12s 266ms/step - loss: 4.5929 - accuracy: 0.4087 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 4.3786 - val_accuracy: 0.4300 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 4s 144ms/step - loss: 4.1959 - accuracy: 0.4248 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 3.9999 - val_accuracy: 0.4300 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 6s 202ms/step - loss: 3.8393 - accuracy: 0.4248 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 3.6579 - val_accuracy: 0.4326 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 6s 206ms/step - loss: 3.5157 - accuracy: 0.4419 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 3.3488 - val_accuracy: 0.4508 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 6s 205ms/step - loss: 3.2187 - accuracy: 0.4478 - precision: 1.0000 - recall: 0.0015 - val_loss: 3.0663 - val_accuracy: 0.4724 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 4s 144ms/step - loss: 2.9443 - accuracy: 0.4824 - precision: 0.8889 - recall: 0.0039 - val_loss: 2.8073 - val_accuracy: 0.4602 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 6s 204ms/step - loss: 2.6945 - accuracy: 0.5005 - precision: 0.7439 - recall: 0.0298 - val_loss: 2.5714 - val_accuracy: 0.5032 - val_precision: 0.8305 - val_recall: 0.0129\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 6s 205ms/step - loss: 2.4631 - accuracy: 0.5137 - precision: 0.7639 - recall: 0.0537 - val_loss: 2.3554 - val_accuracy: 0.5047 - val_precision: 0.7450 - val_recall: 0.0692\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 4s 142ms/step - loss: 2.2459 - accuracy: 0.5483 - precision: 0.7991 - recall: 0.0913 - val_loss: 2.1615 - val_accuracy: 0.4995 - val_precision: 0.7500 - val_recall: 0.0837\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 6s 208ms/step - loss: 2.0515 - accuracy: 0.5547 - precision: 0.7907 - recall: 0.0996 - val_loss: 1.9813 - val_accuracy: 0.5169 - val_precision: 0.7074 - val_recall: 0.0929\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 6s 206ms/step - loss: 1.8642 - accuracy: 0.5908 - precision: 0.7919 - recall: 0.1431 - val_loss: 1.8229 - val_accuracy: 0.5240 - val_precision: 0.7107 - val_recall: 0.1106\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 6s 203ms/step - loss: 1.6915 - accuracy: 0.6348 - precision: 0.8384 - recall: 0.2280 - val_loss: 1.6801 - val_accuracy: 0.5527 - val_precision: 0.6958 - val_recall: 0.1469\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 6s 207ms/step - loss: 1.5258 - accuracy: 0.6699 - precision: 0.8437 - recall: 0.3374 - val_loss: 1.5540 - val_accuracy: 0.5671 - val_precision: 0.6787 - val_recall: 0.2135\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 6s 209ms/step - loss: 1.3681 - accuracy: 0.7188 - precision: 0.8596 - recall: 0.4663 - val_loss: 1.4428 - val_accuracy: 0.5848 - val_precision: 0.7358 - val_recall: 0.3212\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 6s 206ms/step - loss: 1.2156 - accuracy: 0.7642 - precision: 0.8643 - recall: 0.5879 - val_loss: 1.3487 - val_accuracy: 0.5929 - val_precision: 0.7212 - val_recall: 0.4258\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 6s 208ms/step - loss: 1.0617 - accuracy: 0.8125 - precision: 0.8706 - recall: 0.6899 - val_loss: 1.2564 - val_accuracy: 0.6116 - val_precision: 0.7063 - val_recall: 0.4908\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 6s 207ms/step - loss: 0.8968 - accuracy: 0.8369 - precision: 0.8810 - recall: 0.7661 - val_loss: 1.1868 - val_accuracy: 0.6282 - val_precision: 0.6933 - val_recall: 0.5590\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 7s 210ms/step - loss: 0.7368 - accuracy: 0.8794 - precision: 0.9055 - recall: 0.8418 - val_loss: 1.1383 - val_accuracy: 0.6453 - val_precision: 0.6834 - val_recall: 0.5951\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 4s 144ms/step - loss: 0.6181 - accuracy: 0.8975 - precision: 0.9130 - recall: 0.8809 - val_loss: 1.1490 - val_accuracy: 0.6288 - val_precision: 0.6598 - val_recall: 0.5948\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 7s 212ms/step - loss: 0.5361 - accuracy: 0.9106 - precision: 0.9260 - recall: 0.8916 - val_loss: 1.1193 - val_accuracy: 0.6490 - val_precision: 0.6768 - val_recall: 0.6124\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 5s 146ms/step - loss: 0.4663 - accuracy: 0.9258 - precision: 0.9328 - recall: 0.9146 - val_loss: 1.1071 - val_accuracy: 0.6461 - val_precision: 0.6688 - val_recall: 0.6151\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 7s 215ms/step - loss: 0.4040 - accuracy: 0.9487 - precision: 0.9529 - recall: 0.9385 - val_loss: 1.0892 - val_accuracy: 0.6567 - val_precision: 0.6743 - val_recall: 0.6253\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 5s 147ms/step - loss: 0.3586 - accuracy: 0.9531 - precision: 0.9588 - recall: 0.9443 - val_loss: 1.0951 - val_accuracy: 0.6524 - val_precision: 0.6692 - val_recall: 0.6232\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 6s 206ms/step - loss: 0.3281 - accuracy: 0.9556 - precision: 0.9610 - recall: 0.9497 - val_loss: 1.1236 - val_accuracy: 0.6585 - val_precision: 0.6713 - val_recall: 0.6324\n",
            "Mean entropy for test data: 0.4597606434377062 - Lowest entropy for test data: 0.4597606434377062\n",
            "\n",
            "Iteration :2 - Samples: 4096 \n",
            " \n",
            "Epoch 1/30\n",
            "64/64 [==============================] - 8s 116ms/step - loss: 4.5178 - accuracy: 0.3879 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 4.1806 - val_accuracy: 0.4394 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/30\n",
            "64/64 [==============================] - 5s 79ms/step - loss: 3.8966 - accuracy: 0.4321 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 3.5998 - val_accuracy: 0.4221 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/30\n",
            "64/64 [==============================] - 5s 80ms/step - loss: 3.3692 - accuracy: 0.4243 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 3.1160 - val_accuracy: 0.4071 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/30\n",
            "64/64 [==============================] - 5s 78ms/step - loss: 2.9166 - accuracy: 0.4365 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.7030 - val_accuracy: 0.4329 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/30\n",
            "64/64 [==============================] - 5s 80ms/step - loss: 2.5401 - accuracy: 0.4424 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.3636 - val_accuracy: 0.4379 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 6/30\n",
            "64/64 [==============================] - 7s 110ms/step - loss: 2.2324 - accuracy: 0.4573 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.0864 - val_accuracy: 0.4573 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 7/30\n",
            "64/64 [==============================] - 7s 110ms/step - loss: 1.9782 - accuracy: 0.4539 - precision: 1.0000 - recall: 9.7656e-04 - val_loss: 1.8600 - val_accuracy: 0.5029 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 8/30\n",
            "64/64 [==============================] - 5s 79ms/step - loss: 1.7734 - accuracy: 0.4834 - precision: 0.5714 - recall: 0.0029 - val_loss: 1.6797 - val_accuracy: 0.4995 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 9/30\n",
            "64/64 [==============================] - 5s 79ms/step - loss: 1.6079 - accuracy: 0.5054 - precision: 0.7755 - recall: 0.0186 - val_loss: 1.5355 - val_accuracy: 0.5003 - val_precision: 0.8660 - val_recall: 0.0221\n",
            "Epoch 10/30\n",
            "64/64 [==============================] - 7s 110ms/step - loss: 1.4638 - accuracy: 0.5442 - precision: 0.7932 - recall: 0.0571 - val_loss: 1.4152 - val_accuracy: 0.5087 - val_precision: 0.7438 - val_recall: 0.0795\n",
            "Epoch 11/30\n",
            "64/64 [==============================] - 7s 112ms/step - loss: 1.3444 - accuracy: 0.5767 - precision: 0.8135 - recall: 0.1001 - val_loss: 1.3133 - val_accuracy: 0.5340 - val_precision: 0.7219 - val_recall: 0.1148\n",
            "Epoch 12/30\n",
            "64/64 [==============================] - 7s 114ms/step - loss: 1.2235 - accuracy: 0.6311 - precision: 0.8452 - recall: 0.2053 - val_loss: 1.2245 - val_accuracy: 0.5682 - val_precision: 0.7506 - val_recall: 0.1711\n",
            "Epoch 13/30\n",
            "64/64 [==============================] - 7s 112ms/step - loss: 1.0991 - accuracy: 0.6855 - precision: 0.8431 - recall: 0.4119 - val_loss: 1.1461 - val_accuracy: 0.5985 - val_precision: 0.7445 - val_recall: 0.3223\n",
            "Epoch 14/30\n",
            "64/64 [==============================] - 7s 113ms/step - loss: 0.9669 - accuracy: 0.7476 - precision: 0.8394 - recall: 0.5728 - val_loss: 1.0782 - val_accuracy: 0.6359 - val_precision: 0.7385 - val_recall: 0.4708\n",
            "Epoch 15/30\n",
            "64/64 [==============================] - 7s 114ms/step - loss: 0.7981 - accuracy: 0.8088 - precision: 0.8685 - recall: 0.7290 - val_loss: 1.0841 - val_accuracy: 0.6403 - val_precision: 0.6768 - val_recall: 0.5927\n",
            "Epoch 16/30\n",
            "64/64 [==============================] - 7s 113ms/step - loss: 0.6310 - accuracy: 0.8594 - precision: 0.8864 - recall: 0.8303 - val_loss: 1.0436 - val_accuracy: 0.6588 - val_precision: 0.6951 - val_recall: 0.6224\n",
            "Epoch 17/30\n",
            "64/64 [==============================] - 7s 113ms/step - loss: 0.5146 - accuracy: 0.9019 - precision: 0.9191 - recall: 0.8823 - val_loss: 1.0634 - val_accuracy: 0.6611 - val_precision: 0.6910 - val_recall: 0.6340\n",
            "Epoch 18/30\n",
            "64/64 [==============================] - 7s 113ms/step - loss: 0.4291 - accuracy: 0.9285 - precision: 0.9423 - recall: 0.9165 - val_loss: 1.1949 - val_accuracy: 0.6627 - val_precision: 0.6786 - val_recall: 0.6388\n",
            "Mean entropy for test data: 0.46152850335388235 - Lowest entropy for test data: 0.4597606434377062\n",
            "\n",
            "Iteration :3 - Samples: 6144 \n",
            " \n",
            "Epoch 1/30\n",
            "96/96 [==============================] - 11s 102ms/step - loss: 4.3474 - accuracy: 0.4196 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 3.8522 - val_accuracy: 0.4071 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/30\n",
            "96/96 [==============================] - 7s 68ms/step - loss: 3.4595 - accuracy: 0.4279 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 3.0665 - val_accuracy: 0.4071 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/30\n",
            "96/96 [==============================] - 6s 65ms/step - loss: 2.7658 - accuracy: 0.4242 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.4650 - val_accuracy: 0.4071 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/30\n",
            "96/96 [==============================] - 6s 67ms/step - loss: 2.2425 - accuracy: 0.4303 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.0148 - val_accuracy: 0.4071 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/30\n",
            "96/96 [==============================] - 9s 91ms/step - loss: 1.8594 - accuracy: 0.4377 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.6935 - val_accuracy: 0.4616 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 6/30\n",
            "96/96 [==============================] - 9s 90ms/step - loss: 1.5848 - accuracy: 0.4468 - precision: 1.0000 - recall: 1.6276e-04 - val_loss: 1.4698 - val_accuracy: 0.4671 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 7/30\n",
            "96/96 [==============================] - 9s 92ms/step - loss: 1.3951 - accuracy: 0.4678 - precision: 0.6786 - recall: 0.0031 - val_loss: 1.3125 - val_accuracy: 0.4882 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 8/30\n",
            "96/96 [==============================] - 9s 91ms/step - loss: 1.2533 - accuracy: 0.5050 - precision: 0.7836 - recall: 0.0436 - val_loss: 1.1967 - val_accuracy: 0.5145 - val_precision: 0.7552 - val_recall: 0.0666\n",
            "Epoch 9/30\n",
            "96/96 [==============================] - 9s 91ms/step - loss: 1.1373 - accuracy: 0.5534 - precision: 0.7494 - recall: 0.1465 - val_loss: 1.1075 - val_accuracy: 0.5374 - val_precision: 0.7454 - val_recall: 0.1380\n",
            "Epoch 10/30\n",
            "96/96 [==============================] - 9s 90ms/step - loss: 1.0293 - accuracy: 0.6045 - precision: 0.7646 - recall: 0.3097 - val_loss: 1.0344 - val_accuracy: 0.5740 - val_precision: 0.7482 - val_recall: 0.2683\n",
            "Epoch 11/30\n",
            "96/96 [==============================] - 9s 89ms/step - loss: 0.9254 - accuracy: 0.6698 - precision: 0.7837 - recall: 0.4640 - val_loss: 0.9683 - val_accuracy: 0.6119 - val_precision: 0.7509 - val_recall: 0.4192\n",
            "Epoch 12/30\n",
            "96/96 [==============================] - 9s 91ms/step - loss: 0.8011 - accuracy: 0.7414 - precision: 0.8232 - recall: 0.6144 - val_loss: 0.8992 - val_accuracy: 0.6519 - val_precision: 0.7484 - val_recall: 0.5232\n",
            "Epoch 13/30\n",
            "96/96 [==============================] - 9s 90ms/step - loss: 0.6567 - accuracy: 0.8169 - precision: 0.8654 - recall: 0.7386 - val_loss: 0.8367 - val_accuracy: 0.6848 - val_precision: 0.7421 - val_recall: 0.6137\n",
            "Epoch 14/30\n",
            "96/96 [==============================] - 9s 92ms/step - loss: 0.4912 - accuracy: 0.8773 - precision: 0.9018 - recall: 0.8429 - val_loss: 0.8343 - val_accuracy: 0.7012 - val_precision: 0.7290 - val_recall: 0.6543\n",
            "Epoch 15/30\n",
            "96/96 [==============================] - 9s 91ms/step - loss: 0.3765 - accuracy: 0.9165 - precision: 0.9311 - recall: 0.8979 - val_loss: 0.8373 - val_accuracy: 0.7072 - val_precision: 0.7299 - val_recall: 0.6704\n",
            "Epoch 16/30\n",
            "96/96 [==============================] - 6s 67ms/step - loss: 0.2949 - accuracy: 0.9409 - precision: 0.9519 - recall: 0.9316 - val_loss: 0.8842 - val_accuracy: 0.7025 - val_precision: 0.7224 - val_recall: 0.6764\n",
            "Mean entropy for test data: 0.5173740955222067 - Lowest entropy for test data: 0.4597606434377062\n",
            "\n",
            "Iteration :4 - Samples: 8192 \n",
            " \n",
            "Epoch 1/30\n",
            "128/128 [==============================] - 11s 80ms/step - loss: 4.1769 - accuracy: 0.4221 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 3.5369 - val_accuracy: 0.4071 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/30\n",
            "128/128 [==============================] - 7s 58ms/step - loss: 3.0524 - accuracy: 0.4257 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.5920 - val_accuracy: 0.4071 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/30\n",
            "128/128 [==============================] - 7s 57ms/step - loss: 2.2642 - accuracy: 0.4368 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.9546 - val_accuracy: 0.4071 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/30\n",
            "128/128 [==============================] - 10s 77ms/step - loss: 1.7478 - accuracy: 0.4434 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.5480 - val_accuracy: 0.4655 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/30\n",
            "128/128 [==============================] - 10s 78ms/step - loss: 1.4261 - accuracy: 0.4581 - precision: 0.7000 - recall: 8.5449e-04 - val_loss: 1.3034 - val_accuracy: 0.4824 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 6/30\n",
            "128/128 [==============================] - 10s 76ms/step - loss: 1.2304 - accuracy: 0.4839 - precision: 0.6667 - recall: 0.0195 - val_loss: 1.1498 - val_accuracy: 0.5079 - val_precision: 0.8061 - val_recall: 0.0208\n",
            "Epoch 7/30\n",
            "128/128 [==============================] - 10s 78ms/step - loss: 1.1035 - accuracy: 0.5187 - precision: 0.6777 - recall: 0.1196 - val_loss: 1.0558 - val_accuracy: 0.5382 - val_precision: 0.7071 - val_recall: 0.1519\n",
            "Epoch 8/30\n",
            "128/128 [==============================] - 10s 77ms/step - loss: 1.0088 - accuracy: 0.5541 - precision: 0.6738 - recall: 0.2449 - val_loss: 0.9950 - val_accuracy: 0.5537 - val_precision: 0.6992 - val_recall: 0.1964\n",
            "Epoch 9/30\n",
            "128/128 [==============================] - 10s 78ms/step - loss: 0.9219 - accuracy: 0.6196 - precision: 0.7323 - recall: 0.3816 - val_loss: 0.9393 - val_accuracy: 0.5882 - val_precision: 0.7374 - val_recall: 0.3246\n",
            "Epoch 10/30\n",
            "128/128 [==============================] - 10s 78ms/step - loss: 0.8225 - accuracy: 0.6810 - precision: 0.7606 - recall: 0.5248 - val_loss: 0.8851 - val_accuracy: 0.6237 - val_precision: 0.7318 - val_recall: 0.4805\n",
            "Epoch 11/30\n",
            "128/128 [==============================] - 10s 80ms/step - loss: 0.7081 - accuracy: 0.7561 - precision: 0.8100 - recall: 0.6530 - val_loss: 0.8209 - val_accuracy: 0.6627 - val_precision: 0.7339 - val_recall: 0.5766\n",
            "Epoch 12/30\n",
            "128/128 [==============================] - 10s 82ms/step - loss: 0.5776 - accuracy: 0.8232 - precision: 0.8584 - recall: 0.7684 - val_loss: 0.7748 - val_accuracy: 0.6962 - val_precision: 0.7398 - val_recall: 0.6527\n",
            "Epoch 13/30\n",
            "128/128 [==============================] - 11s 83ms/step - loss: 0.4608 - accuracy: 0.8723 - precision: 0.8934 - recall: 0.8428 - val_loss: 0.7708 - val_accuracy: 0.7125 - val_precision: 0.7402 - val_recall: 0.6848\n",
            "Epoch 14/30\n",
            "128/128 [==============================] - 10s 81ms/step - loss: 0.3574 - accuracy: 0.9077 - precision: 0.9190 - recall: 0.8944 - val_loss: 0.7769 - val_accuracy: 0.7233 - val_precision: 0.7482 - val_recall: 0.7033\n",
            "Epoch 15/30\n",
            "128/128 [==============================] - 8s 61ms/step - loss: 0.2794 - accuracy: 0.9387 - precision: 0.9454 - recall: 0.9307 - val_loss: 0.8610 - val_accuracy: 0.7220 - val_precision: 0.7367 - val_recall: 0.7059\n",
            "Mean entropy for test data: 0.4972204389303983 - Lowest entropy for test data: 0.4597606434377062\n",
            "\n",
            "Iteration :5 - Samples: 10240 \n",
            " \n",
            "Epoch 1/30\n",
            "160/160 [==============================] - 12s 71ms/step - loss: 4.0141 - accuracy: 0.4226 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 3.2465 - val_accuracy: 0.4071 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/30\n",
            "160/160 [==============================] - 8s 52ms/step - loss: 2.7079 - accuracy: 0.4243 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.2158 - val_accuracy: 0.4071 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/30\n",
            "160/160 [==============================] - 11s 69ms/step - loss: 1.8989 - accuracy: 0.4392 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.6075 - val_accuracy: 0.4679 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/30\n",
            "160/160 [==============================] - 11s 69ms/step - loss: 1.4471 - accuracy: 0.4491 - precision: 0.7500 - recall: 0.0015 - val_loss: 1.2850 - val_accuracy: 0.4863 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/30\n",
            "160/160 [==============================] - 11s 68ms/step - loss: 1.2056 - accuracy: 0.4843 - precision: 0.7256 - recall: 0.0341 - val_loss: 1.1144 - val_accuracy: 0.5066 - val_precision: 0.7454 - val_recall: 0.0748\n",
            "Epoch 6/30\n",
            "160/160 [==============================] - 11s 69ms/step - loss: 1.0700 - accuracy: 0.5292 - precision: 0.7135 - recall: 0.1196 - val_loss: 1.0187 - val_accuracy: 0.5261 - val_precision: 0.7337 - val_recall: 0.1335\n",
            "Epoch 7/30\n",
            "160/160 [==============================] - 12s 73ms/step - loss: 0.9782 - accuracy: 0.5715 - precision: 0.7074 - recall: 0.2563 - val_loss: 0.9581 - val_accuracy: 0.5682 - val_precision: 0.7403 - val_recall: 0.2409\n",
            "Epoch 8/30\n",
            "160/160 [==============================] - 11s 66ms/step - loss: 0.8808 - accuracy: 0.6401 - precision: 0.7450 - recall: 0.4185 - val_loss: 0.8960 - val_accuracy: 0.6108 - val_precision: 0.7434 - val_recall: 0.4355\n",
            "Epoch 9/30\n",
            "160/160 [==============================] - 11s 67ms/step - loss: 0.7590 - accuracy: 0.7199 - precision: 0.7984 - recall: 0.5847 - val_loss: 0.8127 - val_accuracy: 0.6696 - val_precision: 0.7574 - val_recall: 0.5516\n",
            "Epoch 10/30\n",
            "160/160 [==============================] - 11s 68ms/step - loss: 0.6012 - accuracy: 0.8048 - precision: 0.8507 - recall: 0.7304 - val_loss: 0.7429 - val_accuracy: 0.7077 - val_precision: 0.7514 - val_recall: 0.6551\n",
            "Epoch 11/30\n",
            "160/160 [==============================] - 11s 68ms/step - loss: 0.4388 - accuracy: 0.8743 - precision: 0.8946 - recall: 0.8505 - val_loss: 0.7283 - val_accuracy: 0.7301 - val_precision: 0.7589 - val_recall: 0.7012\n",
            "Epoch 12/30\n",
            "160/160 [==============================] - 11s 69ms/step - loss: 0.3289 - accuracy: 0.9167 - precision: 0.9271 - recall: 0.9032 - val_loss: 0.7317 - val_accuracy: 0.7385 - val_precision: 0.7642 - val_recall: 0.7125\n",
            "Epoch 13/30\n",
            "160/160 [==============================] - 8s 53ms/step - loss: 0.2636 - accuracy: 0.9387 - precision: 0.9463 - recall: 0.9304 - val_loss: 0.8807 - val_accuracy: 0.7288 - val_precision: 0.7384 - val_recall: 0.7143\n",
            "Mean entropy for test data: 0.49718306148493946 - Lowest entropy for test data: 0.4597606434377062\n",
            "\n",
            "Iteration :6 - Samples: 12288 \n",
            " \n",
            "Epoch 1/30\n",
            "192/192 [==============================] - 13s 65ms/step - loss: 3.8788 - accuracy: 0.4160 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 3.0160 - val_accuracy: 0.4071 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/30\n",
            "192/192 [==============================] - 9s 49ms/step - loss: 2.4451 - accuracy: 0.4264 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.9506 - val_accuracy: 0.4071 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/30\n",
            "192/192 [==============================] - 12s 62ms/step - loss: 1.6614 - accuracy: 0.4421 - precision: 1.0000 - recall: 1.6276e-04 - val_loss: 1.4085 - val_accuracy: 0.4418 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/30\n",
            "192/192 [==============================] - 12s 62ms/step - loss: 1.2837 - accuracy: 0.4652 - precision: 0.6243 - recall: 0.0088 - val_loss: 1.1592 - val_accuracy: 0.4908 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/30\n",
            "192/192 [==============================] - 12s 62ms/step - loss: 1.1038 - accuracy: 0.5046 - precision: 0.6440 - recall: 0.1104 - val_loss: 1.0393 - val_accuracy: 0.5284 - val_precision: 0.7323 - val_recall: 0.1174\n",
            "Epoch 6/30\n",
            "192/192 [==============================] - 12s 61ms/step - loss: 1.0049 - accuracy: 0.5453 - precision: 0.6465 - recall: 0.2450 - val_loss: 0.9705 - val_accuracy: 0.5585 - val_precision: 0.7368 - val_recall: 0.2204\n",
            "Epoch 7/30\n",
            "192/192 [==============================] - 12s 61ms/step - loss: 0.9088 - accuracy: 0.6071 - precision: 0.7206 - recall: 0.3748 - val_loss: 0.9013 - val_accuracy: 0.5929 - val_precision: 0.7223 - val_recall: 0.4273\n",
            "Epoch 8/30\n",
            "192/192 [==============================] - 12s 60ms/step - loss: 0.7815 - accuracy: 0.7029 - precision: 0.7837 - recall: 0.5512 - val_loss: 0.8096 - val_accuracy: 0.6635 - val_precision: 0.7488 - val_recall: 0.5448\n",
            "Epoch 9/30\n",
            "192/192 [==============================] - 11s 57ms/step - loss: 0.6295 - accuracy: 0.7869 - precision: 0.8364 - recall: 0.7068 - val_loss: 0.7191 - val_accuracy: 0.7141 - val_precision: 0.7734 - val_recall: 0.6461\n",
            "Epoch 10/30\n",
            "192/192 [==============================] - 11s 57ms/step - loss: 0.4765 - accuracy: 0.8577 - precision: 0.8823 - recall: 0.8236 - val_loss: 0.6811 - val_accuracy: 0.7470 - val_precision: 0.7734 - val_recall: 0.7054\n",
            "Epoch 11/30\n",
            "192/192 [==============================] - 11s 57ms/step - loss: 0.3612 - accuracy: 0.9062 - precision: 0.9189 - recall: 0.8877 - val_loss: 0.6877 - val_accuracy: 0.7538 - val_precision: 0.7721 - val_recall: 0.7259\n",
            "Epoch 12/30\n",
            "192/192 [==============================] - 11s 57ms/step - loss: 0.2855 - accuracy: 0.9310 - precision: 0.9398 - recall: 0.9180 - val_loss: 0.6900 - val_accuracy: 0.7575 - val_precision: 0.7765 - val_recall: 0.7364\n",
            "Mean entropy for test data: 0.44111022476874384 - Lowest entropy for test data: 0.44111022476874384\n",
            "\n",
            "Iteration :7 - Samples: 14336 \n",
            " \n",
            "Epoch 1/30\n",
            "224/224 [==============================] - 13s 53ms/step - loss: 3.7297 - accuracy: 0.4191 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.7763 - val_accuracy: 0.4071 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/30\n",
            "224/224 [==============================] - 12s 52ms/step - loss: 2.1805 - accuracy: 0.4365 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.6941 - val_accuracy: 0.4189 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/30\n",
            "224/224 [==============================] - 11s 51ms/step - loss: 1.4383 - accuracy: 0.4622 - precision: 0.6980 - recall: 0.0119 - val_loss: 1.2197 - val_accuracy: 0.5126 - val_precision: 0.7164 - val_recall: 0.0519\n",
            "Epoch 4/30\n",
            "224/224 [==============================] - 11s 51ms/step - loss: 1.1270 - accuracy: 0.5119 - precision: 0.6432 - recall: 0.1314 - val_loss: 1.0354 - val_accuracy: 0.5282 - val_precision: 0.7329 - val_recall: 0.1409\n",
            "Epoch 5/30\n",
            "224/224 [==============================] - 11s 51ms/step - loss: 0.9850 - accuracy: 0.5656 - precision: 0.6787 - recall: 0.2742 - val_loss: 0.9499 - val_accuracy: 0.5692 - val_precision: 0.7344 - val_recall: 0.2722\n",
            "Epoch 6/30\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8748 - accuracy: 0.6371 - precision: 0.7273 - recall: 0.4457 - val_loss: 0.8735 - val_accuracy: 0.6251 - val_precision: 0.7462 - val_recall: 0.4576\n",
            "Epoch 7/30\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.7321 - accuracy: 0.7288 - precision: 0.7872 - recall: 0.6212 - val_loss: 0.7774 - val_accuracy: 0.6798 - val_precision: 0.7479 - val_recall: 0.5898\n",
            "Epoch 8/30\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.5780 - accuracy: 0.8082 - precision: 0.8430 - recall: 0.7530 - val_loss: 0.7149 - val_accuracy: 0.7264 - val_precision: 0.7688 - val_recall: 0.6777\n",
            "Epoch 9/30\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.4421 - accuracy: 0.8684 - precision: 0.8882 - recall: 0.8410 - val_loss: 0.6983 - val_accuracy: 0.7417 - val_precision: 0.7695 - val_recall: 0.7127\n",
            "Epoch 10/30\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.3346 - accuracy: 0.9106 - precision: 0.9211 - recall: 0.8949 - val_loss: 0.7156 - val_accuracy: 0.7478 - val_precision: 0.7679 - val_recall: 0.7275\n",
            "Epoch 11/30\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.2613 - accuracy: 0.9355 - precision: 0.9423 - recall: 0.9268 - val_loss: 0.7545 - val_accuracy: 0.7530 - val_precision: 0.7661 - val_recall: 0.7322\n",
            "Mean entropy for test data: 0.38883785261659326 - Lowest entropy for test data: 0.38883785261659326\n",
            "\n",
            "Iteration :8 - Samples: 16384 \n",
            " \n",
            "Epoch 1/30\n",
            "256/256 [==============================] - 13s 47ms/step - loss: 3.6142 - accuracy: 0.4238 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.5934 - val_accuracy: 0.4071 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/30\n",
            "256/256 [==============================] - 12s 46ms/step - loss: 2.0132 - accuracy: 0.4310 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.5565 - val_accuracy: 0.4073 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/30\n",
            "256/256 [==============================] - 12s 45ms/step - loss: 1.3488 - accuracy: 0.4565 - precision: 0.6356 - recall: 0.0046 - val_loss: 1.1732 - val_accuracy: 0.5026 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/30\n",
            "256/256 [==============================] - 12s 45ms/step - loss: 1.1072 - accuracy: 0.5051 - precision: 0.6465 - recall: 0.1083 - val_loss: 1.0343 - val_accuracy: 0.5369 - val_precision: 0.7294 - val_recall: 0.1419\n",
            "Epoch 5/30\n",
            "256/256 [==============================] - 12s 45ms/step - loss: 0.9996 - accuracy: 0.5498 - precision: 0.6616 - recall: 0.2516 - val_loss: 0.9597 - val_accuracy: 0.5714 - val_precision: 0.7367 - val_recall: 0.2622\n",
            "Epoch 6/30\n",
            "256/256 [==============================] - 11s 45ms/step - loss: 0.8921 - accuracy: 0.6234 - precision: 0.7260 - recall: 0.4109 - val_loss: 0.8779 - val_accuracy: 0.6203 - val_precision: 0.7493 - val_recall: 0.4439\n",
            "Epoch 7/30\n",
            "256/256 [==============================] - 11s 44ms/step - loss: 0.7548 - accuracy: 0.7186 - precision: 0.7955 - recall: 0.5842 - val_loss: 0.7694 - val_accuracy: 0.6883 - val_precision: 0.7669 - val_recall: 0.5814\n",
            "Epoch 8/30\n",
            "256/256 [==============================] - 11s 44ms/step - loss: 0.5974 - accuracy: 0.8042 - precision: 0.8480 - recall: 0.7367 - val_loss: 0.6876 - val_accuracy: 0.7407 - val_precision: 0.7859 - val_recall: 0.6880\n",
            "Epoch 9/30\n",
            "256/256 [==============================] - 11s 44ms/step - loss: 0.4541 - accuracy: 0.8679 - precision: 0.8904 - recall: 0.8362 - val_loss: 0.6563 - val_accuracy: 0.7662 - val_precision: 0.7894 - val_recall: 0.7312\n",
            "Epoch 10/30\n",
            "256/256 [==============================] - 11s 44ms/step - loss: 0.3465 - accuracy: 0.9091 - precision: 0.9225 - recall: 0.8919 - val_loss: 0.6627 - val_accuracy: 0.7680 - val_precision: 0.7890 - val_recall: 0.7451\n",
            "Epoch 11/30\n",
            "256/256 [==============================] - 10s 39ms/step - loss: 0.2800 - accuracy: 0.9335 - precision: 0.9423 - recall: 0.9223 - val_loss: 0.6862 - val_accuracy: 0.7678 - val_precision: 0.7848 - val_recall: 0.7488\n",
            "Mean entropy for test data: 0.45199424758422185 - Lowest entropy for test data: 0.38883785261659326\n",
            "\n",
            "Iteration :9 - Samples: 18432 \n",
            " \n",
            "Epoch 1/30\n",
            "288/288 [==============================] - 13s 43ms/step - loss: 3.4758 - accuracy: 0.4250 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.3799 - val_accuracy: 0.4071 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/30\n",
            "288/288 [==============================] - 12s 42ms/step - loss: 1.8103 - accuracy: 0.4438 - precision: 0.7097 - recall: 0.0024 - val_loss: 1.3792 - val_accuracy: 0.4600 - val_precision: 0.8571 - val_recall: 0.0095\n",
            "Epoch 3/30\n",
            "288/288 [==============================] - 12s 42ms/step - loss: 1.2089 - accuracy: 0.4907 - precision: 0.6376 - recall: 0.0782 - val_loss: 1.0656 - val_accuracy: 0.5255 - val_precision: 0.7342 - val_recall: 0.1251\n",
            "Epoch 4/30\n",
            "288/288 [==============================] - 12s 42ms/step - loss: 1.0101 - accuracy: 0.5500 - precision: 0.6653 - recall: 0.2694 - val_loss: 0.9561 - val_accuracy: 0.5637 - val_precision: 0.7244 - val_recall: 0.2678\n",
            "Epoch 5/30\n",
            "288/288 [==============================] - 12s 41ms/step - loss: 0.8834 - accuracy: 0.6271 - precision: 0.7228 - recall: 0.4275 - val_loss: 0.8526 - val_accuracy: 0.6390 - val_precision: 0.7743 - val_recall: 0.4697\n",
            "Epoch 6/30\n",
            "288/288 [==============================] - 12s 41ms/step - loss: 0.7080 - accuracy: 0.7427 - precision: 0.8084 - recall: 0.6255 - val_loss: 0.7131 - val_accuracy: 0.7175 - val_precision: 0.7797 - val_recall: 0.6356\n",
            "Epoch 7/30\n",
            "288/288 [==============================] - 12s 41ms/step - loss: 0.5181 - accuracy: 0.8360 - precision: 0.8676 - recall: 0.7861 - val_loss: 0.6387 - val_accuracy: 0.7701 - val_precision: 0.7975 - val_recall: 0.7309\n",
            "Epoch 8/30\n",
            "288/288 [==============================] - 12s 41ms/step - loss: 0.3776 - accuracy: 0.8951 - precision: 0.9083 - recall: 0.8754 - val_loss: 0.6098 - val_accuracy: 0.7838 - val_precision: 0.8023 - val_recall: 0.7565\n",
            "Epoch 9/30\n",
            "288/288 [==============================] - 11s 39ms/step - loss: 0.2922 - accuracy: 0.9265 - precision: 0.9357 - recall: 0.9164 - val_loss: 0.6369 - val_accuracy: 0.7809 - val_precision: 0.7918 - val_recall: 0.7612\n",
            "Epoch 10/30\n",
            "288/288 [==============================] - 11s 39ms/step - loss: 0.2378 - accuracy: 0.9433 - precision: 0.9499 - recall: 0.9364 - val_loss: 0.6593 - val_accuracy: 0.7799 - val_precision: 0.7908 - val_recall: 0.7654\n",
            "Mean entropy for test data: 0.4829179345496672 - Lowest entropy for test data: 0.38883785261659326\n",
            "\n",
            "Iteration :10 - Samples: 20480 \n",
            " \n",
            "Epoch 1/30\n",
            "320/320 [==============================] - 14s 41ms/step - loss: 3.3552 - accuracy: 0.4295 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.2044 - val_accuracy: 0.4071 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/30\n",
            "320/320 [==============================] - 13s 40ms/step - loss: 1.6591 - accuracy: 0.4491 - precision: 0.6168 - recall: 0.0032 - val_loss: 1.2672 - val_accuracy: 0.5029 - val_precision: 1.0000 - val_recall: 7.8989e-04\n",
            "Epoch 3/30\n",
            "320/320 [==============================] - 13s 40ms/step - loss: 1.1324 - accuracy: 0.5040 - precision: 0.6268 - recall: 0.1210 - val_loss: 1.0199 - val_accuracy: 0.5329 - val_precision: 0.7039 - val_recall: 0.1527\n",
            "Epoch 4/30\n",
            "320/320 [==============================] - 13s 40ms/step - loss: 0.9730 - accuracy: 0.5563 - precision: 0.6585 - recall: 0.2864 - val_loss: 0.9266 - val_accuracy: 0.5756 - val_precision: 0.7382 - val_recall: 0.2725\n",
            "Epoch 5/30\n",
            "320/320 [==============================] - 13s 40ms/step - loss: 0.8384 - accuracy: 0.6492 - precision: 0.7387 - recall: 0.4684 - val_loss: 0.8084 - val_accuracy: 0.6535 - val_precision: 0.7673 - val_recall: 0.5182\n",
            "Epoch 6/30\n",
            "320/320 [==============================] - 13s 40ms/step - loss: 0.6556 - accuracy: 0.7637 - precision: 0.8194 - recall: 0.6648 - val_loss: 0.6747 - val_accuracy: 0.7367 - val_precision: 0.7892 - val_recall: 0.6603\n",
            "Epoch 7/30\n",
            "320/320 [==============================] - 13s 40ms/step - loss: 0.4840 - accuracy: 0.8461 - precision: 0.8721 - recall: 0.8058 - val_loss: 0.6071 - val_accuracy: 0.7757 - val_precision: 0.8041 - val_recall: 0.7425\n",
            "Epoch 8/30\n",
            "320/320 [==============================] - 13s 40ms/step - loss: 0.3575 - accuracy: 0.9005 - precision: 0.9121 - recall: 0.8852 - val_loss: 0.5877 - val_accuracy: 0.7878 - val_precision: 0.8043 - val_recall: 0.7659\n",
            "Epoch 9/30\n",
            "320/320 [==============================] - 13s 40ms/step - loss: 0.2831 - accuracy: 0.9265 - precision: 0.9329 - recall: 0.9174 - val_loss: 0.6000 - val_accuracy: 0.7886 - val_precision: 0.8029 - val_recall: 0.7746\n",
            "Epoch 10/30\n",
            "320/320 [==============================] - 13s 40ms/step - loss: 0.2305 - accuracy: 0.9436 - precision: 0.9488 - recall: 0.9368 - val_loss: 0.6298 - val_accuracy: 0.7915 - val_precision: 0.8007 - val_recall: 0.7762\n",
            "Mean entropy for test data: 0.3622302620232341 - Lowest entropy for test data: 0.3622302620232341\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import asarray\n",
        "from numpy import savetxt\n",
        "from google.colab import files\n",
        "'''\n",
        "Saving the learning curve in csv file for later analyzes.\n",
        "'''\n",
        "\n",
        "savetxt('val_accuracies.csv', np.round(val_accuracies, decimals= 4) , delimiter=',')\n",
        "\n",
        "savetxt('train_accuracies.csv', np.round(train_accuracies, decimals= 4) , delimiter=',')\n",
        "\n",
        "files.download('train_accuracies.csv')\n",
        "files.download('val_accuracies.csv')\n",
        "\n",
        "print(val_accuracies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Ns-U2F9aSoCY",
        "outputId": "16a3c28f-305a-44e4-e185-8233f45bbef5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_2be63756-7e25-4a79-969a-046100c15d51\", \"train_accuracies.csv\", 250)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_00964704-375a-439f-ad2c-8312a067ce6f\", \"val_accuracies.csv\", 250)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.6585044860839844, 0.6627172231674194, 0.7072142958641052, 0.7232754230499268, 0.73854660987854, 0.7575039267539978, 0.7530279159545898, 0.7680358290672302, 0.7838336229324341, 0.7914692163467407]\n"
          ]
        }
      ]
    }
  ]
}